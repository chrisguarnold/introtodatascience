# (PART\*) Scenario II: Knowing It All By Knowing A Few{-} 

```{r echo=FALSE, include=FALSE}
# -- housekeeping -------------------------------------------------------------- 
rm(list=ls())
# Libraries
library(foreign)
library(xtable)


# data: Class surveys
dat18 <- read.csv("preparing_data/data/class_survey/lecture_survey_18.csv")
dat19 <- read.csv("preparing_data/data/class_survey/lecture_survey_19.csv")
dat <- read.csv("preparing_data/data/class_survey/lecture_survey_21.csv")

# names(dat)
attach(dat)

# Predefined Cardiff colours
# primary
cardiffred <- rgb(211,55,74, maxColorValue = 255)
cardiffblack <- rgb(35,31,32, maxColorValue = 255)
cardiffgrey <- rgb(47,68,78, maxColorValue = 255)
cardiffgold <- rgb(189,158,94, maxColorValue = 255)
# secondary 
cardiffblue <- rgb(21,44,81, maxColorValue = 255)
cardiffpurple1 <- rgb(29,15,51, maxColorValue = 255)
cardiffpurple2 <- rgb(60,44,89, maxColorValue = 255)

```






# Testing a Hypothesis 
Welcome to the second part of the module PL9239. In our second scenario we turn to one of the core tasks of any data analysis: inference. The basic idea is that you want to be able to make a statement---e.g. calculate a statistic---about the world out there. But since the world out there is so large it is impossible to collect data about everything, we will have to chose a different approach. Instead, we are going to learn how to collect data about a sample and then say something with a little uncertainty about the world at large. 

To show you how useful all this actually is, I interviewed Michelle Brown who is working as a Senior Advisor on Elections for the [National Democratic Institute](https://www.ndi.org/). This NGO cooperates with local civil society actors in fragile democracies and trains them to do voting observation missions. These election observers tabulate the ballots in parallel to the official representatives and can thus get an idea about the amount of voting fraud that might be occurring. 

Obviously, it is prohibitive to observe every single polling station. In some countries there might be tens of thousands of polling stations and an NGO never has the resources to cover all that. Instead, the NDI relies on sampling. Election observers only monitor randomly selected polling stations, and the NDI then uses statistics to infer what is really going on. 



```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" src="https://www.youtube.com/embed/y5W0Xj6bQh0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'
)
```


Together with colleagues, we are doing research with the National Democratic Institute that helps them improve their technology stack even further. In case you are interested, feel free to take a look at a [presentation](https://youtu.be/3vpP8MA5Ocg?t=1005) I gave at last year's CogX---it might give you a good overview about what we are doing. Of course, feel free to reach out in case you want to know more.


## Generating Good Hypotheses
All research requires hypotheses. What is a good hypothesis? Good Hypotheses have a number of characteristics.

* A hypothesis is falsifiable.
* A hypothesis relates two observations.
* A hypothesis qualifies this relation, i.e. makes a statement whether the relation is the same, larger or smaller.

Let me walk you through good hypothesis in a short video using our data as an example.


```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" src="https://www.youtube.com/embed/nPuJ1lzk0Bk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'
)
```






## Studying Our Sample


### Investigating the Hypothesis 

We investigate our data and take a look at what the data is telling us. First we compare the hours that you are studying with the hours that you think you should be studying. Given that we assume that you have a bit of a bad conscious regarding your efforts, there should be a clear difference.

> **Hypothesis 1**: On average, you think that you should be studying more than you actually are. 


Let us investigate the question. We begin with Hypothesis 1.

```{r echo=FALSE}
par(mar = c(4,5,1,1))

boxplot(list(studyideal, studyperweek), horizontal = TRUE
        , col = c(cardiffgold, cardiffred)
        , axes = FALSE
        , boxwex = .6
        , ylim = c(0,40)
        , xlab = 'Hours'
)

axis(2, col = "white", las = 1, at = c(1,2),lab =c("Ideally \n Studied", "Ideally \n Studied"))
axis(1)

```

It seems, there is a lot to our hypothesis. Given the two distributions, there seems to be a clear difference. 

Next we will compare you to the cohort here in Cardiff two years ago. Actually, I asked them the same questions that I asked you. Would we expect any difference between you and the previous cohorts? There is no real reason to believe that the cohorts are different, so:

> **Hypothesis 2a**: On average, all three cohorts are putting in the same effort.

> **Hypothesis 2b**: On average, you and your previous cohorts have the same expectations regarding the efforts that you think you should be making for your university degree.

Let us take a look at what the data is telling us.


```{r echo=FALSE, fig.show="hold", out.width="50%"}

par(mar = c(4,7,1,1))

boxplot(list(dat18$studyperweek, dat19$studyperweek, studyperweek), horizontal = TRUE
        , col = cardiffgold
        , axes = FALSE
        , boxwex = .6
        , ylim = c(0,60)
        , xlab = 'Hours'
        , main = 'How Much You Study'
)
axis(2, col = "white", las = 1, at = c(1,2,3),lab =c("Module 2018", "Module 2019", "Module 2021"))
axis(1)

boxplot(list(dat18$studyideal, dat19$studyideal, studyideal), horizontal = TRUE
        , col = cardiffred
        , axes = FALSE
        , boxwex = .6
        , ylim = c(0,60)
        , xlab = 'Hours'
        , main = 'How Much You Think You Should Be Studying'
)
axis(2, col = "white", las = 1, at = c(1,2,3),lab =c("Module 2018", "Module 2019", "Module 2021"))
axis(1)


```

It seems as if on average you are investing less than the cohorts in 2019 and 2018. It is also interesting that the variation between you is much lower than the variation in the previous cohorts. Now when you compare your cohort to the cohorts with regards to how much you think you should be studying, your ideal number of hours is on average not so different and you seem to be a bit more in line with the other students. 



## Is What We See Just Coincidence?

We have seen that indeed there are slight differences between your module and the students in the module before you. Now, can we say something about your cohort---all Politics and IR students in your year---more in general? It could be, for example, that out of pure coincidence you are a particularly efficient crowd of students who do not need as much time to study as the cohorts before you. So given the differences in the observations from your class and the classes in previous years, can we say something about whether we would expect differences in your cohorts in general?

### Are the Two Cohorts the Same?
To be able to answer this question, we first need to understand the idea of sampling. Sampling means to draw a number of observations randomly from a larger population. Intuitively, our sample should be able to provide us with a good account of what is going on in the population. Of course, with bad luck we can always be wrong and sample a result that is quite unusual given what is going on in the population.

It is also intuitive that larger samples should be less prone to coincidences. If we only ask a few number of individuals, there is fair likelihood that these few people do not represent the overall population well. However, if we ask a larger amount of people, we could be fairly certain that this sample gives us a good insight about the overall population. 

<!-- Let us do an easy sampling exercise with Lego bricks:  -->
<!-- VIDEO WITH SAMPLING. FIRST ROUND SAMPLE 2 BLOCKS. SECOND ROUND SAMPLE 10 BLOCKS. -->


How does it help us understand whether your cohort is different from the cohort before you? Let us assume that on average the effort that you put into your studies is the same, for you and for the cohorts before you. How likely is it that we would observe these two samples assuming that the cohorts invest on average the same amount of time? What are the odds to observe these differences? We have a clear expectation. If the average amount of hours in our data sample is largely different, then this is already a good indication that it should be quite unlikely to observe so different samples given that they all come from the same population. 

If it is unlikely that we observe such a group of students on the basis of pure coincidence, we should maybe revisit our initial assumption. Maybe, it was wrong to assume that you and the other cohorts all share the same efficiency in studying. We would then have to refute this initial assumption and rather conclude that, well maybe your group indeed puts in a different effort into your studies. 




### How Can We Tell the Difference?

In the light of the two samples that we observe, we want to be able to determine how likely it is that they are from the same population? So we need a measure. It is called the p-value and in our example it will be able to tell us how likely is it to observe the mean of the two samples given that we assume they come from the same population. 

This is a very useful measure, because now we can say something about the likelihood that these samples really are from cohorts with the same amount of effort in both groups. If the samples look quite similar, then we can be certain that they are from same population. However, if they are really distinct, the probability that the cohorts have similar efforts is quite small. So what should we conclude? We might want to reconsider our initial assumption: Maybe the two cohorts are not one large population. Instead, maybe the two cohorts are different after all. 


### Hypothesis Testing


What we are doing here is called hypothesis testing. More formally, we distinguish between two hypothesis. The first one is called the *null-hypothesis* and it assumes that there is no difference between the two groups. We can specify it explicitly as follows.

> Null hypothesis ($H_0$): The samples come from populations with the same mean.

As a default, we will always at first believe in the fact that there is no difference between the cohorts. We change our minds only if we see that it is quite unlikely that we would observe our two samples. In that case we will then rather prefer a different hypothesis, the *alternative hypothesis*.

> Alternative hypothesis ($H_A$): The samples come from populations with different means.

If we believe in the alternative hypothesis, then we say that the two populations are different. 

How unlikely is 'unlikely enough' to change our minds? By convention, in the Social Sciences we define a threshold of 5% for that: We will believe in the null-hypothesis until the two samples are so different that we think it would be less than 5% probable to observe two samples that are so different by chance. More formally, the threshold $\alpha$ for the acceptable p-value is set to $p \leq 0.05$.



### Test! Test! Test!
Now, let us go and test the hypotheses in our data. 

Regarding the actual effort you put into studying, is your cohort different from the cohort in 2019? With R, we can do a so called t-test. We will cover it more formally next week, so do not worry at this stage how it really works. What matters for now is that it will return us the p-value we are interested in: the probability that the mean in the cohorts is the same given the two samples we observe. We first compare your cohort to the cohort in 2018. 

```{r echo=FALSE}
ttest.perweek.21.18 <- t.test(dat18$studyperweek, studyperweek)
ttest.perweek.21.18
# t.test(dat18$studyperweek, studyperweek)
# dat18$studyideal, dat19$studyideal, studyideal)
```

R reports a number of statistics and for now we only care about some. First, for the mean of both of the samples at the very bottom of the table with `r round(ttest.perweek.21.18$estimate[2], 2)` for your cohort and `r round(ttest.perweek.21.18$estimate[1], 2)` for the cohort back in 2018. The important bit for us is the p-value. Here it is reported as `r round(ttest.perweek.21.18$p.value, 2)`.

How do we interpret this result? Well it tells us that assuming that both of the samples come from the same population, there is a `r round(ttest.perweek.21.18$p.value*100, 2)`% probability that we observe the result if both are from populations with the same mean. In the light of this evidence, we stick with the null hypothesis and believe that apparently the amount of average effort is the same in both cohorts.

Let us now compare your cohort to the one in 2019. Here are the results from the t-test.

```{r echo=FALSE}
ttest.perweek.21.19 <- t.test(dat19$studyperweek, studyperweek)
ttest.perweek.21.19
```


The means are even further apart. The cohort in 2019 was putting in on average `r round(ttest.perweek.21.19$estimate[1], 2)` hours, while you on average study `r round(ttest.perweek.21.19$estimate[2], 2)` hours. Can we still assume that the mean in both cohorts is the same in the light of the data we collected from the students in the two methods classes? 

The p-value is reported as `r round(ttest.perweek.21.19$p.value, 2)`. If both of the samples were drawn from cohorts with the same mean, we would have a `r round((ttest.perweek.21.19$p.value*100), 2)`% probability that we observe the data in our two samples. This is quite unlikely and below the general accepted the threshold of $\alpha$ = 0.05. So what do we do now? Well, we refute the null hypothesis $H_0$. Instead, we will trust in the alternative hypothesis $H_A$. Your cohort is on average studying less than the cohort in 2019. 

We can do the same exercise for the hours that you think you should be studying ideally. Let me show you how this works in a quick video. 


```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" src="https://www.youtube.com/embed/-CS29tNb9hM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'
)
```



### Type 1 and Type 2 Errors
When we test a hypothesis based on samples, we can of course be wrong. Remember that everything do involves probabilistic statements. An $\alpha$ level of 0.05 means that in 19 out of 20 cases we should be correct. However, in one out of 20 cases we might wrongly refute a hypothesis. 

We could observe two samples that seem to be different even though in reality the null hypothesis is actually true. This kind of error is called a type-1 error or also a false positive. Then again, the two samples may come from populations with distinct means. But out of bad luck we get samples that actually look quite similar. In that case, we speak of a false negative, or a type-2 error. Figure \@ref(fig:t1t2e) offers you a nice summary.

```{r t1t2e, out.width=300, fig.align="center", fig.cap="Type 1 and Type 2 Errors", echo=FALSE}
# This works in the browser but not in the R Markdown Viewer.
knitr::include_graphics("./figures/t1t2e.pdf")
```



A good way of remembering this difference is with the help of the following story that you certainly know from your childhood. 


```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" src="https://www.youtube.com/embed/-DRS8DR0Rck" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'
)
```



The first time the young shepherd is calling for help, the farmers believe that the wolf is there even though in reality it is not---clearly a type 1 error or a false positive. In the second instance, the farmers think there is no wolf, even though in reality there is actually one. We speak of a false negative, or a type 2 error. 



## Coding 
This week was quite heavy on conceptual things, so we will be much lighter with the code. 

### Data Management 

We learned that we can select from objects using their position and these brackets  `[ ]`. Since data frames are R's spreadsheets, you can select from them also by the name of their column. 

```{r}
# Create data
people.living.in.household <- c(3,2,5)
education.in.years <- c(10, 13, 15)
favourite.colour <- c('forestgreen', 'darkred', 'lemongreen')
dat <- data.frame(people.living.in.household, education.in.years, favourite.colour)
```


Let us select the first column.

```{r}
dat[,1]
dat$people.living.in.household
```

Or what about the third element of second column?

```{r}
dat[3,2]
dat$education.in.years[3]
```


Sometimes you want to recode a variable. The oldschool way using the `car` package goes as follows.

```{r, warning=FALSE, message=FALSE}
library(car)
dat$favourite.colour <- recode(dat$favourite.colour, "
       c('forestgreen', 'lemongreen') = 'green';
       'darkred' = 'red'")

dat$favourite.colour
```

Finally let us meet a problem that will keep haunting you until the end of your professional life: missing data. Sometimes, data has empty entries. By convention R uses `NA` for it. Please not that even if it looks like a string variable, it actually is not treated as such by R. 


```{r echo=FALSE}
income <- c(50000, 100000, NA)
dat$income <- income
dat$income

```

Things begin to be complicated as soon as you want to do calculations with the variable. The reason is that R protects you from making stupid mistakes. 

```{r}
mean(dat$income)
```

I know this is a pain at the moment, but believe me, once you write more complex programmes for analysing data, it will become really useful. You can set options in the functions to override the default behaviour.

```{r}
mean(dat$income, na.rm = TRUE)
```

Another more general way of handling the issue is to filter your data set for missing values with `na.omit()`.

```{r}
dat.no.na <- na.omit(dat)

dim(dat)
dim(dat.no.na)
```

As you can see, R kicked out a whole row from our observations. This certainly means that quite a bit of information just go lost here. Deleting missing data is a quick and convenient fix, but it comes at a high price. 

There is a whole subfield that is concerned with [imputing values](https://en.wikipedia.org/wiki/Imputation_(statistics)) that are reasonable for the missing observation. The intuition is that you would use information from the fully observed variables to get a good guess for the observation that is missing. For example, in our case the years in education might give us a good proxy for the income that someone receives and we can get a good estimate for it. If we then want to analyse the relationship between---say---preferred colours and income we would then have a better data basis for that.

<!-- ## R:  -->


### Hypothesis Testing 
R is made for statistical analysis. No surprise that doing a t-test is really easy: With the function `t.test()`.

```{r}

ttest.perweek.21.19 <- t.test(dat19$studyperweek, studyperweek)


```




## Mathcamp: Summing With Sigmas

You were asking for a refresher on how to read the symbol $\sum$, so I put together a small video.


```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" src="https://www.youtube.com/embed/YKu77QytJOE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'
)
```

## Reading for This Week
The readings for this week are chapter 6 in @Fogarty2019 which tells you all about generating hypotheses. Chapter 9 is all about how to test them.































