[["index.html", "PL 9239 Introduction to Data Science for Politics and IR Welcome! Data to Answer Your Questions Your Weekly Data Workout The Module Webpage Getting in Touch", " PL 9239 Introduction to Data Science for Politics and IR Christian Arnold, Cardiff University 2022-02-11 Welcome! Welcome to this module. In PL 9239 you will learn how to make the most of data. How can you handle and analyse data efficiently? How to communicate and visualise results? While everything we do here has of course a wider appeal, we will focus in particular on our area: How can we use data to study politics and international relations? I am really looking forward to teaching the module again in Spring of 2022. Data to Answer Your Questions Today, data are everywhere. Data are not only collected for scientific studies. Governments, companies and non-profit organisations are amassing and increasingly sharing data at an unprecedented level. Data on their own can actually be a bit boring. Few people might enjoy to just browse numbers. Everything changes, however, if we can use data to answer a question we are interested in. This is why we we will build the module around three scenarios. Scenario 1: The Data Report Whatever you actually want to do with data, the first step is always getting a good oversight over the data itself. You will have to open the data and describe its most characteristic features. Often, you want to communicate these results to other colleagues in a data report. And all of your work with the data is ideally documented in a transparent way so that others—or even your future self—can easily understand what you were doing. This first scenario might actually be quite typical for your first years at work. Scenario 2: Knowing It All By Knowing A Few For the second scenario, imagine you are working for an NGO that helps implement voting observation missions in fragile democracies. Even though you are partnering up with local civil society organisations, you cannot send election observers too all polling stations during a mission. Luckily, this is not strictly necessary: As long as you properly randomise the polling stations you want to observe, you can infer the real, countrywide numbers on the basis of just a few hundred few hundred observations. In this class, we will learn why this is working and how you can do inference yourself. Scenario 3: Did the New Policy Have an Effect? Finally, take another example: A new law introduced mandatory environmental protection for farm land. A year later, biologists measure that harmful substances in ground water are down by 20%. Was this the effect of the policy? Or was it maybe just a particularly rainy year that washed all pollutants away? Together we will learn how we can find answers by asking our data the right kind of questions. The module is particularly relevant if you are planning to undertake a dissertation, since it will help you find the right data and how to work with it. Data literacy is not only a really useful skill to have as a modern citizen, it is also really valuable when you are looking for a job. Your Weekly Data Workout In this module, we will expose you to core concepts prior to the actual class. This frees up large chunks of time during the class that we can spend on activities where you typically need the most help, such as application of basic material and engaging in deeper discussions and creative work with it. Two years ago, all this would have been quite revolutionary. But now, 24 months into the pandemic, all of this feels actually quite intuitive. Let me introduce all of our activities one by one. 1 Work Through This Homepage Your first port of call in every week will be the respective chapter in this module homepage. This site is a mixture between a website and a teaching book. It will walk you through the content of each week and you can find the most important content explained here. The website allows you to take your own pace and revisit the parts that you find most challenging as many times as you want and at the pace that you feel comfortable with. The respective content of the week will be uploaded on Monday morning, latest. I expect that you work through that home page on your own over the course of the week. In case you have questions, please reach out to me via email or book time in the office hour. We will also use the module cafe every second week for answering any of your questions. 2 Apply Your Knowledge in Our Workshops While the homepage is focusing on knowledge acquisition, our workshops on Tuesdays between 1pm-1.45pm and Friday 3pm-3.45pm will focus on skills development. Learning how to do data analysis requires practice. And practice means failing a lot and learning from the own mistakes. The workshops on Tuesdays are meant to serve as a place where we can jointly explore how to properly manage, analyse and visualise data. The workshops will always cover the content of the past week, e.g. in the workshop in week 2 we will be covering the content of the homepage from week 1. Please make sure that you come prepared. Workshops will run from week 1 until week 11. Over the course of these two workshop hours, you will be working online in small groups. For each week, I will prepare lab sheets that you will jointly solve. Do not worry in case you struggle: I will be always around to help. Without participating in these workshops, the assignments will be really challenging. I strongly recommend you do not miss a single session. 3 Bring Your Questions to the Seminars We will have bi-weekly module cafes where you can discuss any additional questions that you might have. In addition, these module cafes will also serve to answer any question regarding the assignments. 4 Read Guideance on Planning and Structuring To guide you through all the hoops, I will also stay in touch with you with am email every Friday to make sure that you are on board. It is vital that you check your university email regularly, since we will only communicate with you through this address. The Module Webpage Let us take a closer look at how this website is working. This space is going to be the main learning tool for this term’s Introduction to Data Science for Politics and IR. Obviously, I will guide you through the material on this web-page via text. Think of it as a replacement for the lecture slides that you might find in other contexts: A place to find the core concepts and central ideas of this class. But not all is text. Sometimes, nothing beats someone actually explaining you core ideas. For that purpose, I will record small videos and embedd them here. The best thing about videos: You can watch them as many times as you want… Finally, this format is also really handy, because we will be able to take a look at R code. R is a computer language invented for data analysis, and you will learn how to use it in PL 9239. This is how it looks like when we will be covering R code. # All text in a line after the hashtag is a comment. # Here we assign some variables a &lt;- 1 b &lt;- 2 # This is how to add them and assign the result to another variable c &lt;- a+b # And finally we print the result cat(c) ## 3 Last but not least, each week will have some readings that will reflect the core content of the respective week from yet another perspective. I will add pointers to the readings whenever appropriate. You can find all information about the readings in the [bibliography]. Getting in Touch If you have questions or need any kind of help, feel free to get in touch. The whole purpose of our weekly workshops and our bi-weekly module cafe is to provide you with ample space for your questions. Make use of this! You can always drop me an email. I will respond at the end of the working day. You can also book time in my office hours Tuesday 3-4pm; Wednesday 3.15-4.15pm; Friday 5.15-6.15pm. "],["tools-for-working-with-data.html", "Chapter 1 Tools for Working with Data 1.1 What Is In a Data Spreadsheet? 1.2 Key Concepts 1.3 Meet: R 1.4 Readings for This Week", " Chapter 1 Tools for Working with Data This week we will get to know the basic tools we need. 1.1 What Is In a Data Spreadsheet? When we work with data, the first step is always to get an overview of the data—be it in academia, for a company or in government. The whole first part of this module is dedicated to teaching you how to understand your data in a quick and efficient way. We will learn a couple of important things. How to access data. How to get a quick overview over the data. How to describe data and extract their core characteristics. How to communicate and visualise results. How to manage everything efficiently. To give you an idea how and why even these basic skills matter in real life, I talked to Dr Sebastian Sternberg. He has a degree in politics and works as a data scientist with KPMG at the moment. In the interview, he was so kind to offer us insights into how he is working with data and how relevant data reports are in his daily work. 1.2 Key Concepts Let as begin with some key definitions. 1.2.1 What is a Statistic? Statistics summarise large amounts of numerical data. Statistics are really useful if one wants to get a good overview over data. A statistic is a characteristic of a sample. Imagine we make a number of observations and put in numbers what we see. When we calculate a statistic on this data, the statistic is able to describe the sample we collected. If we collect a different sample, the statistic is very likely to have slightly different values. The goal of statistical methods is to make inference about a population based on information from a sample of that population. Often, we might be interested in more than just the data at hand. By drawing a sample, we hope to generalise beyond our sample and learn something about the the overall population. For example, by asking a few hundred Welsh voters about their voting intentions we aim at saying something about the voting intention of all voters in Wales. To estimate parameters, we use statistical methods. Estimating a parameter means that you need to be able to say something about a certain parameter on the basis of a couple of data points that you collected. We will be learning how to use the correct statistics to infer what we believe is the most likely value for a certain parameter. In the voting example, we might be interested to understand what share of voters who would cast their vote for Plaid Cymru for example. Statistics can separate the probable from the possible. When we collect data, the data can have a range of different values. Of course, not all possible data points are equally likely. The beauty of statistics is that they can be used to tell us which values we can expect to see more often and which ones least often. For example, voters might be able to cast their vote for a whole range of parties—e.g. Labour, Conservatives, Plaid Cymru or the Welsh Nation Party. However, voting for these parties is not necessarily equally likely. It is reasonable to expect that, say, the Conservatives will receive more votes than the Welsh Nation Party. 1.2.2 Some More Definitions Some more definitions that a really useful for our module. It would not hurt if you could learn them by heart. Population: The full set of cases about which we want to generalise. Sample: A subset of the population. Variable: Logical set of attributes (characteristics) of an object (person, thing, etc.) that can vary across a range. Parameter: A characteristic of a population; usually unknown. Descriptive Statistics: Statistics that summarise the distribution of the values of variables in a sample. Inferential Statistics: The use of statistics to make inferences about a larger population based on data collected from a sample. 1.2.3 Types of Data Levels Example from our Survey Real World Example Measurement Possible Operations Nominal Tea or coffee? Gender, religion Categories Frequencies Ordinal Do you like summer? Social class, attitudes Categories and ranking Frequencies and ranking Interval How many hours do you study? Age, income All above and distance All above and addition and subtraction 1.2.4 Data in Action Remember that you were filling out the survey just before term? Overall, 23 of you responded (thanks!). Let us go and take a look at some examples of your data to better understand the different types of data that are out there. Nominal Data What do you prefer to drink in the morning? The variable encodes three different answer categories. These categories cannot be ranked, obviously. These are your responses: Ordinal Data How much do you like summer? You were given a scale that ranged from 1 to 5. The answers to this question can be ranked. However, it is not necessarily true that the distances between the answer categories is always the same. Now look at your responses: Here comes the sun! Interval Data Finally, let us consider some interval data. How many hours do you study per week? This data provides full hours. The data is discrete, its values can be ranked, and the distance between any two neighbouring categories is always the same. 1.3 Meet: R Now that we have a bit of an overview how data can look like, let us take a look at the main tool that we will use to manage, analyse and visualise data. We will use a programming language for this module that is called R. It is free and open source, so you can install it easily on your computers. It is also very powerful which means that all the effort that you are investing in these 10 weeks of the term to learn it will heavily pay-off when you are analysing data in the future—be it for research or in any other professional context. And ‘paying-off’ is meant quite literally here. R is a really valuable skill set to have on your CV and is certain to boost your employability quite a bit. If you want to know more about R, where its coming from and how it all developed, check the Wikipedia Page as a start. Also, feel free to go wild on the homepage of the R Project itself or in any other of your favourite corners of the internet. Beware: The rabbit hole is quite deep. #nerdalert 1.3.1 Install R Installing R is pretty straightforward: Go and visit the homepage of the The Comprehensive R Archive Network where you can find the latest version of R. At the top of the page, you can chose between your operating system: 1.3.2 Install R Studio Now that you have installed the programming language, let’s go and get a nice interface that actually helps us get our work down. R-Studio is a programme that makes it much more easy to write and execute R code. Go and get the free R-Studio desktop version. Install the version that suits your operating system. Nota bene: Make sure you first install R and then R-Studio. 1.3.3 How to Work with R Here is a quick video in which I show you around. You can also take a look at Chapter 2 in Fogarty (2019) for a lot of helpful details. 1.3.4 First Steps in R Now that you know how R-Studio looks like and how to use it, let’s go and try it out. As you saw, in the console tab you can run commands directly. But it is better practice to type them in an R script and send them. Every line of the text editor can be sent using Str + Enter in Windows and Cmd + Enter in MacOS. Objects R can keep several objects in memory at the same time. To distinguish them, object have names. Objects are assigned with an arrow like this: &lt;- Let us assign some values to objects. a &lt;- 5 b &lt;- 6 c &lt;- a * b Objects can be called using their name. Here on this homepage, you will see a second grey block that will give you the output. If you type all up in R Studio, you will find all R related output in the console. a ## [1] 5 b ## [1] 6 c ## [1] 30 We can combine several values with the function c(). Functions are really useful. There are basically three elements to a function. * First of call, they have a name—here in this case it is the letter c which stands for concatenate. * Then, functions always need one more more inputs. A function receives its inputs in the brackets (). * If you call a function and provide it with its correct inputs, it will do its thing and return the output that you asked for. a &lt;- c(1,2,3) a The object a is not a scalar any more. It is now a vector that has three elements to it, the numbers 1, 2 and 3. You can of course also assign more than just numbers. We can assign strings—here a couple of three digit country codes. cntry &lt;- c(&quot;BRA&quot;, &quot;GER&quot;, &quot;FRA&quot;, &quot;NLD&quot;) cntry ## [1] &quot;BRA&quot; &quot;GER&quot; &quot;FRA&quot; &quot;NLD&quot; If you want to see all the objects that we have in the workspace so far, try the function ls(). ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;cntry&quot; &quot;dat21&quot; You want to remove an object? Then use the function rm() like so. rm(cntry) See, the object cntry is not in the workspace any more. ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;dat21&quot; R Data Types In R, there can be different types of objects. Some can only take specific types of data. Scalar: numbers, characters, logical values Vector: sets of scalars Matrix: two-dimensional set of scalars of same type Data frame: Collections of vectors of (possibly) different types but with same length Let us begin from scratch with am empty workspace. To delete everything, we will nest two of the functions above. rm(list=ls()) First we assign some scalars. # scalar a &lt;- 43 b &lt;- a + 7 a ## [1] 43 b ## [1] 50 And here we go with some vectors. x &lt;- c(a,b,a,b) y &lt;- x + 10 cntry &lt;- c(&#39;Brasil&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Singapore&#39;) x ## [1] 43 50 43 50 y ## [1] 53 60 53 60 cntry ## [1] &quot;Brasil&quot; &quot;Canada&quot; &quot;China&quot; &quot;Singapore&quot; Something interesting just happened here: You see the object x which is a vector? R added 10 to each of the scalars in x when calculating y. In programming you call this broadcasting. Pretty nifty! Next, we build a matrix, for example by binding two columns with the function cbind() or two rows with the function rbind(). z &lt;-cbind(x,y) z2 &lt;-rbind(x,y) z ## x y ## [1,] 43 53 ## [2,] 50 60 ## [3,] 43 53 ## [4,] 50 60 z2 ## [,1] [,2] [,3] [,4] ## x 43 50 43 50 ## y 53 60 53 60 This is how a data frame looks like. It accepts vectors with any value. Data frames are quite similar to spreadsheets, for example in Excel or LibreOffice. dat1 &lt;- data.frame(cntry, z) dat1 ## cntry x y ## 1 Brasil 43 53 ## 2 Canada 50 60 ## 3 China 43 53 ## 4 Singapore 50 60 1.3.4.1 Selecting Elements Finally, we will learn how to select elements from objects. We will begin with vectors. a &lt;- c(1,2,3,4,5) b &lt;- a + 10 We can select elements in R with [ ]. a ## [1] 1 2 3 4 5 a[2] ## [1] 2 a[2:4] ## [1] 2 3 4 Like in many other programming languages, the colon : expresses a range. Here we select all values in the range from the second to the fourth entry. We can also select elements from a matrix. The , helps to distinguish between the two dimensions. The selection m[1,1] will return the first element of the first column. In a similar vein, the selection m[3,2] will return the third element of the second column. m &lt;- cbind(a,b) m[1,1] ## a ## 1 m[3,2] ## b ## 13 Finally, we select elements from a data frame. We create a variable ‘name’ with the names of Mark, Luise and Peter. The variable ‘bike’ contains their bikes: Mountainbike, Single Speed and Racing Bike. We capture the hours per week they ride on it: 4, 7, 8 and finally we bring all variables together in a common data frame dat2. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat2 &lt;- data.frame(name, bike, hours) In data frames, you can select elements with the operator $ and the name of the variable dat2$bike ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; You can of course still use the positions, too! dat2[,2] # gives you all entries for the second column ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; dat2[1,2] # gives you the first entry for the second column ## [1] &quot;Mountainbike&quot; Since just selecting might not always be enough, R allows us to select elements based on conditions. R comes with everything formal logic requires. is equal == is not != smaller &lt; larger &gt; smaller equal &lt;= larger equal =&gt; You can build more complex queries via AND &amp; OR | Let’s go: x &lt;- c(1,56,23,89,-3,5) y &lt;- c(24,78,32,27,8,1) x[x &gt;20] # greater as 20 ## [1] 56 23 89 x[x &gt;20 &amp; x !=89] # greater as 20 and unequal 89 ## [1] 56 23 x[x&gt;0 | x==-3] # x where x greater 0 oder x=-3 ## [1] 1 56 23 89 -3 5 y[x==1] # y where x=1 ## [1] 24 1.4 Readings for This Week Please read chapter 2 and chapter 3 in (Fogarty 2019). This is where we end for this week. I am looking forward to meeting you in the workshops on Tuesday 1pm–1.45pm in Law 2.27 and Friday online on Zoom! References "],["describing-data.html", "Chapter 2 Describing Data 2.1 Statistics to Summarise Data 2.2 Data Management with R 2.3 Describing Data Using R 2.4 Readings for This Week", " Chapter 2 Describing Data 2.1 Statistics to Summarise Data When we describe data, we typically have three questions: What do typical values look like? How clustered or dispersed are these values? In short: How are the values of a variable in a sample distributed? 2.1.1 Tables Data that comes in categories—be they nominal or ordinal—can be easily summarised. Simply list possible values for a variable, together with the number of observations for each value. Can be used to summarise distributions of one nominal, ordinal or interval variable. Intervals must be constructed for interval-level variables (e.g. age, income). Absolute frequencies record the actual number of observations for each value. Relative frequencies record the proportional distribution of observations. This is an example from our data: Frequency Percent Coffee 10 0.43 Other 4 0.17 Tea 9 0.39 This is how the data looks like for the question on how much you like summer. Frequency Percent Highest 15 0.68 4 4 0.18 3 3 0.14 2.1.2 Central Tendencies One of the first questions that comes to our minds when we consider a variable is: What is a typical value for it? Intuitively it makes sense to chose a value that shows the central tendency of a variable. There are three different statistics that can help summarise the distribution of scores by reporting the most typical or representative value of it. Mode The mode is the value that occurs most frequently. You can use it for all kind of categorical data. There can be more than one mode and under this circumstance you would speak of multimodal data. In case the answer categories can be ranked, the mode does not need to be near the centre of the distribution. Finally, the mode is resistant to outliers. For political scientists, the mode is a really important measure: We use it whenever we want to determine the winner of an election. When I asked you for your vote (if you had had the choice), this is how you responded. Frequency Percent Biden 21 0.91 Trump 2 0.09 The modal candidate is clearly Biden for your class. Two students would have voted for Trump. Median The median is the value that falls in the middle of an ordered sample. Obviously, the measure cannot be used for nominal variables—they can not be ranked. The median is the 50th percentile point. This means that when you count all cases, half of the sample will be smaller than the median and the other half is larger than the median. When the sample size is even, the median is the midpoint between the two measurements in the centre. By definition, the median is resistant to outliers: Irrespective of how small the smallest value or how large the largest one, the one value that splits the sample in half will remain always the same. These are your responses to the question how many hours you think you should study. I chose the dark blue line to indicate the median value of the data. Overall, we have 23 responses in the data. If the number of respondents is odd, the median is the average between two central responses that come into questions. Mean The mean is the sum of the observations divided by the number of observations \\[\\begin{equation*} \\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_N}{N} = \\frac{\\sum_{i=1}^{N}x_i}{N} \\end{equation*}\\] with \\(\\bar{x}\\) being the mean of variable \\(x\\); \\(\\sum\\) being the sum; \\(i\\) the individual cases (of x); \\(N\\) the number of cases. \\end{itemize} This all looks a bit fancy, but it is actually just a matter of understanding the notation. Conceptually, the mean is really straightforward—it is nothing different than the good old average. The mean has a couple of interesting characteristics. It is only applicable to interval variables. The mean is a good measure of central tendency for roughly symmetric distributions, but it can be misleading in skewed distributions. Most importantly, the mean is really susceptible to outliers. There is a nice physical interpretation of the mean: it is the centre of gravity of the observations. Take a look at your data. Your mean is slightly higher than your median. Why is that so? Basically, those who study more than 30h per week have a considerable influence on the mean and push the mean to the right of the median. Compare your data with the data from the previous cohort: Here the median and the mean are closer together. Why is this so? 2.1.3 Spread You now know how to chose a typical value that summarises your data. Next on the list is to characterise their spread. Are all values really close to one another? Are they far apart? Do many of them hang out on one side of the distribution, and are they far apart on the other side, i.e. is their distribution skewed? To measure all this, we will now take a look at different measures of spread. In essence, they are statistics that summarise the variation around our average value. We will consider four different measures that all build on each other. Range: Difference between two values, typically the minimum and the maximum. Deviation: Difference of a value from the mean. Variance: Squared difference of a value from the mean. Standard Deviation: Square root of the squared difference of a value from the mean. Range The range is the the distance between the largest and the smallest values, i.e. maximum–minimum. It will be distorted by extreme values. The interquartile range is another really important range. It covers the middle 50% of observations, so the range from the 25th percentile to the 75th percentile (lower quartile–upper quartile). Here you can a number of concepts that we covered working together to describe the data: The 25% quantile, the 50% quantile, the 75% quantile, the median, the mean and the interquartile range. Let us describe some of your data, here how old you are. How large would be the interquartile range? Hint: the value is 0.75. Deviation The deviation of any observation is its difference from the mean. \\[\\begin{equation*} (x-\\bar{x}) \\end{equation*}\\] What is the sum of deviations? Do the maths with a couple of numbers on a piece of paper. \\[\\begin{equation*} \\sum(x-\\bar{x}) = ? \\end{equation*}\\] Just in case some of you might still wonder about the \\(\\sum\\) at this point: I put together a small video as a refresher. And it also shows the answer to the issue with deviations. Apparently, the deviations always sum up to 0. The values keep canceling each other out. So what can we do? One solution would be to calculate the Mean Absolute Deviation \\(\\text{MAD}\\). \\[\\begin{equation*} \\text{MAD}=\\frac{\\sum(|x_i-\\bar{x}|)}{n} \\end{equation*}\\] In case you do not know the sign “\\(|\\)”: anything that is in between two “\\(|\\)” will always return its positive value. So \\(|5| = 5\\) and also \\(|-5| = 5\\). Let us take a look at your data, here how much you actually study and how much you think you should study. I am including again the interquartile range in red (which goes from where to where again?), and the mean in gold. When you do the maths you will find out that the \\(\\text{MAD}_{\\text{actual}} = 9.37\\) and \\(\\text{MAD}_{\\text{ideal}} = 10.03\\). Variance In practice, however, you will find that the Mean Absolute Deviation, is rarely used. Instead, you can often find the variance. It is basically the same as the \\(\\text{MAD}\\), but different. To avoid the canceling out, we will square the distance of each value to the mean. And for arcane statistical reasons that are irrelevant for this class, we now subtract 1 from the overall cases \\(n\\) in the denominator. \\[\\begin{equation*} s^2=\\frac{\\sum(x_i-\\bar{x})^2}{n-1} \\end{equation*}\\] The variance of the actual number of hours you study is \\(s^2_{\\text{actual}} = 118.88\\) and the variance for the number of hours you consider ideal is \\(s^2_{\\text{ideal}} = 153.93\\). Contrast the difference between the two variances to the difference in the two \\(\\text{MAD}\\)—it is much larger! The reason is simply that we are now taking the sum of the square of the distances and not just the absolute distances, which of course weighs much more for larger numbers. With the \\(\\text{MAD}\\) each data point contributes an equal share to the overall measure of spread. For the variance, this is no longer true. Those data that are further apart from the mean will drive the variance to a much larger degree than those data that are close to the mean. Standard Deviation While the variance is already a big step forward in measuring spread, it has one important drawback: It is quite abstract and really hard to interpret. Ideally, we would want to understand the measure for spread on the same metric as the data themselves. Doing so is straightforward. We simply take the square root of the variance—and the resulting standard deviation is in the metric of our data \\(x\\). \\[\\begin{equation*} s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}} \\end{equation*}\\] In our running example—the number of h you study and the number of h you think you should study—this is what we get. The standard deviation for the former is \\(s_{\\text{actual}} = 10.9\\) and the standard deviation for the latter is \\(s_{\\text{ideal}} = 12.41\\). Compare these values to the Mean Absolute Distances: \\(\\text{MAD}_{\\text{actual}} = 9.37\\) and \\(\\text{MAD}_{\\text{ideal}} = 10.03\\). They, too, are on the original scale. The standard deviation, however, is much more sensitive to outliers, which is a really desirable characteristic. 2.1.4 Ratios and Rates Finally, ratios and rates. Proportions We start with something really simple, the proportion. It is calculated as \\[\\begin{equation*} p = \\frac{f}{N} \\end{equation*}\\] with \\(p\\) being the proportion; \\(f\\) being the number of cases (frequency) in one category; \\(N\\) being the number of cases in all categories of the variable. The proportion is useful if we want to answer a question like: What is the proportion of students having tea for breakfast? \\[\\begin{equation*} p = \\frac{9}{23} = 0.39 \\end{equation*}\\] Percentages You like percentages better? Simply multiply your proportions with \\(100\\). \\[\\begin{equation*} P = \\left( \\frac{f}{N}\\right) 100 = \\left( \\frac{9}{23} \\right) 100 = 39.13\\% \\end{equation*}\\] Rates A rate is really useful if you want to express for example how often a proportion occurs in a given amount of time. We can calculate the rate as \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} \\end{equation*}\\] with \\(r\\) being the rate: the frequency per time in a certain set \\(f\\) being the number of cases (frequency) in one category \\(t\\) being the time under consideration \\(N\\) being the number of cases in all categories of the variable \\(u\\) being the unit under consideration Let us advance step by step and wrap our head around this with the help of an example. We want to understand how many of you actually bought a computer during 2020. This simplifies our formula a bit. \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\text{all students}} = \\frac{\\frac{12}{1}}{\\frac{23}{1}} = 0.52 \\end{equation*}\\] We can take this a little further and ask How many computers did 10 student buy in 2020?. To answer this question, we simply adapt the number of people in the unit and set \\(u=10\\). \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\frac{\\text{all students}}{\\text{unit of 10 students}}} = \\frac{\\frac{12}{1}}{\\frac{23}{10}} = 5.22 \\end{equation*}\\] So 10 students bought 5.22 computers in 2020. Growth The last thing we want to look at is growth. In particular in economics, growth has a really prominent role, and a lot of theory is built around all kinds of growth related to different cash flow: GDP, GDP per capita, return on investments to just name a few. But of course, growth can also happen in other areas like literacy rates in low-income countries, unemployment or votes. Growth can be expressed as a percentage change. \\[\\begin{equation*} G = \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\end{equation*}\\] with \\(G\\) being the growth rate in a variable from time 1 to time 2 \\(f_1\\) being the number of cases (frequency) at time \\(t_1\\) \\(f_2\\) being the number of cases (frequency) at time \\(t_2\\) Again, let us take a look at our own data to understand what is going on here. \\[\\begin{align*} G &amp;= \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\\\ &amp;= \\left(\\frac{\\text{Purchases in 2021} - \\text{Purchases in 2020}}{\\text{Purchases in 2020}}\\right) 100 \\\\ &amp;= \\left(\\frac{-8}{12}\\right) 100 = -66.67\\% \\end{align*}\\] Apparently, you were purchasing less computers during 2021, more specifically, the sales had a negative growth of \\(G = -66.67\\%\\). It could be that a number of you bought a new computer when you started your university career back in 2020. 2.2 Data Management with R To calculate all above, we first need to take a closer look at some data management this week. 2.2.1 Libraries Libraries are functions that do not ship on board your original R programme. Instead, you have to get them from them internet. Think of it like wanting to read a book. You first have to get it from a shop and bring it home, where you will add it to your book shelf in your own, personal library at home. In R, you can use the command install.packages() to download a package to your computer. If you execute the command, R might prompt you for a location—simply pick one that is close to you. Obviously, \"name_of_library\" is a placeholder here, so don’t try this at home with that particular code snippet, but replace it with the package you actually need. install.packages(&quot;name_of_library&quot;) You now have downloaded the programme to your computer. Or, in other words, you have added the book into your bookshelf. However, you are not sitting in your lounge chair with the book in your hand, yet. For that, you would still have to go to the library in your house and get the book. This is exactly what we will be doing now with the R package. We will collect the package from our library and load it into the active work space. library(&quot;name_of_library&quot;) 2.2.2 Setting the Working Directory R is quite stupid. It does not know where to look for files—be they R code or any other data—unless you really tell it where to look for a file. Typically, we will instruct R to make its home in the exact place where you save your main R script with a function called setwd(). As its argument, you provide the path you are working in. For me on my office machine, this is how it looks like. setwd(&#39;/Users/foo/PL9239 Intro_data_science/intro_data_science_homepage&#39;) Now, R will start looking for everything starting in that particular working directory. To see which working directory you are in, you can type getwd() This step might seem a bit minor and technical, but it is the nr.1 rookie mistake to forget setting your working directory properly. 2.2.3 Reading Data Working directories are particularly relevant if you want to read in data sets. Data mostly comes in two formats: comma separated values, or short .csv, and as a Microsoft Excel spreadsheet .xls. Most open data formats can be read in with a function that begins with read.foo. Of course, just reading it is not enough—you have to assign it to an object if you want to work with it, so we type for example: csvdata &lt;- read.csv(&quot;dataset1.csv&quot;) If we want to read in .xls data, we have to load a library that can help us with that. We will go with the readxl package. Again, we are assigning the data to an object so that we can call it later. library(readxl) xlsdata &lt;- read_excel(&quot;dataset1.xls&quot;, sheet=&#39;sheet_foo&#39;) 2.2.4 Saving Data You can also save data. Let us create a toy data set again. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat &lt;- data.frame(name, bike, hours) Now save it. save(dat, file = &quot;toydata.RData&quot;) To check the magic of this we remove the data set and then try to call the object. rm(dat) dat ## Error in eval(expr, envir, enclos): object &#39;dat&#39; not found Nothing there. load(&quot;toydata.RData&quot;) dat ## name bike hours ## 1 Mark Mountainbike 4 ## 2 Luise Single_Speed 7 ## 3 Peter Racing_Bike 8 Tada! Worth noting at this stage, that when you use the native R way of saving data, R saves your actual object, here the object dat. You can of course also save data as an Excel spreadsheet. library(writexl) write_xlsx(dat, &quot;toydata.xlsx&quot;) 2.3 Describing Data Using R Now, with a bit more of a background, we can calculate all of this week’s statistics. In R, this is really straightforward. 2.3.1 Working with Data Frames First, we load the data. Then we check which type it is: Apparently a data frame. dat &lt;- read.csv(&quot;preparing_data/data/class_survey/lecture_survey_22.csv&quot;) class(dat) ## [1] &quot;data.frame&quot; Data frames are particularly useful, because we can call the individual variables simply by adding the $ symbol and then calling the name of the variable we are interested in. For example, if we want calculate how much you spend per week partying, we could simply multiply how much you spend on one night out and multiply it with how often you go out per week. dat$spend * dat$partydays ## [1] 0.0 50.0 40.0 100.0 240.0 90.0 35.0 60.0 150.0 50.0 250.0 125.0 ## [13] 10.0 75.0 120.0 90.0 90.0 30.0 150.0 40.0 180.0 52.5 150.0 2.3.2 Central Tendencies You can take a look at the frequency of categorical data with the function table(). This is your data on how many days you are partying per week. table(dat$partydays) ## ## 1 2 3 4 5 6 ## 2 8 8 1 2 2 Indeed some party animals here. Now, what is the mode? We have to call a package for that function. library(DescTools) Mode(dat$partydays) ## [1] 2 3 ## attr(,&quot;freq&quot;) ## [1] 8 The output is a bit cryptic at first, but it tells us that the value 8 is the most frequent one. For the median and the mean, let us take a look at a continuous variable. For example, how much money you spend when you go out. You call the median with median() and the mean with mean(). median(dat$spend) ## [1] 30 mean(dat$spend) ## [1] 32.17391 Stop for a second and think about the results: What does the relationship between the median and the mean tell us here? Is the distribution really symmetric? Is the distribution maybe skewed? If so, how? 2.3.3 Spread The rest is a piece of cake just the like. You can calculate the variance using var() and the standard deviation with sd(). var(dat$spend) ## [1] 292.2184 sd(dat$spend) ## [1] 17.0944 2.4 Readings for This Week Please read chapter 7 in Fogarty (2019). Chapters 4 and 5 are a good idea, but you do not necessarily have to. References "],["visualising-data.html", "Chapter 3 Visualising Data 3.1 Communicating with Data Effectively 3.2 Plotting Data with R 3.3 Readings for This Week", " Chapter 3 Visualising Data 3.1 Communicating with Data Effectively Good data visualisations tell stories that do not need much explanation. Those who view the data can understand what insights you want to communicate from the data without necessarily using a lot of text (like this one) as their explanation. Viewing data visualisations is a lot of fun and a seemingly effortless way to explore and process the information in the data. The Internet is full with great examples at different levels of complexity. It is easy to get lost, take a look for example here, here or here. I am sure you can find many more interesting examples on the web. From a creator point of view, they allow to communicate a lot of information in a direct way. But it goes without saying that a well done data visualisation requires a bit of thinking. 3.1.1 Thinking About your Data Visualisation At its core, data visualisation is actually pretty simple. How can we possibly visualise the data in a way that it a) communicates the key information in our data well while b) transporting it so that it is easy to understand for the viewer? Whenever you want to visualise data you should probably first sit down and think about what it is that you want to tell the end user of your visualisation? What is your key message? The visualisation ideally expresses more content with less ink. Or in more mathy terms: \\[\\begin{equation} \\text{maximize} \\left( \\frac{\\text{content}}{\\text{ink}} \\right) \\end{equation}\\] This optimization problem comes with an important sub-constraint: The skill level of your potential audience. It will greatly determine the level of complexity that you can pitch. Who is your audience? Are they a lay audience without a lot of statistical training? So they would probably require a visualisation that allows for a more intuitive understanding. If the audience is more technically versed, your approach to data visualisation can be more sophisticated. Let us talk about colour. Think about the medium on which you want to use your figure. Is it made for publication in any printed format such as a report or a book? If so, be aware that not everybody is printing in colour, but usually in black and white. This means that you have to consider closely how light or how dark your colours are. For example, it might not be a good idea to use green and blue at the same time, simply because they are likely to have similar shadings. A colour that is too light might not be seen well on a white paper when printed. Of course, you are much more free in the choice of your colours when you your figures are for display on screens—like this homepage. Also think about the colourblind. Can they view your figures, too? Be careful when you use the spectrum from red to green. Ultimately, the choice of colours is a matter of preference and expression. If you feel arty, go wild! At the same time, picking the right colours is also a deep and awesome rabbit whole. Check this talk by the guys who invented the recent viridis colourscheme in case you want to get a taste. 3.1.2 Visualising ‘The Data’ When we are visualising data, we probably want to get an overview over it ourselves, so the first move is to take a look at ‘the data’ itself. How does it look like? How is it distributed? We learned that we have different data types, i.e. nominal, ordinal, interval and continuous data. 3.1.2.1 Nominal Data The best way to visualise nominal data is the bar chart. It gives you a nice and succinct overview over the data without distorting your perception. You can plot data vertically as we have done here in blue or you can plot them horizontally—here in red. Be aware of using of not using pie charts. While they are indeed quite popular, they are wrongly so. It seems that Microsoft made us believe that this is a good way to represent data when they introduced visualisations in Excel. You can plot them with R, but it will remind you that there is research showing that it distorts your perception of the data. You will systematically overestimate the relevance of small proportions of the data and think that these proportions are much larger than they really are. 3.1.2.2 Ordinal Data Ordinal data are nominal data that can be put into a logic order. Of course you can again use the barcharts like before. But you can also make use of the fact that now you actually have ranked data. Why not plotting them as a stacked barchart? 3.1.2.3 Interval or Continuous Data Interval data converges to continuous data in the extreme, so let us treat them at the same time here. The left figure charts data simply on the basis of their value in red. We run into a pretty straight forward problem: We cannot see how often we are actually seeing each observation. The solution is to stack values in case they occur more often, like in the purple figure to the right. Here we have a really nice overview of how often we observe each case. Plotting each individual data point is realistic for our case where we have just a few observations. However, this can become way too complex to process in case you have a larger number of values. In these circumstances you can visualise the distribution of the data with bins in which you collect your data. On the left in the blue histogram you can see how our data would look like if we split it up into eight groups—it results in a really nice pattern! On the right you can see how the data looks like if we just used four bins for it. 3.1.3 Visualising Typical Values and Spreads Now that we have a first overview over how the data usually looks like, we want to become a bit more systematic in our approach to getting to know the data. Remember that we talked about typical values and spread when we wanted to describe data? This is what we want to visualise now. When you have interval or continuous data, there are a number of typical questions that you might want to know from the data. What is a typical value for the data? How are the values of a variable in a sample distributed? How clustered or dispersed are these values? There is one visualisation that summarised this all: boxplots. They easily maximise the content-per-ink criterion. We can identify a number of things from this plot. The median of the distribution is a vertical bar and somewhere in the the colourful box in the middle. This box itself also expresses information: it covers the interquartile range, so the central 50% of the distribution are within the range of this box. If the distribution is symmetric, you can see that the median is in the centre of this box. In case the distribution is skewed, the median will push to one side or the other. The whiskers that you see give an impression of the magnitude of the overall spread of the data. Typically they are 1.5 times the interquartile range starting from the box. Data that goes beyond this range will be plotted with a circle. Let us take a look at how useful this really is in practice. In this first figure we are plotting the hours that you are actually studying in red and the hours that you think you should be studying in yellow. Let us begin with the median values. The median for the hours actually studied is 14, the median for the hours ideally studied is 20. You also get an impression of the magnitude of the interquartile range: it ranges from 5.5 to 22.5 for the top figure in red. For the lower figure, it begins at 9 and goes to a value to 32.5. Both distributions are a little skewed. On the top, the median is a bit to the left of the box. This means that a quarter of you study between the lower end of the box and the median value so somewhere between 5.5 and 14. The same interquartile range between the lower 25% quantile and the median is much larger for the hours that you think you should ideally be studying. The whiskers offer an impression of the data beyond the interquartile range. The smallest value for the hours that you actually study is 3 in the top figure. The maximum hours that any of you studies is 36. The whiskers are a little different for the hours that you think you should ideally be studying. On the left, they range from 4 hours and they grow all the way up to 40 hours. In the next figure, let us take a look at your age: I plotted your age between 18 and all the way up to 50 years and you can see it is not necessary to cover anymore space. Large parts of you—at least 75%—are either 19, 20 or 21 years. You can also see an outlier for one person who is 25 years old and another one with the age 43, both outside of the whiskers. They can be seen as clearly distinct circle. Lastly, you also offered information about a variable that I think is particularly interesting. How much money do you spend when you go out? Here we can see the information nicely summarised in a box plot. The median person spends 30 GBP when he or she goes out at night. A quarter of you spent between 22 GBP and 30 GBP. Another 25% spent between 30 GBP and 45 GBP when you go out. The whiskers give an indication of the whole spread of the distribution. There are people who typically spend 0 GBP when they go out. On the other side of the spectrum somebody seems to have quite deep pockets and is able of spending up to 75 GBP per night out. 3.2 Plotting Data with R Now it is your turn. When it comes to plotting, there is a package that has been put out a couple of years ago which is called ggplot2. On the web, you can find a lot of R visualisations that make use of it. For this class, however, we will stick to the basics—simply because I think it is good to first learn how to really control R from scratch. You can then always switch to higher level printing packages such as ggplot. Think of it as learning how to drive: In this module we will shift gears manually and you will find that switching to a car with automatic gears will not be any problem later. If you want to know more about ggplot2, this week’s reading of Chapter 8 in Fogarty (2019) is your friend. In all we do here, I am showing you some basic options. Look for the name of the function in the help tab to find out more details about the functions. 3.2.1 Basics in Plotting First, let us whip up some data. # This creates a vector with a sequence of integers from -10 to 10 with a distance of 1 x &lt;- seq(-10,10,1) x ## [1] -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 ## [20] 9 10 We calculate a second variable. a &lt;- 5 b &lt;- 3 c &lt;- .5 y &lt;- a + b*x + c*x^2 y ## [1] 25.0 18.5 13.0 8.5 5.0 2.5 1.0 0.5 1.0 2.5 5.0 8.5 13.0 18.5 25.0 ## [16] 32.5 41.0 50.5 61.0 72.5 85.0 And then we use the function plot() to chart the data. plot(x,y) Simple, right? R takes over a number of jobs for you, from selecting the right data range in both axes, to chosing the shape and colour of representations for the data etc.. To give you a first idea, this is what happens when you tweak a number of options. plot(x,y, cex = 3, # size of the data points pch = 16, # point character col = &#39;aquamarine3&#39;, # chose the colour. R defaults with inverted commas. las = 1, # rotates labels in y axis by 90 degrees, xlim = c(-15, 15), # This defines the limit of the x-axis you want to plot in main = &#39;My First Figure&#39; # Title ) You select the options by adding them with a comma. Check the help file in R-Studio for what is available. We will have a proper session on plotting bivariate data in a couple of weeks for all this, so do not worry about the details at this stage. The important message that needs to come across is that R will handle a lot for you—but the possibilities to adapt everything that you were seeing are literally endless. 3.2.2 Colours R is great for using all different kinds of colours. You can check the already pre-defined colours by calling the function colors(). Here, let us call the first 15 ones. colors()[1:15] ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; ## [5] &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; ## [9] &quot;aquamarine1&quot; &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; ## [13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; And you can even define your own colours using the rgb function. Cardiff University’s corporate identity colours for example can be generated like so. # Predefined Cardiff colours # primary cardiffred &lt;- rgb(211,55,74, maxColorValue = 255) cardiffblack &lt;- rgb(35,31,32, maxColorValue = 255) cardiffgrey &lt;- rgb(47,68,78, maxColorValue = 255) cardiffgold &lt;- rgb(189,158,94, maxColorValue = 255) # secondary cardiffblue &lt;- rgb(21,44,81, maxColorValue = 255) cardiffpurple1 &lt;- rgb(29,15,51, maxColorValue = 255) cardiffpurple2 &lt;- rgb(60,44,89, maxColorValue = 255) And then you can use them for plotting. a &lt;- seq(1,10,1) b &lt;- rep(1,10) # repeats the value &#39;1&#39; 10 times plot(a, b, col = c(cardiffred, cardiffgrey, cardiffgold), cex = 2, las = 1, pch = 15 ) The number of data points is larger than the number of colours. R will therefore cycle through the colours. 3.2.3 Visualising Nominal and Ordinal Data To visualise data in a way that we can chart them as a barplot, we first have to count them. How often do we observe each value? There is a really useful function for that in R, it is called table(). Just calling it without plotting is quite revealing. table.breakfast &lt;- table(breakfast) table.breakfast ## breakfast ## Coffee Other Tea ## 10 4 9 Just a quick fyi: You can use the function prop.table() to get percentages. Then round() will round the resulting values. prop.table.breakfast &lt;- prop.table(table.breakfast) prop.table.breakfast.rounded &lt;- round(prop.table.breakfast, digits = 2) prop.table.breakfast.rounded ## breakfast ## Coffee Other Tea ## 0.43 0.17 0.39 But we are here for visualising data. We need the object table.breakfast for our barplot. barplot(table.breakfast) Let us make this a bit more pretty. barplot(table.breakfast, ylab = &quot;Frequency&quot;, # Adds label to the y-axis col = cardiffred, # You know this already las = 1, # and this border = NA # Removes the small black border around the bars ) barplot(table.breakfast, xlab = &quot;Frequency&quot;, las = 1, col = cardiffgold, horiz = TRUE # plots bars horizontally instead of vertically ) 3.2.4 Visualising Continuous Data We also wanted to take a look at continuous data. Let us see how far you are living from the university for that matter. # R uses an algorithm to determine the number of bins hist(commute, col = cardiffgold, xlab = &quot;Minutes&quot;, ) # Here we control a bit more by ourselves hist(commute, breaks = 3, # This is a very rough break into three bins col = cardiffpurple1, xlab = &quot;Minutes&quot;, main =&#39;&#39;, # suppress the default main title border = &quot;white&quot; # border around the histograms--here set to white ) 3.2.5 Boxplots Finally, let us take a look at boxplots. The default function is called boxplot(). Calling it with a continuous variable such as spend as its argument will already generate a plot that has all the important information. boxplot(spend) But admittedly it is far from pleasent to look at the figure. Since we want that others pay attention to our message, let us make our visual communication more appealing. # here, we create a data object that is a list. Think of it as a super flexible data object. list.study &lt;- list(studyideal, studyperweek) boxplot(list.study, horizontal = TRUE, col = c(cardiffred, cardiffpurple1), # different colours for the plots axes = FALSE, # we are suppressing all axes boxwex = .6, # this is about the width of the interquartile range box ylim = c(0,40), # defines the limit of the x-axis (!). # It would have been the y-axis, but we rotated the figure. # This is a superconfusing command of the boxplot function # We just have to live with it... xlab = &#39;Hours&#39; # Label for the x-axis ) # Here, we are adding axes manually to have more control over them. axis(2, col = &quot;white&quot;, las = 1, at = c(1,2), # where to put the tickmarks lab =c(&quot;Ideally \\n Studied&quot;, &quot;Really \\n Studied&quot;) # Labels. &#39;\\n&#39; is a new line ) axis(1) This is it for this week. 3.3 Readings for This Week Don’t forget to check into the readings of this week which is chapter 8 in Fogarty (2019). As mentioned it is all about ggplot2—take it as a pointer to what is possible. We will not cover ggplot2 in this class, but you are of course free to go wild and become a proper ggplotter. References "],["useful-internet-resources.html", "Chapter 4 Useful Internet Resources 4.1 Homepages 4.2 Online Books", " Chapter 4 Useful Internet Resources 4.1 Homepages Quick-R has been a go-to reference when you look up simple examples for quite some time. It is a really helpful source. This is a nice overview over all kinds of PolSci and IR data sets that are out there. Data Science is fun! Why not starting your own project on a topic that catches your interest? You could share your insights as a homepage. Lillian Petersen could be a great inspiration. The project Seeing Theory is a visually really beautiful and interactive take on what we are doing in this class. Note: The authors are undergrads from Brown University. 4.2 Online Books Here is another good book that covers all we do in depth: Andrew Gelman, Jennifer Hill and Aki Vehtari (2020): “Regression and Other Stories.” You can download it here. (all legal…) The LOST homepage is a bit of a repository for all kinds of models. It not only has code in R, but also a couple of other relevant statistical softwares, such as Julia or Python. If you want to dig a little deeper on the causal aspects, you can check out this beautiful online book titled Causal Inference: The Mix Tape. Also available as a print version of course… If you want to go even further down the causal path, this is another module at Mannheim University that also comes with a teaching homepage. Interested in Data Visualization? This is a really great book on the topic from Jonathan Schwabish. If you are keen to critically reflecting on what you are doing when you are analysing data, go and read Critical Thinking from Tom Chatfield. He also has a great video where he explains the core ideas. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
