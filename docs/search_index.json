[["index.html", "PL 9239 Introduction to Data Science for Politics and IR Welcome! Data to Answer Your Questions Your Weekly Data Workout The Module Webpage Getting in Touch", " PL 9239 Introduction to Data Science for Politics and IR Christian Arnold, Cardiff University 2021-04-26 Welcome! Welcome to this module. In PL 9239 you will learn how to make the most of data. How can you handle and analyse data efficiently? How to communicate and visualise results? While everything we do here has of course a wider appeal, we will focus in particular on our area: How can we use data to study politics and international relations? Data to Answer Your Questions Today, data are everywhere. Data are not only collected for scientific studies. Governments, companies and non-profit organisations are amassing and increasingly sharing data at an unprecedented level. Data on their own can actually be a bit boring. Few people might enjoy to just browse numbers. Everything changes, however, if we can use data to answer a question we are interested in. This is why we we will build the module around three scenarios. Scenario 1: The Data Report Whatever you actually want to do with data, the first step is always getting a good oversight over the data itself. You will have to open the data and describe its most characteristic features. Often, you want to communicate these results to other colleagues in a data report. And all of your work with the data is ideally documented in a transparent way so that others—or even your future self—can easily understand what you were doing. This first scenario might actually be quite typical for your first years at work. Scenario 2: Knowing It All By Knowing A Few For the second scenario, imagine you are working for an NGO that helps implement voting observation missions in fragile democracies. Even though you are partnering up with local civil society organisations, you cannot send election observers too all polling stations during a mission. Luckily, this is not strictly necessary: As long as you properly randomise the polling stations you want to observe, you can infer the real, countrywide numbers on the basis of just a few hundred few hundred observations. In this class, we will learn why this is working and how you can do inference yourself. Scenario 3: Did the New Policy Have an Effect? Finally, take another example: A new law introduced mandatory environmental protection for farm land. A year later, biologists measure that harmful substances in ground water are down by 20%. Was this the effect of the policy? Or was it maybe just a particularly rainy year that washed all pollutants away? Together we will learn how we can find answers by asking our data the right kind of questions. The module is particularly relevant if you are planning to undertake a dissertation, since it will help you find the right data and how to work with it. Data literacy is not only a really useful skill to have as a modern citizen, it is also really valuable when you are looking for a job. Your Weekly Data Workout In this module, we will expose you to core concepts prior to the actual class. This frees up large chunks of time during the class that we can spend on activities where you typically need the most help, such as application of basic material and engaging in deeper discussions and creative work with it. A year ago, all this would have been quite revolutionary. But now, 12 months into the pandemic, all of this feels actually quite intuitive. Let me introduce all of our activities one by one. 1 Work Through This Homepage Your first port of call in every week will be the respective chapter in this module homepage. This site is a mixture between a website and a teaching book. It will walk you through the content of each week and you can find the most important content explained here. The website allows you to take your own pace and revisit the parts that you find most challenging as many times as you want and at the pace that you feel comfortable with. The respective content of the week will be uploaded on Monday morning, latest. I expect that you work through that home page on your own over the course of the week. In case you have questions, please reach out to me via email or book time in the office hour. We will also use the module cafe every second week for answering any of your questions. 2 Apply Your Knowledge in Our Workshops While the homepage is focusing on knowledge acquisition, our workshops on Tuesdays between 12:00 and 2:00 o’clock will focus on skills development. Learning how to do data analysis requires practice. And practice means failing a lot and learning from the own mistakes. The workshops on Tuesdays are meant to serve as a place where we can jointly explore how to properly manage, analyse and visualise data. The workshops will always cover the content of the past week, e.g. in the workshop in week 2 we will be covering the content of the homepage from week 1. Please make sure that you come prepared. Workshops will run from week 1 until week 11. Over the course of these two hours, you will be working online in small groups. For each week, I will prepare a lab sheet that you will jointly solve. Do not worry in case you struggle: I will be always around to help. Without participating in these workshops, the assignments will be really challenging. I strongly recommend you do not miss a single session. 3 Bring Your Questions to the Module Cafés We will have bi-weekly module cafes where you can discuss any additional questions that you might have. In addition, these module cafes will also serve to answer any question regarding the assignments. 4 Read Guideance on Planning and Structuring To guide you through all the hoops, I will also stay in touch with you with am email every Friday to make sure that you are on board. It is vital that you check your university email regularly, since we will only communicate with you through this address. The Module Webpage Let us take a closer look at how this website is working. This space is going to be the main learning tool for this term’s Introduction to Data Science for Politics and IR. Obviously, I will guide you through the material on this web-page via text. Think of it as a replacement for the lecture slides that you might find in other contexts: A place to find the core concepts and central ideas of this class. But not all is text. Sometimes, nothing beats someone actually explaining you core ideas. For that purpose, I will record small videos and embedd them here. The best thing about videos: You can watch them as many times as you want… Finally, this format is also really handy, because we will be able to take a look at R code. R is a computer language invented for data analysis, and you will learn how to use it in PL 9239. This is how it looks like when we will be covering R code. # All text in a line after the hashtag is a comment. # Here we assign some variables a &lt;- 1 b &lt;- 2 # This is how to add them and assign the result to another variable c &lt;- a+b # And finally we print the result cat(c) ## 3 Last but not least, each week will have some readings that will reflect the core content of the respective week from yet another perspective. I will add pointers to the readings whenever appropriate. You can find all information about the readings in the bibliography. Getting in Touch If you have questions or need any kind of help, feel free to get in touch. The whole purpose of our weekly workshops and our bi-weekly module cafe is to provide you with ample space for your questions. Make use of this! You can always drop me an email. I will respond at the end of the working day. You can also book time in my office hours Tuesday 3-4pm; Wednesday 3.15-4.15pm; Friday 5.15-6.15pm. ## The following object is masked from package:datasets: ## ## sleep "],["tools-for-working-with-data.html", "Chapter 1 Tools for Working with Data 1.1 What Is In a Data Spreadsheet? 1.2 Key Concepts 1.3 Meet: R 1.4 Readings for This Week", " Chapter 1 Tools for Working with Data This week we will get to know the basic tools we need. 1.1 What Is In a Data Spreadsheet? When we work with data, the first step is always to get an overview of the data—be it in academia, for a company or in government. The whole first part of this module is dedicated to teaching you how to understand your data in a quick and efficient way. We will learn a couple of important things. How to access data. How to get a quick overview over the data. How to describe data and extract their core characteristics. How to communicate and visualise results. How to manage everything efficiently. To give you an idea how and why even these basic skills matter in real life, I talked to Dr Sebastian Sternberg. He has a degree in politics and works as a data scientist with KPMG at the moment. In the interview, he was so kind to offer us insights into how he is working with data and how relevant data reports are in his daily work. 1.2 Key Concepts Let as begin with some key definitions. 1.2.1 What is a Statistic? Statistics summarise large amounts of numerical data. Statistics are really useful if one wants to get a good overview over data. A statistic is a characteristic of a sample. Imagine we make a number of observations and put in numbers what we see. When we calculate a statistic on this data, the statistic is able to describe the sample we collected. If we collect a different sample, the statistic is very likely to have slightly different values. The goal of statistical methods is to make inference about a population based on information from a sample of that population. Often, we might be interested in more than just the data at hand. By drawing a sample, we hope to generalise beyond our sample and learn something about the the overall population. For example, by asking a few hundred Welsh voters about their voting intentions we aim at saying something about the voting intention of all voters in Wales. To estimate parameters, we use statistical methods. Estimating a parameter means that you need to be able to say something about a certain parameter on the basis of a couple of data points that you collected. We will be learning how to use the correct statistics to infer what we believe is the most likely value for a certain parameter. In the voting example, we might be interested to understand what share of voters who would cast their vote for Plaid Cymru for example. Statistics can separate the probable from the possible. When we collect data, the data can have a range of different values. Of course, not all possible data points are equally likely. The beauty of statistics is that they can be used to tell us which values we can expect to see more often and which ones least often. For example, voters might be able to cast their vote for a whole range of parties—e.g. Labour, Conservatives, Plaid Cymru or the Welsh Nation Party. However, voting for these parties is not necessarily equally likely. It is reasonable to expect that, say, the Conservatives will receive more votes than the Welsh Nation Party. 1.2.2 Some More Definitions Some more definitions that a really useful for our module. It would not hurt if you could learn them by heart. Population: The full set of cases about which we want to generalise. Sample: A subset of the population. Variable: Logical set of attributes (characteristics) of an object (person, thing, etc.) that can vary across a range. Parameter: A characteristic of a population; usually unknown. Descriptive Statistics: Statistics that summarise the distribution of the values of variables in a sample. Inferential Statistics: The use of statistics to make inferences about a larger population based on data collected from a sample. 1.2.3 Types of Data Levels Example from our Survey Real World Example Measurement Possible Operations Nominal Tea or coffee? Gender, religion Categories Frequencies Ordinal Do you like summer? Social class, attitudes Categories and ranking Frequencies and ranking Interval How many hours do you study? Age, income All above and distance All above and addition and subtraction Remember that you were filling out the survey just before term? Let us go and take a look at some examples of your data to better understand the different types of data that are out there. Nominal Data What do you prefer to drink in the morning? The variable encodes three different answer categories. These categories cannot be ranked, obviously. These are your responses: Ordinal Data How much do you like summer? You were given a scale that ranged from 1 to 5. The answers to this question can be ranked. However, it is not necessarily true that the distances between the answer categories is always the same. Now look at your responses: Here comes the sun! Interval Data Finally, let us consider some interval data. How many hours do you study per week? This data provides full hours. The data is discrete, its values can be ranked, and the distance between any two neighbouring categories is always the same. 1.3 Meet: R Now that we have a bit of an overview how data can look like, let us take a look at the main tool that we will use to manage, analyse and visualise data. We will use a programming language for this module that is called R. It is free and open source, so you can install it easily on your computers. It is also very powerful which means that all the effort that you are investing in these 10 weeks of the term to learn it will heavily pay-off when you are analysing data in the future—be it for research or in any other professional context. And ‘paying-off’ is meant quite literally here. R is a really valuable skill set to have on your CV and is certain to boost your employability quite a bit. If you want to know more about R, where its coming from and how it all developed, check the Wikipedia Page as a start. Also, feel free to go wild on the homepage of the R Project itself or in any other of your favourite corners of the internet. Beware: The rabbit hole is quite deep. #nerdalert 1.3.1 Install R Installing R is pretty straightforward: Go and visit the homepage of the The Comprehensive R Archive Network where you can find the latest version of R. At the top of the page, you can chose between your operating system: 1.3.2 Install R Studio Now that you have installed the programming language, let’s go and get a nice interface that actually helps us get our work down. R-Studio is a programme that makes it much more easy to write and execute R code. Go and get the free R-Studio desktop version. Install the version that suits your operating system. Nota bene: Make sure you first install R and then R-Studio. 1.3.3 How to Work with R Here is a quick video in which I show you around. You can also take a look at Chapter 2 in Fogarty (2019) for a lot of helpful details. 1.3.4 First Steps in R Now that you know how R-Studio looks like and how to use it, let’s go and try it out. As you saw, in the console tab you can run commands directly. But it is better practice to type them in an R script and send them. Every line of the text editor can be sent using Str + Enter in Windows and Cmd + Enter in MacOS. Objects R can keep several objects in memory at the same time. To distinguish them, object have names. Objects are assigned with an arrow like this: &lt;- Let us assign some values to objects. a &lt;- 5 b &lt;- 6 c &lt;- a * b Objects can be called using their name. Here on this homepage, you will see a second grey block that will give you the output. If you type all up in R Studio, you will find all R related output in the console. a ## [1] 5 b ## [1] 6 c ## [1] 30 We can combine several values with the function c(). Functions are really useful. There are basically three elements to a function. * First of call, they have a name—here in this case it is the letter c which stands for concatenate. * Then, functions always need one more more inputs. A function receives its inputs in the brackets (). * If you call a function and provide it with its correct inputs, it will do its thing and return the output that you asked for. a &lt;- c(1,2,3) a The object a is not a scalar any more. It is now a vector that has three elements to it, the numbers 1, 2 and 3. You can of course also assign more than just numbers. We can assign strings—here a couple of three digit country codes. cntry &lt;- c(&quot;BRA&quot;, &quot;GER&quot;, &quot;FRA&quot;, &quot;NLD&quot;) cntry ## [1] &quot;BRA&quot; &quot;GER&quot; &quot;FRA&quot; &quot;NLD&quot; If you want to see all the objects that we have in the workspace so far, try the function ls(). ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;cntry&quot; You want to remove an object? Then use the function rm() like so. rm(cntry) See, the object cntry is not in the workspace any more. ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; R Data Types In R, there can be different types of objects. Some can only take specific types of data. Scalar: numbers, characters, logical values Vector: sets of scalars Matrix: two-dimensional set of scalars of same type Data frame: Collections of vectors of (possibly) different types but with same length Let us begin from scratch with am empty workspace. To delete everything, we will nest two of the functions above. rm(list=ls()) First we assign some scalars. # scalar a &lt;- 43 b &lt;- a + 7 a ## [1] 43 b ## [1] 50 And here we go with some vectors. x &lt;- c(a,b,a,b) y &lt;- x + 10 cntry &lt;- c(&#39;Brasil&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Singapore&#39;) x ## [1] 43 50 43 50 y ## [1] 53 60 53 60 cntry ## [1] &quot;Brasil&quot; &quot;Canada&quot; &quot;China&quot; &quot;Singapore&quot; Something interesting just happened here: You see the object x which is a vector? R added 10 to each of the scalars in x when calculating y. In programming you call this broadcasting. Pretty nifty! Next, we build a matrix, for example by binding two columns with the function cbind() or two rows with the function rbind(). z &lt;-cbind(x,y) z2 &lt;-rbind(x,y) z ## x y ## [1,] 43 53 ## [2,] 50 60 ## [3,] 43 53 ## [4,] 50 60 z2 ## [,1] [,2] [,3] [,4] ## x 43 50 43 50 ## y 53 60 53 60 This is how a data frame looks like. It accepts vectors with any value. Data frames are quite similar to spreadsheets, for example in Excel or LibreOffice. dat1 &lt;- data.frame(cntry, z) dat1 ## cntry x y ## 1 Brasil 43 53 ## 2 Canada 50 60 ## 3 China 43 53 ## 4 Singapore 50 60 1.3.4.1 Selecting Elements Finally, we will learn how to select elements from objects. We will begin with vectors. a &lt;- c(1,2,3,4,5) b &lt;- a + 10 We can select elements in R with [ ]. a ## [1] 1 2 3 4 5 a[2] ## [1] 2 a[2:4] ## [1] 2 3 4 Like in many other programming languages, the colon : expresses a range. Here we select all values in the range from the second to the fourth entry. We can also select elements from a matrix. The , helps to distinguish between the two dimensions. The selection m[1,1] will return the first element of the first column. In a similar vein, the selection m[3,2] will return the third element of the second column. m &lt;- cbind(a,b) m[1,1] ## a ## 1 m[3,2] ## b ## 13 Finally, we select elements from a data frame. We create a variable ‘name’ with the names of Mark, Luise and Peter. The variable ‘bike’ contains their bikes: Mountainbike, Single Speed and Racing Bike. We capture the hours per week they ride on it: 4, 7, 8 and finally we bring all variables together in a common data frame dat2. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat2 &lt;- data.frame(name, bike, hours) In data frames, you can select elements with the operator $ and the name of the variable dat2$bike ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; You can of course still use the positions, too! dat2[,2] # gives you all entries for the second column ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; dat2[1,2] # gives you the first entry for the second column ## [1] &quot;Mountainbike&quot; Since just selecting might not always be enough, R allows us to select elements based on conditions. R comes with everything formal logic requires. is equal == is not != smaller &lt; larger &gt; smaller equal &lt;= larger equal =&gt; You can build more complex queries via AND &amp; OR | Let’s go: x &lt;- c(1,56,23,89,-3,5) y &lt;- c(24,78,32,27,8,1) x[x &gt;20] # greater as 20 ## [1] 56 23 89 x[x &gt;20 &amp; x !=89] # greater as 20 and unequal 89 ## [1] 56 23 x[x&gt;0 | x==-3] # x where x greater 0 oder x=-3 ## [1] 1 56 23 89 -3 5 y[x==1] # y where x=1 ## [1] 24 1.4 Readings for This Week Please read chapter 2 and chapter 3 in (Fogarty 2019). This is where we end for this week. I am looking forward to meeting you in the workshop on Tuesday 12pm–2pm! "],["describing-data.html", "Chapter 2 Describing Data 2.1 Statistics to Summarise Data 2.2 Data Management with R 2.3 Describing Data Using R", " Chapter 2 Describing Data 2.1 Statistics to Summarise Data When we describe data, we typically have three questions: How are the values of a variable in a sample distributed? What do typical values look like? How clustered or dispersed are these values? 2.1.1 Tables Data that comes in categories—be they nominal or ordinal—can be easily summarised. Simply list possible values for a variable, together with the number of observations for each value. Can be used to summarise distributions of one nominal, ordinal or interval variable. Intervals must be constructed for interval-level variables (e.g. age, income). Absolute frequencies record the actual number of observations for each value. Relative frequencies record the proportional distribution of observations. This is an example from our data: Frequency Percent Coffee 7 0.44 Other 4 0.25 Tea 5 0.31 This is how the data looks like for the question on how much you like summer. Frequency Percent Highest 14 0.88 4 2 0.12 2.1.2 Central Tendencies One of the first questions that comes to our minds when we consider a variable is: What is a typical value for it? Intuitively it makes sense to chose a value that shows the central tendency of a variable. There are three different statistics that can help summarise the distribution of scores by reporting the most typical or representative value of it. Mode The mode is the value that occurs most frequently. You can use it for all kind of categorical data. There can be more than one mode and under this circumstance you would speak of multimodal data. In case the answer categories can be ranked, the mode does not need to be near the centre of the distribution. Finally, the mode is resistant to outliers. For political scientists, the mode is a really important measure: We use it whenever we want to determine the winner of an election. When I asked you for your vote (if you had had the choice), this is how you responded. Votes Biden 15 NA 1 The modal candidate is clearly Biden for your class. Nobody would have voted for Trump. One response is missing. Median The median is the value that falls in the middle of an ordered sample. Obviously, the measure cannot be used for nominal variables—they can not be ranked. The median is the 50th percentile point. This means that when you count all cases, half of the sample will be smaller than the median and the other half is larger than the median. When the sample size is even, the median is the midpoint between the two measurements in the centre. By definition, the median is resistant to outliers: Irrespective of how small the smallest value or how large the largest one, the one value that splits the sample in half will remain always the same. These are your responses to the question how many hours you think you should study. I chose the dark blue line to indicate the median value of the data. Overall, we have 16 responses in the data. Since the number of respondents is odd, the median is the average between the 8th and the 9th response. Mean The mean is the sum of the observations divided by the number of observations \\[\\begin{equation*} \\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_N}{N} = \\frac{\\sum_{i=1}^{N}x_i}{N} \\end{equation*}\\] with \\(\\bar{x}\\) being the mean of variable \\(x\\); \\(\\sum\\) being the sum; \\(i\\) the individual cases (of x); \\(N\\) the number of cases. \\end{itemize} This all looks a bit fancy, but it is actually just a matter of understanding the notation. Conceptually, the mean is really straightforward—it is nothing different than the good old average. The mean has a couple of interesting characteristics. It is only applicable to interval variables. The mean is a good measure of central tendency for roughly symmetric distributions, but it can be misleading in skewed distributions. Most importantly, the mean is really susceptible to outliers. There is a nice physical interpretation of the mean: it is the centre of gravity of the observations. Take a look at your data. Your mean is slightly lower than your median. Why is that so? Basically, those who study less than 10h per week have a considerable influence on the mean and push the mean to the left of the median. 2.1.3 Spread You now know how to chose a typical value that summarises your data. Next on the list is to characterise their spread. Are all values really close to one another? Are they far apart? Do many of them hang out on one side of the distribution, and are they far apart on the other side, i.e. is their distribution skewed? To measure all this, we will now take a look at different measures of spread. In essence, they are statistics that summarise the variation around our average value. We will consider four different measures that all build on each other. Range: Difference between two values, typically the minimum and the maximum. Deviation: Difference of a value from the mean. Variance: Squared difference of a value from the mean. Standard Deviation: Square root of the squared difference of a value from the mean. Range The range is the the distance between the largest and the smallest values, i.e. maximum–minimum. It will be distorted by extreme values. The interquartile range is another really important range. It covers the middle 50% of observations, so the range from the 25th percentile to the 75th percentile (lower quartile–upper quartile). Here you can a number of concepts that we covered working together to describe the data: The 25% quantile, the 50% quantile, the 75% quantile, the median, the mean and the interquartile range. Let us describe some of your data, here how old you are. How large would be the interquartile range? Hint: the value is 1. Deviation The deviation of any observation is its difference from the mean. \\[\\begin{equation*} (x-\\bar{x}) \\end{equation*}\\] What is the sum of deviations? Do the maths with a couple of numbers on a piece of paper. \\[\\begin{equation*} \\sum(x-\\bar{x}) = ? \\end{equation*}\\] You will find that it is always 0—simply because the values keep cancelling each other out. So what can we do? One solution would be to calculate the Mean Absolute Deviation \\(\\text{MAD}\\). \\[\\begin{equation*} \\text{MAD}=\\frac{\\sum(|x_i-\\bar{x}|)}{n} \\end{equation*}\\] In case you do not know the sign “\\(|\\)”: anything that is in between two “\\(|\\)” will always return its positive value. So \\(|5| = 5\\) and also \\(|-5| = 5\\). Let us take a look at your data, here how much you actually study and how much you think you should study. I am including again the interquartile range in red (which goes from where to where again?), and the mean in gold. When you do the maths you will find out that the \\(\\text{MAD}_{\\text{actual}} = 5.19\\) and \\(\\text{MAD}_{\\text{ideal}} = 6.12\\) Variance In practice, however, you will find that the Mean Absolute Deviation, is rarely used. Instead, you can often find the variance. It is basically the same as the \\(\\text{MAD}\\), but different. To avoid the canceling out, we will square the distance of each value to the mean. And for arcane statistical reasons that irrelevant for this class, we now subtract 1 from the overall cases \\(n\\) in the denominator. \\[\\begin{equation*} s^2=\\frac{\\sum(x_i-\\bar{x})^2}{n-1} \\end{equation*}\\] With the study data, the variance of the actual number of hours you study is \\(s^2_{\\text{actual}} = 35\\) and the variance for the number of hours you consider ideal is \\(s^2_{\\text{ideal}} = 53.67\\). Contrast the difference between the two variances to the difference in the two \\(\\text{MAD}\\)—it is much larger! The reason is simply that we are now taking the sum of the square of the distances and not just that absolute distances, which of course weighs much more for larger numbers. With the \\(\\text{MAD}\\) each data point contributes an equal share to the overall measure of spread. For the variance, this is no longer true. Those data that are further apart from the mean will drive the variance to a much larger degree than those data that are close to the mean. Standard Deviation While the variance is already a big step forward in measuring spread, it has one important drawback: It is quite abstract and really hard to interpret. Ideally, we would want to understand the measure for spread on the same metric as the data themselves. Doing so is straightforward. We simply take the square root of the variance—and the resulting standard deviation is in the metric of our data \\(x\\). \\[\\begin{equation*} s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}} \\end{equation*}\\] In our running example—the number of h you study and the number of h you think you should study—this is what we get. The standard deviation for the former is \\(s_{\\text{actual}} = 5.92\\) and the standard deviation for the latter is \\(s_{\\text{ideal}} = 7.33\\). Compare these values to the Mean Absolute Distances: \\(\\text{MAD}_{\\text{actual}} = 5.19\\) and \\(\\text{MAD}_{\\text{ideal}} = 6.12\\). They, too, are on the original scale. The standard deviation, however, is much more sensitive to outliers, which is a really desirable characteristic. 2.1.4 Ratios and Rates Finally, ratios and rates. Proportions We start with something really simply, the proportion. It is calculated as \\[\\begin{equation*} p = \\frac{f}{N} \\end{equation*}\\] With * \\(p\\) being the proportion; * \\(f\\) being the number of cases (frequency) in one category; * \\(N\\) being the number of cases in all categories of the variable. The proportion is useful if we want to answer a question like: What is the proportion of students having tea for breakfast? \\[\\begin{equation*} p = \\frac{5}{16} = 0.31 \\end{equation*}\\] Percentages You like percentages better? Simply multiply your proportions with \\(100\\). \\[\\begin{equation*} P = \\left( \\frac{f}{N}\\right) 100 = \\left( \\frac{5}{16} \\right) 100 = 31.25\\% \\end{equation*}\\] Rates A rate is really useful if you want to express for example how often a proportion occurs in a given amount of time. We can calculate the rate as \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} \\end{equation*}\\] with * \\(r\\) being the rate: the frequency per time in a certain set * \\(f\\) being the number of cases (frequency) in one category * \\(t\\) being the time under consideration * \\(N\\) being the number of cases in all categories of the variable * \\(u\\) being the unit under consideration Let us advance step by step and wrap our head around this with the help of an example. We want to understand how many of you actually bought a computer during 2019. This simplifies our formula a bit. \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\text{all students}} = \\frac{\\frac{6}{1}}{\\frac{16}{1}} = 0.375 \\end{equation*}\\] We can take this a little further and ask How many computers did 10 student buy in 2019?. To answer this question, we simply adapt the number of people in the unit and set \\(u=10\\). \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\frac{\\text{all students}}{\\text{unit of 10 students}}} = \\frac{\\frac{6}{1}}{\\frac{16}{10}} = 3.75 \\end{equation*}\\] So 10 students bought 3.75 computers in 2019. Growth The last thing we want to look at is how much growth. In particular in economics, growth has a really prominent role, and a lot of theory is built around all kinds of growth related to different cash flow: GDP, GDP per capita, return on investments to just name a few. But of course, growth can also happen in other areas like literacy rates in development countries, unemployment or votes. Growth can be expressed as a percentage change. \\[\\begin{equation*} G = \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\end{equation*}\\] with * \\(G\\) being the growth rate in a variable from time 1 to time 2 * \\(f_1\\) being the number of cases (frequency) at time \\(t_1\\) * \\(f_2\\) being the number of cases (frequency) at time \\(t_2\\) Again, let us take a look at our own data to understand what is going on here. \\[\\begin{align*} G &amp;= \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\\\ &amp;= \\left(\\frac{\\text{Purchases in 2020} - \\text{Purchases in 2019}}{\\text{Purchases in 2019}}\\right) 100 \\\\ &amp;= \\left(\\frac{-2}{6}\\right) 100 = -33.33\\% \\end{align*}\\] Apparently, you were purchasing less computers during the pandemic year, more specifically, the sales had a negative growth of \\(G = -33.33\\%\\). This could be related to the fact that a number of you bought a new computer when you started your university career back in 2019. 2.2 Data Management with R To calculate all above, we first need to take a closer look at some data management this week. 2.2.1 Libraries Libraries are functions that do not ship on board your original R programme. Instead, you have to get them from them internet. Think of it like wanting to read a book. You first have to get it from a shop and bring it home, where you will add it to your book shelf. In R, you can use the command install.packages() to download a package to your computer. If you execute the command, R might prompt you for a location—simply pick one that is close to you. Obviously, \"name_of_library\" is a placeholder here, so don’t try this at home with that particular code snippet and replace it with the package you actually need. install.packages(&quot;name_of_library&quot;) Now you have downloaded the programme to your computer. Or, in other words, you have added the book into your bookshelf. However, you are not sitting in your lounge chair with the book in your hand, yet. For that, you would still have to go to the library in your house and get the book. This is exactly what we will be doing now with the R package. We will collect the package from our library and load it into the active workingspace. library(&quot;name_of_library&quot;) 2.2.2 Setting the Working Directory R is quite stupid. It does not know where to look for files—be they R code or any other data—unless you really tell it where to look for a file. Typically, we will instruct R to make its home in the exact place where you save your main R script with a function called setwd(). As its argument, you provide the path you are working in. For me on my office machine, this is how it looks like. setwd(&#39;/Users/foo/PL9239 Intro_data_science/intro_data_science_homepage&#39;) Now, R will start looking for everything starting in that particular working directory. To see which working directory you are in, you can type getwd() 2.2.3 Reading Data Working directories are particularly relevant if you want to read in data sets. Data mostly comes in two formats: comma separated values, or short .csv, and as a Microsoft Excel spreadsheet .xls. Most open data formats can be read in with a function that begins with read.foo. Of course, just reading it is not enough—you have to assign it to an object if you want to work with it. csvdata &lt;- read.csv(&quot;dataset1.csv&quot;) If we want to read in .xls data, we have to load a library that can help us with that. We will go with the readxl package. Again, we are assigning the data to an object so that we can call it later. library(readxl) xlsdata &lt;- read_excel(&quot;dataset1.xls&quot;, sheet=&#39;sheet_foo&#39;) 2.2.4 Saving Data You can also save data. Let us create a toy data set again. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat &lt;- data.frame(name, bike, hours) Now save it. save(dat, file = &quot;toydata.RData&quot;) To check the magic of this we remove the data set and then try to call the object. rm(dat) dat ## Error in eval(expr, envir, enclos): object &#39;dat&#39; not found Nothing there. load(&quot;toydata.RData&quot;) dat ## name bike hours ## 1 Mark Mountainbike 4 ## 2 Luise Single_Speed 7 ## 3 Peter Racing_Bike 8 Tada! Worth noting at this stage, that when you use the native R way of saving data, R saves your actual object, here the object dat. 2.3 Describing Data Using R Now, with a bit more of a background, we can calculate all of this week’s statistics. In R, this is really straightforward. Let us cover the commands for central tendencies and spread for now, since they are built into R. 2.3.1 Central Tendencies You can take a look at the frequency of categorical data with the function table(). This is your data on how many data you are partying per week—of course, out of pandemic times… table(partydays) ## partydays ## 1 2 3 4 5 6 ## 2 2 5 2 3 2 Quite a number of party animals here. Now, what is the mode? We have to call a package for that function. library(DescTools) Mode(partydays) ## [1] 3 ## attr(,&quot;freq&quot;) ## [1] 5 The output is a bit cryptic at first, but it tells us that the value \\(3\\) is the most frequent one and that \\(5\\) individuals chose it. For the median and the mean, let us take a look at a continuous variable. For example, how much money you spend when you go out. You call the median with median() and the mean with mean(). median(spend) ## [1] 20 mean(spend) ## [1] 20.78125 Stop for a second and think about the results: * What does the relationship between the median and the mean tell us here? * Is the distribution really symmetric? * Is the distribution maybe skewed? If so, how? 2.3.2 Spread The rest is a piece of cake just the like. You can calculate the variance using var() and the standard deviation with sd(). var(spend) ## [1] 154.7656 sd(spend) ## [1] 12.44048 Please read chapter 7 in (Fogarty 2019). Chapters 4 and 5 are a good idea, but you do not necessarily have to. "],["visualising-data.html", "Chapter 3 Visualising Data 3.1 Communicating with Data Effectively 3.2 Plotting Data with R 3.3 Readings for This Week", " Chapter 3 Visualising Data 3.1 Communicating with Data Effectively Good data visualisations tell stories that do not need much explanation. Those who view the data can understand what insights you want to communicate from the data without necessarily using a lot of text (like this one) as their explanation. Viewing data visualisations is a lot of fun and a seemingly effortless way to explore and process the information in the data. The Internet is full with great examples at different levels of complexity. It is easy to get lost, take a look for example here, here or here. I am sure you can find many more interesting examples on the web. From a creator point of view, they allow to communicate a lot of information in a direct way. But it goes without saying that a well done data visualisation requires a bit of thinking. 3.1.1 Thinking About your Data Visualisation At its core, data visualisation is actually pretty simple. How can we possibly visualise the data in a way that it a) communicates the key information in our data well while b) transporting it so that it is easy to understand for the viewer? Whenever you want to visualise data you should probably first sit down and think about what it is that you want to tell the end user of your visualisation? What is your key message? The visualisation ideally expresses more content with less ink. Or in more mathy terms: \\[\\begin{equation} \\text{maximize} \\left( \\frac{\\text{content}}{\\text{ink}} \\right) \\end{equation}\\] This optimization problem comes with an important sub-constraint: The skill level of your potential audience. It will greatly determine the level of complexity that you can pitch. Who is your audience? Are they a lay audience without a lot of statistical training? So they would probably require a visualisation that allows for a more intuitive understanding. If the audience is more technically versed, your approach to data visualisation can be more sophisticated. Let us talk about colour. Think about the medium on which you want to use your figure. Is it made for publication in any printed format such as a report or a book? If so, be aware that not everybody is printing in colour, but usually in black and white. This means that you have to consider closely how light or how dark your colours are. For example, it might not be a good idea to use green and blue at the same time, simply because they are likely to have similar shadings. A colour that is too light might not be seen well on a white paper when printed. Of course, you are much more free in the choice of your colours when you your figures are for display on screens—like this homepage. Also think about the colourblind. Can they view your figures, too? Be careful when you use the spectrum from red to green. Ultimately, the choice of colours is a matter of preference and expression. If you feel arty, go wild! At the same time, picking the right colours is also a deep and awesome rabbit whole. Check this talk by the guys who invented the recent viridis colourscheme in case you want to get a taste. 3.1.2 Visualising ‘The Data’ When we are visualising data, we probably want to get an overview over it ourselves, so the first move is to take a look at ‘the data’ itself. How does it look like? How is it distributed? We learned that we have different data types, i.e. nominal, ordinal, interval and continuous data. 3.1.2.1 Nominal Data The best way to visualise nominal data is the bar chart. It gives you a nice and succinct overview over the data without distorting your perception. You can plot data vertically as we have done here in blue or you can plot them horizontally—here in red. Be aware of using of not using pie charts. While they are indeed quite popular, they are wrongly so. It seems that Microsoft made us believe that this is a good way to represent data when they introduced visualisations in Excel. You can plot them with R, but it will remind you that there is research showing that it distorts your perception of the data. You will systematically overestimate the relevance of small proportions of the data and think that these proportions are much larger than they really are. 3.1.2.2 Ordinal Data Ordinal data are nominal data that can be put into a logic order. Of course you can again use the barcharts like before. But you can also make use of the fact that now you actually have ranked data. Why not plotting them as a stacked barchart? 3.1.2.3 Interval or Continuous Data Interval data converges to continuous data in the extreme, so let us treat them at the same time here. The left figure charts data simply on the basis of their value in red. We run into a pretty straight forward problem: We cannot see how often we are actually seeing each observation. The solution is to stack values in case they occur more often, like in the purple figure to the right. Here we have a really nice overview of how often we observe each case. Plotting each individual data point is realistic for our case where we have just a few observations. However, this can become way too complex to process in case you have a larger number of values. In these circumstances you can visualise the distribution of the data with bins in which you collect your data. On the left in the blue histogram you can see how our data would look like if we split it up into eight groups—it results in a really nice pattern! On the right you can see how the data looks like if we just used four bins for it. 3.1.3 Visualising Typical Values and Spreads Now that we have a first overview over how the data usually looks like, we want to become a bit more systematic in our approach to getting to know the data. Remember that we talked about typical values and spread when we wanted to describe data? This is what we want to visualise now. When you have interval or continuous data, there are a number of typical questions that you might want to know from the data. What is a typical value for the data? How are the values of a variable in a sample distributed? How clustered or dispersed are these values? There is one visualisation that summarised this all: boxplots. They easily maximise the content-per-ink criterion. We can identify a number of things from this plot. The median of the distribution is a vertical bar and somewhere in the the colourful box in the middle. This box itself also expresses information: it covers the interquartile range, so the central 50% of the distribution are within the range of this box. If the distribution is symmetric, you can see that the median is in the centre of this box. In case the distribution is skewed, the median will push to one side or the other. The whiskers that you see give an impression of the magnitude of the overall spread of the data. Typically they are 1.5 times the interquartile range starting from the box. Data that goes beyond this range will be plotted with a circle. Let us take a look at how useful this really is in practice. In this first figure we are plotting the hours that you are actually studying in red and the hours that you think you should be studying in yellow. Let us begin with the median values. The median for the hours actually studied is 12, the median for the hours ideally studied is 21. You also get an impression of the magnitude of the interquartile range: it ranges from 8.75 to 18.5 for the top figure in red. For the lower figure, it begins at 14.75 and goes to a value to 25. Both distributions are a little skewed. On the top, the median is a bit to the left of the box. This means that a quarter of you study between the lower end of the box and the median value so somewhere between 8.75 and 12. The same interquartile range between the lower 25% quantile and the median is much larger for the hours that you think you should ideally be studying. The whiskers offer an impression of the data beyond the interquartile range. The smallest value for the hours that you actually study is 2 in the top figure. The maximum hours that any of you studies is 20. The whiskers are a little different for the hours that you think you should ideally be studying. On the left, they range from 8 hours and they grow all the way up to 30 hours. In the next figure, let us take a look at your age: I plotted your age between 18 and all the way up to 30 years and you can see it is not necessary to cover anymore space. Large parts of you—at least 75%—are either 19 or 20 years. You can also see an outlier for one person who is 25 years old and is outside of the whiskers. He or she can be seen as clearly distinct circle. Lastly, you also offered information about a variable that I think is particularly interesting. How much money do you spend when you go out? Here we can see the information nicely summarised in a box plot. The median person spends 20 GBP when he or she goes out at night. A quarter of you spent between 10 GBP and 20 GBP. Another 25% spent between 20 GBP and 26 GBP when you go out. The whiskers give an indication of the whole spread of the distribution. There are people who typically spend 0 GBP when they go out. On the other side of the spectrum somebody seems to have quite deep pockets and is able of spending up to 50 GBP per night out. 3.2 Plotting Data with R Now it is your turn. When it comes to plotting, there is a package that has been put out a couple of years ago which is called ggplot2. On the web, you can find a lot of R visualisations that make use of it. For this class, however, we will stick to the basics—simply because I think it is good to first learn how to really control R from scratch. You can then always switch to higher level printing packages such as ggplot. Think of it as learning how to drive: In this module we will shift gears manually and you will find that switching to a car with automatic gears will not be any problem later. If you want to know more about ggplot2, this week’s reading of Chapter 8 in Fogarty (2019) is your friend. In all we do here, I am showing you some basic options. Look for the name of the function in the help tab to find out more details about the functions. 3.2.1 Basics in Plotting First, let us whip up some data. # This creates a vector with a sequence of integers from -10 to 10 with a distance of 1 x &lt;- seq(-10,10,1) x ## [1] -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 ## [20] 9 10 We calculate a second variable. a &lt;- 5 b &lt;- 3 c &lt;- .5 y &lt;- a + b*x + c*x^2 y ## [1] 25.0 18.5 13.0 8.5 5.0 2.5 1.0 0.5 1.0 2.5 5.0 8.5 13.0 18.5 25.0 ## [16] 32.5 41.0 50.5 61.0 72.5 85.0 And then we use the function plot() to chart the data. plot(x,y) Simple, right? R takes over a number of jobs for you, from selecting the right data range in both axes, to chosing the shape and colour of representations for the data etc.. To give you a first idea, this is what happens when you tweak a number of options. plot(x,y, cex = 3, # size of the data points pch = 16, # point character col = &#39;aquamarine3&#39;, # chose the colour. R defaults with inverted commas. las = 1, # rotates labels in y axis by 90 degrees, xlim = c(-15, 15), # This defines the limit of the x-axis you want to plot in main = &#39;My First Figure&#39; # Title ) You select the options by adding them with a comma. Check the help file in R-Studio for what is available. We will have a proper session on plotting bivariate data in a couple of weeks for all this, so do not worry about the details at this stage. The important message that needs to come across is that R will handle a lot for you—but the possibilities to adapt everything that you were seeing are literally endless. 3.2.2 Colours R is great for using all different kinds of colours. You can check the already pre-defined colours by calling the function colors(). Here, let us call the first 15 ones. colors()[1:15] ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; ## [5] &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; ## [9] &quot;aquamarine1&quot; &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; ## [13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; And you can even define your own colours using the rgb function. Cardiff University’s corporate identity colours for example can be generated like so. # Predefined Cardiff colours # primary cardiffred &lt;- rgb(211,55,74, maxColorValue = 255) cardiffblack &lt;- rgb(35,31,32, maxColorValue = 255) cardiffgrey &lt;- rgb(47,68,78, maxColorValue = 255) cardiffgold &lt;- rgb(189,158,94, maxColorValue = 255) # secondary cardiffblue &lt;- rgb(21,44,81, maxColorValue = 255) cardiffpurple1 &lt;- rgb(29,15,51, maxColorValue = 255) cardiffpurple2 &lt;- rgb(60,44,89, maxColorValue = 255) And then you can use them for plotting. a &lt;- seq(1,10,1) b &lt;- rep(1,10) # repeats the value &#39;1&#39; 10 times plot(a, b, col = c(cardiffred, cardiffgrey, cardiffgold), cex = 2, las = 1, pch = 15 ) The number of data points is larger than the number of colours. R will therefore cycle through the colours. 3.2.3 Visualising Nominal and Ordinal Data To visualise data in a way that we can chart them as a barplot, we first have to count them. How often do we observe each value? There is a really useful function for that in R, it is called table(). Just calling it without plotting is quite revealing. table.breakfast &lt;- table(breakfast) table.breakfast ## breakfast ## Coffee Other Tea ## 7 4 5 Just a quick fyi: You can use the function prop.table() to get percentages. Then round() will round the resulting values. prop.table.breakfast &lt;- prop.table(table.breakfast) prop.table.breakfast.rounded &lt;- round(prop.table.breakfast, digits = 2) prop.table.breakfast.rounded ## breakfast ## Coffee Other Tea ## 0.44 0.25 0.31 But we are here for visualising data. We need the object table.breakfast for our barplot. barplot(table.breakfast) Let us make this a bit more pretty. barplot(table.breakfast, ylab = &quot;Frequency&quot;, # Adds label to the y-axis col = cardiffred, # You know this already las = 1, # and this border = NA # Removes the small black border around the bars ) barplot(table.breakfast, xlab = &quot;Frequency&quot;, las = 1, col = cardiffgold, horiz = TRUE # plots bars horizontally instead of vertically ) 3.2.4 Visualising Continuous Data We also wanted to take a look at continuous data. Let us see how far you are living from the university for that matter. # R uses an algorithm to determine the number of bins hist(commute, col = cardiffgold, xlab = &quot;Minutes&quot;, ) # Here we control a bit more by ourselves hist(commute, breaks = 3, # This is a very rough break into three bins col = cardiffpurple1, xlab = &quot;Minutes&quot;, main =&#39;&#39;, # suppress the default main title border = &quot;white&quot; # border around the histograms--here set to white ) 3.2.5 Boxplots Finally, let us take a look at boxplots. The default function is called boxplot(). Calling it with a continuous variable such as spend as its argument will already generate a plot that has all the important information. boxplot(spend) But admittedly it is far from pleasent to look at the figure. Since we want that others pay attention to our message, let us make our visual communication more appealing. # here, we create a data object that is a list. Think of it as a super flexible data object. list.study &lt;- list(studyideal, studyperweek) boxplot(list.study, horizontal = TRUE, col = c(cardiffred, cardiffpurple1), # different colours for the plots axes = FALSE, # we are suppressing all axes boxwex = .6, # this is about the width of the interquartile range box ylim = c(0,40), # defines the limit of the x-axis (!). # It would have been the y-axis, but we rotated the figure. # This is a superconfusing command of the boxplot function # We just have to live with it... xlab = &#39;Hours&#39; # Label for the x-axis ) # Here, we are adding axes manually to have more control over them. axis(2, col = &quot;white&quot;, las = 1, at = c(1,2), # where to put the tickmarks lab =c(&quot;Ideally \\n Studied&quot;, &quot;Ideally \\n Studied&quot;) # Labels. &#39;\\n&#39; is a new line ) axis(1) This is it for this week. 3.3 Readings for This Week Don’t forget to check into the readings of this week which is chapter 8 in Fogarty (2019). As mentioned it is all about ggplot2—take it as a pointer to what is possible. We will not cover ggplot2 in this class, but you are of course free to go wild and become a proper ggplotter. "],["testing-a-hypothesis.html", "Chapter 4 Testing a Hypothesis 4.1 Generating Good Hypotheses 4.2 Studying Our Sample 4.3 Is What We See Just Coincidence? 4.4 Coding 4.5 Mathcamp: Summing With Sigmas 4.6 Reading for This Week", " Chapter 4 Testing a Hypothesis Welcome to the second part of the module PL9239. In our second scenario we turn to one of the core tasks of any data analysis: inference. The basic idea is that you want to be able to make a statement—e.g. calculate a statistic—about the world out there. But since the world out there is so large it is impossible to collect data about everything, we will have to chose a different approach. Instead, we are going to learn how to collect data about a sample and then say something with a little uncertainty about the world at large. To show you how useful all this actually is, I interviewed Michelle Brown who is working as a Senior Advisor on Elections for the National Democratic Institute. This NGO cooperates with local civil society actors in fragile democracies and trains them to do voting observation missions. These election observers tabulate the ballots in parallel to the official representatives and can thus get an idea about the amount of voting fraud that might be occurring. Obviously, it is prohibitive to observe every single polling station. In some countries there might be tens of thousands of polling stations and an NGO never has the resources to cover all that. Instead, the NDI relies on sampling. Election observers only monitor randomly selected polling stations, and the NDI then uses statistics to infer what is really going on. Together with colleagues, we are doing research with the National Democratic Institute that helps them improve their technology stack even further. In case you are interested, feel free to take a look at a presentation I gave at last year’s CogX—it might give you a good overview about what we are doing. Of course, feel free to reach out in case you want to know more. 4.1 Generating Good Hypotheses All research requires hypotheses. What is a good hypothesis? Good Hypotheses have a number of characteristics. A hypothesis is falsifiable. A hypothesis relates two observations. A hypothesis qualifies this relation, i.e. makes a statement whether the relation is the same, larger or smaller. Let me walk you through good hypothesis in a short video using our data as an example. 4.2 Studying Our Sample 4.2.1 Investigating the Hypothesis We investigate our data and take a look at what the data is telling us. First we compare the hours that you are studying with the hours that you think you should be studying. Given that we assume that you have a bit of a bad conscious regarding your efforts, there should be a clear difference. Hypothesis 1: On average, you think that you should be studying more than you actually are. Let us investigate the question. We begin with Hypothesis 1. It seems, there is a lot to our hypothesis. Given the two distributions, there seems to be a clear difference. Next we will compare you to the cohort here in Cardiff two years ago. Actually, I asked them the same questions that I asked you. Would we expect any difference between you and the previous cohorts? There is no real reason to believe that the cohorts are different, so: Hypothesis 2a: On average, all three cohorts are putting in the same effort. Hypothesis 2b: On average, you and your previous cohorts have the same expectations regarding the efforts that you think you should be making for your university degree. Let us take a look at what the data is telling us. It seems as if on average you are investing less than the cohorts in 2019 and 2018. It is also interesting that the variation between you is much lower than the variation in the previous cohorts. Now when you compare your cohort to the cohorts with regards to how much you think you should be studying, your ideal number of hours is on average not so different and you seem to be a bit more in line with the other students. 4.3 Is What We See Just Coincidence? We have seen that indeed there are slight differences between your module and the students in the module before you. Now, can we say something about your cohort—all Politics and IR students in your year—more in general? It could be, for example, that out of pure coincidence you are a particularly efficient crowd of students who do not need as much time to study as the cohorts before you. So given the differences in the observations from your class and the classes in previous years, can we say something about whether we would expect differences in your cohorts in general? 4.3.1 Are the Two Cohorts the Same? To be able to answer this question, we first need to understand the idea of sampling. Sampling means to draw a number of observations randomly from a larger population. Intuitively, our sample should be able to provide us with a good account of what is going on in the population. Of course, with bad luck we can always be wrong and sample a result that is quite unusual given what is going on in the population. It is also intuitive that larger samples should be less prone to coincidences. If we only ask a few number of individuals, there is fair likelihood that these few people do not represent the overall population well. However, if we ask a larger amount of people, we could be fairly certain that this sample gives us a good insight about the overall population. How does it help us understand whether your cohort is different from the cohort before you? Let us assume that on average the effort that you put into your studies is the same, for you and for the cohorts before you. How likely is it that we would observe these two samples assuming that the cohorts invest on average the same amount of time? What are the odds to observe these differences? We have a clear expectation. If the average amount of hours in our data sample is largely different, then this is already a good indication that it should be quite unlikely to observe so different samples given that they all come from the same population. If it is unlikely that we observe such a group of students on the basis of pure coincidence, we should maybe revisit our initial assumption. Maybe, it was wrong to assume that you and the other cohorts all share the same efficiency in studying. We would then have to refute this initial assumption and rather conclude that, well maybe your group indeed puts in a different effort into your studies. 4.3.2 How Can We Tell the Difference? In the light of the two samples that we observe, we want to be able to determine how likely it is that they are from the same population? So we need a measure. It is called the p-value and in our example it will be able to tell us how likely is it to observe the mean of the two samples given that we assume they come from the same population. This is a very useful measure, because now we can say something about the likelihood that these samples really are from cohorts with the same amount of effort in both groups. If the samples look quite similar, then we can be certain that they are from same population. However, if they are really distinct, the probability that the cohorts have similar efforts is quite small. So what should we conclude? We might want to reconsider our initial assumption: Maybe the two cohorts are not one large population. Instead, maybe the two cohorts are different after all. 4.3.3 Hypothesis Testing What we are doing here is called hypothesis testing. More formally, we distinguish between two hypothesis. The first one is called the null-hypothesis and it assumes that there is no difference between the two groups. We can specify it explicitly as follows. Null hypothesis (\\(H_0\\)): The samples come from populations with the same mean. As a default, we will always at first believe in the fact that there is no difference between the cohorts. We change our minds only if we see that it is quite unlikely that we would observe our two samples. In that case we will then rather prefer a different hypothesis, the alternative hypothesis. Alternative hypothesis (\\(H_A\\)): The samples come from populations with different means. If we believe in the alternative hypothesis, then we say that the two populations are different. How unlikely is ‘unlikely enough’ to change our minds? By convention, in the Social Sciences we define a threshold of 5% for that: We will believe in the null-hypothesis until the two samples are so different that we think it would be less than 5% probable to observe two samples that are so different by chance. More formally, the threshold \\(\\alpha\\) for the acceptable p-value is set to \\(p \\leq 0.05\\). 4.3.4 Test! Test! Test! Now, let us go and test the hypotheses in our data. Regarding the actual effort you put into studying, is your cohort different from the cohort in 2019? With R, we can do a so called t-test. We will cover it more formally next week, so do not worry at this stage how it really works. What matters for now is that it will return us the p-value we are interested in: the probability that the mean in the cohorts is the same given the two samples we observe. We first compare your cohort to the cohort in 2018. ## ## Welch Two Sample t-test ## ## data: dat18$studyperweek and studyperweek ## t = 1.3862, df = 26.205, p-value = 0.1774 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.902392 9.792098 ## sample estimates: ## mean of x mean of y ## 16.88235 12.93750 R reports a number of statistics and for now we only care about some. First, for the mean of both of the samples at the very bottom of the table with 12.94 for your cohort and 16.88 for the cohort back in 2018. The important bit for us is the p-value. Here it is reported as 0.18. How do we interpret this result? Well it tells us that assuming that both of the samples come from the same population, there is a 17.74% probability that we observe the result if both are from populations with the same mean. In the light of this evidence, we stick with the null hypothesis and believe that apparently the amount of average effort is the same in both cohorts. Let us now compare your cohort to the one in 2019. Here are the results from the t-test. ## ## Welch Two Sample t-test ## ## data: dat19$studyperweek and studyperweek ## t = 2.161, df = 29.225, p-value = 0.03903 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4413764 15.9444932 ## sample estimates: ## mean of x mean of y ## 21.13043 12.93750 The means are even further apart. The cohort in 2019 was putting in on average 21.13 hours, while you on average study 12.94 hours. Can we still assume that the mean in both cohorts is the same in the light of the data we collected from the students in the two methods classes? The p-value is reported as 0.04. If both of the samples were drawn from cohorts with the same mean, we would have a 3.9% probability that we observe the data in our two samples. This is quite unlikely and below the general accepted the threshold of \\(\\alpha\\) = 0.05. So what do we do now? Well, we refute the null hypothesis \\(H_0\\). Instead, we will trust in the alternative hypothesis \\(H_A\\). Your cohort is on average studying less than the cohort in 2019. We can do the same exercise for the hours that you think you should be studying ideally. Let me show you how this works in a quick video. 4.3.5 Type 1 and Type 2 Errors When we test a hypothesis based on samples, we can of course be wrong. Remember that everything do involves probabilistic statements. An \\(\\alpha\\) level of 0.05 means that in 19 out of 20 cases we should be correct. However, in one out of 20 cases we might wrongly refute a hypothesis. We could observe two samples that seem to be different even though in reality the null hypothesis is actually true. This kind of error is called a type-1 error or also a false positive. Then again, the two samples may come from populations with distinct means. But out of bad luck we get samples that actually look quite similar. In that case, we speak of a false negative, or a type-2 error. Figure 4.1 offers you a nice summary. Figure 4.1: Type 1 and Type 2 Errors A good way of remembering this difference is with the help of the following story that you certainly know from your childhood. The first time the young shepherd is calling for help, the farmers believe that the wolf is there even though in reality it is not—clearly a type 1 error or a false positive. In the second instance, the farmers think there is no wolf, even though in reality there is actually one. We speak of a false negative, or a type 2 error. 4.4 Coding This week was quite heavy on conceptual things, so we will be much lighter with the code. 4.4.1 Data Management We learned that we can select from objects using their position and these brackets [ ]. Since data frames are R’s spreadsheets, you can select from them also by the name of their column. # Create data people.living.in.household &lt;- c(3,2,5) education.in.years &lt;- c(10, 13, 15) favourite.colour &lt;- c(&#39;forestgreen&#39;, &#39;darkred&#39;, &#39;lemongreen&#39;) dat &lt;- data.frame(people.living.in.household, education.in.years, favourite.colour) Let us select the first column. dat[,1] ## [1] 3 2 5 dat$people.living.in.household ## [1] 3 2 5 Or what about the third element of second column? dat[3,2] ## [1] 15 dat$education.in.years[3] ## [1] 15 Sometimes you want to recode a variable. The oldschool way using the car package goes as follows. library(car) dat$favourite.colour &lt;- recode(dat$favourite.colour, &quot; c(&#39;forestgreen&#39;, &#39;lemongreen&#39;) = &#39;green&#39;; &#39;darkred&#39; = &#39;red&#39;&quot;) dat$favourite.colour ## [1] &quot;green&quot; &quot;red&quot; &quot;green&quot; Finally let us meet a problem that will keep haunting you until the end of your professional life: missing data. Sometimes, data has empty entries. By convention R uses NA for it. Please not that even if it looks like a string variable, it actually is not treated as such by R. ## [1] 5e+04 1e+05 NA Things begin to be complicated as soon as you want to do calculations with the variable. The reason is that R protects you from making stupid mistakes. mean(dat$income) ## [1] NA I know this is a pain at the moment, but believe me, once you write more complex programmes for analysing data, it will become really useful. You can set options in the functions to override the default behaviour. mean(dat$income, na.rm = TRUE) ## [1] 75000 Another more general way of handling the issue is to filter your data set for missing values with na.omit(). dat.no.na &lt;- na.omit(dat) dim(dat) ## [1] 3 4 dim(dat.no.na) ## [1] 2 4 As you can see, R kicked out a whole row from our observations. This certainly means that quite a bit of information just go lost here. Deleting missing data is a quick and convenient fix, but it comes at a high price. There is a whole subfield that is concerned with imputing values that are reasonable for the missing observation. The intuition is that you would use information from the fully observed variables to get a good guess for the observation that is missing. For example, in our case the years in education might give us a good proxy for the income that someone receives and we can get a good estimate for it. If we then want to analyse the relationship between—say—preferred colours and income we would then have a better data basis for that. 4.4.2 Hypothesis Testing R is made for statistical analysis. No surprise that doing a t-test is really easy: With the function t.test(). ttest.perweek.21.19 &lt;- t.test(dat19$studyperweek, studyperweek) ttest.perweek.21.19 ## ## Welch Two Sample t-test ## ## data: dat19$studyperweek and studyperweek ## t = 2.161, df = 29.225, p-value = 0.03903 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4413764 15.9444932 ## sample estimates: ## mean of x mean of y ## 21.13043 12.93750 4.5 Mathcamp: Summing With Sigmas You were asking for a refresher on how to read the symbol \\(\\sum\\), so I put together a small video. 4.6 Reading for This Week The readings for this week are chapter 6 in Fogarty (2019) which tells you all about generating hypotheses. Chapter 9 is all about how to test them. "],["why-does-hypothesis-testing-work-a-primer-on-probability.html", "Chapter 5 Why Does Hypothesis Testing Work? A Primer on Probability 5.1 Distributions 5.2 Inference 5.3 Revisiting the Original Questions 5.4 A Simple Figure With Two Variables 5.5 Reading for This Week", " Chapter 5 Why Does Hypothesis Testing Work? A Primer on Probability Why do we care so much about inference? We can calculate central tendencies and spreads. Why learn more? This week we are revisiting the concepts from last week and will take a closer look at them. To goal is that you gain a deeper understanding about what it is that we are actually doing here. What does inference mean? Why does all this inference actually work? And why is it so important to properly randomise the samples that we draw? How can we be 95% confident about the test of a hypothesis? 5.1 Distributions What does inference actually mean? To properly understand the idea behind it, let us begin with distributions and a number of related ideas that will all be useful. 5.1.1 Probability We start with with probability and take a look at a definition. Let us define probability from a frequentist perspective. The probability of an outcome is the frequency of that outcome if the process were repeated a large number of times. This says that we can learn the probability of an event by trying it out many times—whatever it means. Here are some examples that you know from everyday life. Toss a coin: The probability to get a ‘head’ is Pr(head) = 1/2 — if the coin is fair of course. Toss a dice: The probability to throw a 1 is Pr(1) = 1/6 The probability to throw everything else than a 1 is Pr(not 1) = 5/6 While you cannot observe these probabilities from one throw alone, try it out a couple of times, say, 100? Or even better 1000? You should get really close to the true underlying value of the probability. 5.1.2 Distribution The next useful concept is the distribution. You can get to the distribution of an event in a pretty straightforward way. Imagine we were playing Boule in our local club ‘Allez les Bleus.’ On our Thursday evening training sessions we are trying to hit a line that is exactly 5m away. As avid data aficionados we are measuring each try and record the data in a spread sheet. At home you sit down and chart of histogram of all the tries that we have been throwing. Figure 5.1: A Histogram Becoming a Distribution. On the horizontal axis you are observing the possible outcomes. In statistical theory, this area is also called the sampling space. In the example with the coin, the sampling space \\(S\\) of one fair flip one coin is \\(S = {H, T}\\) where \\(H = \\text{heads}\\) and \\(T = \\text{tails}\\) are the outcomes. Here it is how far we can throw. On the vertical axis you are at first simply observing the frequencies. Increasing the number of bins of the histogram leads to increasingly realistic representations of the data. With infinite number of bins, you end up with a continuous representation of the data. Once the data is continuous, we do not speak of frequencies any longer—simply because each point on the x-axis is not infinitely small. We now call the vertical axis density. Given our definition of probability above, if we were to repeatedly throw the boule infinitely many times, the resulting data would be the distribution of the probability to actually hit the 5m line. There is an important property of distributions: Everything under the curve adds up to 1. This means that probability distributions have a straightforward geometric interpretation: we can simply ask for value ranges that we are interested in and see how often we would observe it. 5.1.3 The Normal Distribution But if we want to know more, we have to introduce a more rigorous concept of what a probability distribution actually is. Let us get to know the Normal Distribution, a very special probability distribution. This is how it looks like. Figure 5.2: The Normal Distribution. The Normal Distribution has a number of important properties. It is always a bell-shaped curve. It is always symmetrical. The mean = median = mode. The tails are asymptotic, which means the values get closer to the x-axis the further you go into infinity, but never intercept it. Two parameters drive the distribution: The mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). The Normal Distribution has a proper formula that allows us to calculate its density. Remember, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) determine how the function looks like. Then, given any value \\(x\\) we can calculate the resulting density. The formula for the probability density function was discovered by Carl Friedrich Gauss. \\[f(x; \\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} exp \\left[ - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} \\right] \\] The parameter \\(\\pi\\) is the famous mathematical constant. The expression \\(exp(\\cdot)\\) is short for \\(e^{(\\cdot)}\\), where \\(e\\) is yet another important mathematical constant, the Euler number. You do not have to understand all details here, the main point is that you can see how \\(\\mu\\) and \\(\\sigma\\) drive the shape of the whole thing. The resulting shape of the function has yet another couple of great characteristics. The area covered within the bounds of the standard deviation always remains the same: The area ± 1 SD from the mean always covers 68.3% The area ± 2 SD from the mean always covers 95.4% The area ± 3 SD from the mean always covers 99.7% etc. Take a look at the figure below. We have two different Normal distributions. On the left, \\(\\mu\\) = 5 and \\(\\sigma\\) = 1 and on the right \\(\\mu\\) = 5 and \\(\\sigma\\) = 2. In both cases, the area that covers one standard deviation to the left and to the right of the mean covers 68.3%. This means that if we have a process that we can describe with this function, then we know that there is a probability of 68.3% that we end up with a value between 4 and 6 on the left figure. Likewise, with the same probability of 68.3% we will end up between 3 and 7 on the right figure. The same holds for all other values of the standard deviation (2 SD, 3 SD, …) Fascinating!. Figure 5.3: Two Different Normal Distributions and The Probability Mass Covered By Their Standard Deviations. This is really useful, because independent of the exact shape of the Normal Distribution—that is indenendent of the values of \\(\\mu\\) and \\(\\sigma\\)—we can now draw all kinds of probabilities using simple geometry. Figure 5.4: Probabilities are Just Shapes Under the Distribution Function. Remember what we know: one standard deviation to both sides covers 68.3%, two standard deviations to both sides covers 95.4%. The probability to observe a value between 0 and 1: 34.15%. We know that 1 standard deviation to the left and right adds to 68.3%, so 68.3%/2 = 34.15%. The probability to observe a value &gt; 1: 15.85%. Simply take the left 50% and also add the one standard deviation to the right of the mean 68.3%/2 = 34.15%. So 50% + 34.15% = 84.15%. Now given that all under the curve adds up to 100% we simply subtract the 84.15%—and get 15.85%. 5.1.4 Your Standard Measure z-scores What are z-scores and why do we need them? We can use them if we want to make comparisons across samples. For example, we might want to compare the effort from two students Alexandra and Bastian who are in different cohorts. In comparison to their peers, is Alexandra more diligent than Peter? Alexandra studies 40h per week, Bastian 42h. So prima facie, Bastian is working more. But: Is he working more than Alexandra in the light of the different study regimes of their cohorts? To determine who studies harder we can standardize the effort of each of the students: How different are Alexandra and Bastian from the typical student in their class? The idea is straightforward: We calculate the difference of each student from the average per spread of their respective cohorts. To compare variables from different distributions, we can standardize them by building so called ‘z-scores.’ \\[ z_{i} = \\frac{x_{i}-\\bar x}{\\sigma} \\] A so standardized variable will have mean zero and a standard deviation of one. Let us do the math. We learn that Alexandra’s class has a mean of 30h with a standard deviation of 5. Bastian’s class has mean of 40h with standard deviation 6. We begin with the z-score from Alexandra. It is \\(z_A =(40−30)/5=2\\) Bastian’s z-score is \\(z_B =(42−40)/6=0.33\\) What do we conclude? Well, Alexandra is on average much more ‘off’ in comparison to her peers than Bastian. She is much more of an outlier and he actually is much more of an average guy when it comes to studying. One more thing. The z-scores have a link to the Normal distribution. We can actually use them to calculate the probability of an event. The only thing you have to do is convert your data into z-score. And then you can look up its probability in the probability tables of a normal distribution. Figure 5.5: Reading z-Scores. Here is a quick video where I show you how to use the table. 5.2 Inference Let us continue with a proper look at inference. We will revisit some topics we talked about last week—but this time in a much more rigorous way. We might be interested in all sorts of research questions. Which party would citizens vote for? How many hours do school-children study at home? How many children do parents wish to have? The problem is pretty straightforward: We cannot ask everybody or count everything. What do we do? Well, we take a look at a sample. With the sample, we will try to answer to questions. Given what we observe in the sample: What is the most probable value in the population out there? How certain are we about our results? 5.2.1 Some Key Concepts Before we learn how to answer these questions, we will take a look at some key concepts. Population The full set of cases about which we want to generalise. Sample A subset of the population. Variable Logical set of attributes (characteristics) of an object (person, thing, etc.) that can vary across a range. Parameter A characteristic of a population; usually unknown. Descriptive Statistics Statistics that summarise the distribution of the values of variables in a sample. Inferential Statistics The use of statistics to make inferences about a larger population based on data collected from a sample. 5.2.2 Population and Samples We said already that we can never know the whole population, simply because it is impossible to ask every single individual. Imagine we are interested in learning how the distribution of a particular variable looks like in the population—e.g. income, voting intention etc.. This means that we are particularly keen learn two key variables. The population mean \\(\\mu\\) The population standard deviation \\(\\sigma\\) How could learn about the population? We can draw one sample and measure the sample mean \\(\\bar m_i\\); and the sample standard deviation \\(s_i\\). Note that the notation for population parameters is in Greek letters, while the sample parameters will always be Roman letters. This will give us a first impression about how the distribution looks like. Can we take any sample? Think of the following situation. Imagine you wanted to predict the upcoming elections in Wales. You are asking 1000 people about their voting intention. To do so, you walk in the rainy streets of Cardiff until you collected 1000 responses—and 36% intend to vote for the green party. Can you trust in this result? Well, let us see: You have an idea about how 1000 people who had a reason to be on the streets of Cardiff are intending to vote. But it could be that those people might have a particular political preference. For example, they were out and about despite the weather conditions. It would be quite reasonable to assume that they are more environmentalists than those who stayed at home—or even those who live outside Cardiff. If you want to know about the voting intention in all Wales, you probably need a good representation of what is going on in all Wales. Ideally, you would be able to have a completely random draw from all Welsh voters—and not only those who were on the streets of Cardiff on a particularly rainy day. Luckily, when we generate the sample with our computer, we can simulate a truly random sample How could we improve our knowledge about the population? We could draw many more samples, calculate the mean and standard deviation from them and learn how much all these means and standard deviations vary. This would refine our idea about the average typical value in the population and the average spread of the data in the population. Figure 5.6: Drawing Many Samples from the Population. Doing so, we are retrieving another distribution: The distribution over the averages of our samples. This distribution is also called sampling distribution. As with any distribution, we are interested in two key values. First the mean of sampling distribution of means \\(\\bar m_i\\). Second the standard deviation of sampling distribution of means \\(s_n\\) Let us take a look at a real example in Figure 5.7. On the left, there is the unknown distribution in the population. We then randomly draw one sample from this distribution. You can see that for a random reason, it is a bit off. If this were in the real world, we of course would not know that we are off—we only have this one sample. Since we are simulating everything on the computer we take many more samples. You can see on the right that we are retrieving many samples with their bell-shaped distributions. For each of these individual samples we can calculate an average. And you can see all averages accumulate at the bottom of the right hand figure as red points: if you closely inspect these means, they are clustering in ‘the middle.’ This center actually is the true underlying population parameter. Figure 5.7: Drawing Many Samples from the Population. If we were able to repeat the sampling a great number of times, there actually is a way in which we can really measure the unkown population parameter \\(\\mu\\), the average of the population. We can estimate the population mean \\(\\mu\\) as the average over all means from all samples. The means of all samples also vary of course, which means we can calculate a standard deviation for them. This standard deviation of the means of the samples—so the standard deviation of the sampling distribution—is called the standard error. 5.2.3 The Central Limit Theorem If you closely look at how the means of the samples are actually distributed, do you notice something? Well, it looks a bit as if they were distributed in a bell shaped way—just like the Normal distribution we heard of before. So here is another amazing thing about ‘the Normal’. The whole world would probably not really work if it did not exist. The reason is: all statistical processes boil down to being normal eventually—as proven by the mathematician Pierre-Simon Laplace some 200 years ago. Informally, this is what the Central Limit Theorem is saying. Imagine we have a population distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and we are interested in its mean. Repeatedly taking samples from that distribution, yields the sampling distribution of the mean which approaches a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as the observation in each sample \\(n\\) increases. This holds regardless of the shape of the original distribution. The Central Limit Theorem is the basis for application of statistics to many natural phenomena (which are the sum of many unobserved random events). How? Take a sample, calculate its mean. Do the same thing again and again. The distribution of sample means will be normal. This is a fun application of it: the Galton Board. Balls are entered at the top and have to take a series of left-right decisions. The result of it in the end is the normal distribution. Remember: independent of the original stochastic process, if all tries are truly random, the resulting distribution ends up being normal. By the way, here in Cardiff’s Techniquest there is a massive version of the Galton Board where you can try this yourself. In the video, this works well for the large and the medium balls. But it does not really work for the small ones. Why not? David Bulger offers a great intuition in the comments: Watching the small balls fall, you can see that they tend to gather momentum and run in diagonal paths to the left or right—that is, they don’t change direction much. This illustrates the importance of the independence assumption in the central limit theorem: if the individual random variables are not independent, then their sum may not tend to a normal distribution. So you see how key it is that the sampling happens truly at random. Everything breaks down if we cannot guarantee the randomness of the sampling process. The Central Limit Theorem is the foundation for any inference that we want to do. And it will be really helpful for answering a question you might have on your mind already. With the computer you can draw many samples and calculate their averages to get to the population parameter. But in practice this is not possible: We typically have money to field one survey—and not many different ones. How do all these insights about the sampling distribution help us in practice? Enter the Central Limit Theorem (CLT). We know that if sampling occurs randomly the whole sampling distribution results in a normal distribution. It is OK therefore to be bold enough and assume that the sample we draw is normally distributed. We can therefore calculate an estimate for the key parameters we are interested in from our sample. Given our sample, what would be our best guess for the unknown population parameter \\(\\mu\\)? Well, the average of our sample, right? Our estimate for \\(\\mu\\) is therefore \\[ \\hat{\\mu} = \\bar m_i \\] Note that the hat indicates an estimate. What can we say about our uncertainty of our mean of the sample, the standard error? We know it will follow a normal distribution (CLT!). If we knew the true population standard deviation \\(\\sigma\\), we could easily calculate it from one single sample. \\[ s_n = \\frac{\\sigma}{\\sqrt{n}} \\] Of course we do not know the true population parameter. So what do we do? We again use the value from the sample and assume that we have the right value. The estimate for the standard error then is as follows. \\[ \\hat{s}_n = \\frac{s_i}{\\sqrt{n}} \\] Let us take a look at an example to make this more easy to understand. We assume your sample is a random sample from your cohort. How many hours does your cohort sleep per night? And what is our uncertainty around that? \\[\\hat \\mu = \\bar m_i = 7.69 \\] \\[ \\hat \\sigma_i = \\frac{\\hat \\sigma}{\\sqrt{n}} = \\frac{1.08}{\\sqrt{16}} = 0.27 \\] 5.2.4 Summing Up: What is Inference? Let us take stock and understand the core idea of inference. * We can never know the true population mean \\(\\mu\\). * We will take the next best guesses: Measures on the basis of our sample. * We will make use of the mean, the standard deviation and the size of the sample. * We rely on the fact that random samples would typically give us a good idea about the population (Central Limit Theorem). * And then we assume that the sample mean is the population mean. * We then use the standard deviation from our sample to estimate the standard error of the sampling distribution. 5.2.5 Confidence Intervals: Being 95% Certain Last thing to wrap your heads around for today: We have the standard error. How certain can we be that our sampled mean is close to the population mean? The thing is, in real life we can of course never know the true population mean \\(\\hat \\mu\\). But we can use the Central Limit Theorem! We know that our sample—if well implemented—is a result of a random sampling process. And if that is the case, all samples including ours will follow the Central Limit Theorem (CLT). Let us assume we want to catch the population mean with our sample in 95 out of 100 samples. That means from the mean, we have to cover the area 42.5% to the left of the mean and 42.5% to the right of the mean. We call this area the 95% confidence interval. Figure 5.8: Two Different Normal Distributions and The Probability Mass Covered By Their Standard Deviations. Remember that we were talking about the z-scores above? Using z-scores, we now can translate this statement about probabilities into units of standard deviation in a real distribution. If we want to cover the central 95% of a distribution, we have to go 1.96 times the standard deviation to the left of the mean and 1.96 times the standard deviation to the right of the mean. Knowing that we can assume that the sampling distribution is normally distributed (hat tip CLT), we can calculate the probability that we cover the range of values where we will capture the real mean in 95 out of 100 times. It is the range given by \\[ \\hat\\mu ± 1.96 \\hat \\sigma \\] Or in our example How many hours do you sleep at night? \\[ 7.69 ± 1.96 1.08 = [5.5732, 9.8068] \\] This means that if we drew 100 samples, the true population mean of your overall cohort would be in the interval [5.5732, 9.8068] in 95 out of 100 times. 5.2.6 Revisiting t-tests The great thing about confidence intervals is that we can also use them to understand hypothesis testing. Imagine we are testing whether there is actually a difference in the mean of two samples, just like we did last week with the t-tests. Instead of checking the p-value, we can also study the confidence interval. In 95 out of 100 cases, how much of a difference do we expect to be there between the two samples, given what we observe in the data? ## ## Welch Two Sample t-test ## ## data: dat18$studyperweek and studyperweek ## t = 1.3862, df = 26.205, p-value = 0.1774 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.902392 9.792098 ## sample estimates: ## mean of x mean of y ## 16.88235 12.93750 R returns us the confidence interval: Here, it is anywhere between -1.9 and 9.79. This means that your confidence interval covers the 0—with a probability of 95% you could not tell whether the true difference in the means is lower 0, equal 0 or higher than 0. Hence, you cannot refute the original hypothesis that the means are the same. Therefore, you have to conclude that the means are probably the same. Remember that the difference between you and the cohort in 2019 was statistically significant? ## ## Welch Two Sample t-test ## ## data: dat19$studyperweek and studyperweek ## t = 2.161, df = 29.225, p-value = 0.03903 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4413764 15.9444932 ## sample estimates: ## mean of x mean of y ## 21.13043 12.93750 Now, the confidence interval it is anywhere between 0.44 and 15.94. The confidence interval does not cover the 0. You can conclude that with a probability of 95% the difference between the cohort is larger than 0. Now, you can refute the original hypothesis that the means are the same—at least with a 95% confidence in the results. 5.3 Revisiting the Original Questions Let us revisit the original questions we initially set out above. What does inference mean? Why does all this inference actually work? And why is it so important to properly randomise the samples that we draw? How can we be 95% confident about the test of a hypothesis? If you worked through this class properly and also did the readings, you should be able to answer the questions. If not, let’s be in touch in the weekly workshop on Tuesdays. 5.4 A Simple Figure With Two Variables This week was quite heavy on concepts, hence let us be really quick on the coding. Just a simply plot of two variables. In all we are doing, I am introducing some functions for randomly sampling in different ways. Let us first do a random draw from a normal distribution. We then calculate a second variable from it, adding a bit of normally distributed noise on top. Then we plot it using colour. # Select var1 randomly from a normal distribution var1 &lt;- rnorm(n = 100, mean = 5, sd = 3) # Select some noise again randomly from a normal distribution var2.noise &lt;- rnorm(n=100, mean = 0, sd = 0.5) # create second variable var2 &lt;- var1 + var2.noise # Here I sample two colours randomly allcolours &lt;- colours() two.colours &lt;- sample(allcolours, 2) # Plot two variables plot(var1, var2, pch = 21, # Setting Point Characters, here filled circles col = two.colours[1], # using the first colour for the frame bg = two.colours[2], # using the second colour for the background las = 1, #rotates numbers on the left xlab = &#39;Label on the x-Axis&#39;) # abline is very useful for plotting all sorts of lines # a: intercept, b: slope abline(a= 0, b = 1, col = cardiffred) # horizontal line at 0, lwd: line width abline(h = 0, col = cardiffblue, lwd = 3) # vertical line at 0, lty: line type abline(v = 0, lty = 3) 5.5 Reading for This Week For this week, please read chapter 4 in Agresti (2018). "],["are-different-people-different.html", "Chapter 6 Are Different People Different? 6.1 A Recap from Last Week 6.2 What Are Bivariate Relationships? 6.3 Categorical Data 6.4 Continuous Data 6.5 Code 6.6 Reading for This Week", " Chapter 6 Are Different People Different? Last week, we did a good part of the conceptual heavy lifting for this class. Well done! This week, we will refine the lessons that you have learned. 6.1 A Recap from Last Week Why is inference so useful? The amazing thing about inference is that if you want to learn something about a population—for example their political ideology, how well a certain policy is working or any other characteristic you might want to know—you do not have to collect data on each individual in the population. It is enough to ask several hundred people and then estimate the value that is in the population. This estimate comes at a price. We have to make sure that your sampling from the population is really random. Only if this assumption is fulfilled can we rely on the Central Limit Theorem to do Inference. In practice, this is often easier said then done. We can never be sure about the true value in the population. We have to communicate our uncertainty about our estimate of the population parameter. The 95% confidence interval is really useful for this: Given our assumptions hold, we indeed can make the statement that in 95 out of 100 cases the true population parameter is within these bounds. In short: The goal of statistical inference is to estimate population parameters and summarize our uncertainty about these estimates. 6.2 What Are Bivariate Relationships? We actually talked about bivariate relationships already. Two weeks ago, we wanted to understand the difference between your cohort and the cohort before you. We can think of this data also as a small data set with two variables. One variable encodes the effort that students put into their studies. The other variable would capture information about the cohort of the students. See? It is actually quite natural to think about data not only in the context of one single variable, but actually considering at least two variables at a time. When we analyse data, we study the association between variables. A variable is a “characteristic that can vary in value among subjects in a sample or population” (Agresti 2018). Some typical examples area age, income, gender, education level, race, inequality in a country etc.. We express association formally as a connection between two variables and can use an arrow for that. \\[X \\rightarrow Y \\] The variable Y is called the dependent variable, or response variable. The variable X is called the independent variable, or explanatory variable. For a bivariate analysis, we can measure the values of the variable in question and study “associations” between two or more variables. We first describe this relationship in our sample. We can then of course also do inference and test whether changes in the value of one variable \\(X\\) are associated with changes in the value of the other variable \\(Y\\) in the population at large. Interpret their relationship allows to make a statement about whether a change in one variable \\(X\\) is associated with changes of patterns of variable \\(Y\\). 6.3 Categorical Data If we want to measure the association between two variables, we need to distinguish whether we have categorical data or continuous data. Let us begin with a closer look at categorical data. 6.3.1 Describing the Sample Once we have collected our sample, we want to describe the data that is in the sample. This description is a first valuable step in our analysis of the data. Let us begin with a closer look at categorical data. 6.3.1.1 A First View As long as the number of categories is small enough, we can use a cross tabulation. We can do this either with absolute numbers, or calculate proportions. As an example from our own data, we could for example take a look at the relationship between the number of siblings you have and the amount of sleep you are usually getting per night. Table 6.1: Cross Tabulating the Number of Siblings with the Hours You Sleep at Night. 6 7 8 9 0 0 0 3 1 1 0 3 2 3 2 3 0 1 0 6 7 8 9 0 0 0 0.50 0.25 1 0 1 0.33 0.75 2 1 0 0.17 0.00 Funny enough, there seems to be a clear pattern: The more siblings you have, the less sleep you seem to be getting. We can of course also calculate percentages. If you are eager to understand how siblings affect the amount of sleep, it might make sense to calculate percentages that add up to 100% in each column. You can calculate cross tabulations independent of whether the data is nominal or any kind of ordinal data. 6.3.1.2 Measures for the Strength of the Association Between Variables Measures that express the association between two variables are typically bound between \\(-1\\) and \\(1\\). By convention, they express how strongly related the variables are. You can find an overview in the following table. Association Measure Interpretation -1 Perfect Negative [-0.99, -0.71] Strong Negative [-0.7, -0.31] Moderate Negative [-0.3, -0.01] Weak Negative 0 No Association [0.3, 0.01] Weak Positive [0.7, 0.31] Moderate Positive [0.99, 0.71] Strong Positive 1 Perfect Positive A positive association means that if the variable \\(X\\) grows, then the variable \\(Y\\) grows as well. In contrast, if the association is negative, then a growth in one variable means a reduction in the other. For example, drinking beer and being thirsty is a negative relationship: the more beer you drink, the less thirsty you are. However, the relationship between a headache the next day and drinking beer is a positive one: the more beer you drink, the larger the headache the next morning. 6.3.1.3 Measuring the Strength of the Association Between Variables Let us begin with nominal variables. As soon as at least one nominal variable is involved in the analysis, we use a measure that is called Cramér’s V. It will return the degree of association between variables. Since nominal variables cannot be ordered, the measures of these associations do not offer any insight about the direction of the relationship. It is therefore bound between \\([0,1]\\). How can you interpret this measure? The closer the measure to \\(1\\), the stronger the association between the two variables; the closer the measure to \\(0\\), the weaker the association between the two variables. Let us take a look at an example: Do coffee drinkers have anything to do with how much you love summer? This is how the data looks like. Table 6.2: Cross Tabulating the variables ‘What You Drink in the Morning’ and ‘How Much You Like Summer’ for the Cohort in 2018, 2019 and 2021 (from Left to Right). 2 3 4 5 Coffee 1 0 2 4 Other 0 0 1 1 Tea 0 3 4 1 3 4 5 Coffee 0 3 11 Other 1 1 2 Tea 0 2 5 4 5 Coffee 2 5 Other 0 4 Tea 0 5 We can calulate Cramér’s V between the preference of the morning drink and the love of summer for all three cohorts now. Cohort 2018: 0.47 Cohort 2019: 0.34 Cohort 2021: 0.43 Here, the quick analysis tells us that there is a moderate relationship at play. It is strongest for the cohort in 2018: It looks as if it were driven by summer loving coffee drinkers vs. tea drinkers who seem to prefer other seasons. Whenever we are measuring the association between two ordinal variables, we want to choose something different. After all, now that the variables can both be ordered, we are able to make a statement about the direction of the association between them. The measure of choice is called Goodman and Kruskall’s gamma. Again, we will treat the exact way it works as a black box for this module. Suffice it to say that it ranges from \\(-1\\) to \\(1\\). We revisit the example on the number of siblings and how much you sleep. This time, we also add data from the other cohorts. Table 6.3: Tabulating the Number of Siblings with the Hours You Sleep at Night for the Cohort in 2018, 2019 and 2021 (from Left to Right). 5 6 7 8 9 10 0 0 0 1 1 0 0 1 0 1 2 2 1 1 2 0 0 1 0 1 0 3 0 1 1 1 0 0 4 1 0 0 0 0 0 5 0 1 1 0 0 0 5 6 7 8 9 10 0 0 1 1 1 0 0 1 1 1 3 2 2 2 2 0 2 3 0 0 0 3 0 0 1 2 0 0 4 0 0 2 0 0 0 5 1 0 0 0 0 0 6 7 8 9 0 0 0 3 1 1 0 3 2 3 2 3 0 1 0 Again, we are eager to understand the strength of the relationship. This time, we use Goodman and Kruskall’s gamma. Cohort 2018: -0.44 Cohort 2019: -0.22 Cohort 2021: -0.64 Overall, the relationship turns out to be negative in all three cases. Interesting! It is weakly negative for the cohort in 2019, moderately negative for the cohort 2018. Your cohort even shows an almost strong negative relationship. Looking at the data from your cohort we can immediately see why: Single childs never sleep less than 8 hours and those with one sibling never less than 7. 6.3.2 Estimating the Parameters in the Population We investigated the strength of the association between variables. Now, let us take a look and study how this plays out in the population. How certain can we be that there is actually a systematical relationship between the variables in the population? For any kind of categorical data—be it ordered or not—we will use statistical test that is called the \\(\\chi ^2\\)-Test (Chi-Squared Test). The way we will use it is very similar to the t-test we met two weeks ago. We first formulate the 0-hypothesis that there is actually no relationship in the data. Then, we take a look at our data. If there were really no relationship in the variables, how likely would it be that we observe the data that we have? The \\(\\chi ^2\\)-test is doing the following. The two cross tabulated variables show result in a matrix with counts. The test looks up how many observations it would expect in each cell of the matrix if a sample of our size were collected, given there was no relation between the two variables. It then compares these values with the data we actually collected and calculates one single statistic—i.e. one single value—from it. This test statistic has a distribution, the \\(\\chi ^2\\) distribution, which means that the test can make a probability statement: Given that the data were random, how likely is it to observe our data? Think of the value for the test statistic as our z-scores from last week. Given a z-score, how likely is it to observe a z-score that is larger on a Normal distribution? If the data we observe is likely, then we will stick to the 0-hypothesis. If our sample were quite unlikely, we instead refute the 0-hypothesis. We would rather believe in the alternative hypothesis. Let us implement a \\(\\chi ^2\\)-test for our data to see the results and also learn how to interpret the results. We begin with the data for 2018. As you can see, the p-value value is at 0.40. Assuming there is no relation in the data, there is an almost 40% probability to observe the data that we do. This is still quite likely which is why we we will stick to the 0-hypothesis: it is quite likely to observe a sample like ours if both variables are not related. ## ## Pearson&#39;s Chi-squared test ## ## data: dat18$siblings and dat18$sleep ## X-squared = 26.208, df = 25, p-value = 0.3965 The data for 2019 looks fairly similar. Now, there is only a 28% probability that we would observe this data if we believe there is no relationship between the variables. However, this is still fairly likely, so again we stick with the 0-hypothesis. ## ## Pearson&#39;s Chi-squared test ## ## data: dat19$siblings and dat19$sleep ## X-squared = 28.742, df = 25, p-value = 0.2748 Lastly, we take a look at your data. Here, we have a p-value of 0.02. The \\(\\chi ^2\\)-test is telling us that it is really unlikely to observe your data assuming that these two variables are not related. We will refute the 0-hypothesis and will go with the alternative hypothesis. The two variables are related—of course in your sample, but also in the larger population of your cohort. ## ## Pearson&#39;s Chi-squared test ## ## data: dat$siblings and dat$sleep ## X-squared = 15.5, df = 6, p-value = 0.0167 Summing up, how do we interpret the results from both tests, the Goodman Kruskall gamma and the \\(\\chi^2\\)-test? Remember that the first test was about the substantive relationship between the variables in your sample. We then tested with the \\(\\chi^2\\)-test whether we believe that the relationship also exists in the respective cohort at large. t We are observing a negatively relationship—with differently strong relationships—for all three data sets. The second test on statistical significance now offers insights about the population behind it. We want to know whether the data that we are observing in the sample is also likely to occur in the population. We have seen that for the two cohorts in 2018 and 2019, it is still quite likely to observe the data even if the two variables are not related. But the statistical signal was not clear enough to really believe that it would also happen in the larger population. The case is different for the 2021 cohort. Again we find a negative relation in the sample, but in addition the signal is so clear that we also believe that this relation should exist in the overall cohort. We refute the null hypothesis, and rather believe in the alternative hypothesis: There is a negative relationship between the two variables in the overall cohort. And the most likely strength is the one we calculate—which is the one we will ultimately believe in. Let us sum up results in a table Which Cohort Relation Between Variables in Sample Relation Between Variables in Population (Cohort) 2018 Data Negative None 2019 Data Negative None 2021 Data Negative Negative 6.4 Continuous Data You have learned how to do this kind of analysis for discrete data. Now let us have fun with continuous data. Again, we first take a closer look at how to describe the strength of the association between two variables. We will then proceed to understand what we can do to see whether this relationship also exists in the population at large. 6.4.1 Describe Again, we will distinguish between two different cases. What can you do when you have continuous data in one variable and discrete data in the other? We actually already did this kind of analysis, but now we can understand how this all fits together in a more systematic way. You will end up with as many continuous variables as you have in your discrete variable. To compare the continuous data for these different groups you simply report a typical value for each group. You could pick the median or the mean—it is up to you. Here is an example from your data: How long is your commute to university? It turns out, it takes the men in the class on average 19 mins. The women, however, take 8.75 mins. This looks like quite a substantive difference. To measure the association between two continuous variables we will correlate them. Ultimately we want to tell how much two variables are jointly different from their respective means. Imagine you have a subject in your data with two data entries. You observe that the person is an outlier on one variable \\(X\\). Is this person also an outlier on the other variable \\(Y\\) you recorded about her? If so, our measure should be high. We will get to our measure, the Pearson correlation coefficient \\(r\\), in two steps. The first thing we have to calculate is the covariance. To what extent do two variables co-vary? The covariance is the statistical summary of the linear association between two interval variables. It measures the variation shared by values in two variables \\(X\\) and \\(Y\\). \\[ cov_{XY} = \\frac{\\sum(y_i - \\bar{y})(x_i - \\bar{x})}{n-1} \\] This clearly reminds you of the variance we calculated in week 2, right? It is basically the same formula, but this time, instead of squaring the product terms, we are taking the product of each the difference from the mean of variable in the numerator. But we cannot compare the covariance to other variables, since it depends on the scale of each specific variable. Ideally we could standardise it to measure on a common scale. To do so we divide the covariance with the standard deviations to get to the correlation. \\[ r = \\frac{\\text{covariance of x and y}}{\\text{(standard deviation of x)(standard deviation of y)}}\\] The maths looks a bit scary—but only at first sight. You have seen all necessary building blocks already. We are just sticking them together here. \\[ r = \\frac{cov_{XY}}{s_x s_y} = \\frac{\\frac{\\sum(y_i - \\bar{y})(x_i - \\bar{x})}{n-1}}{\\sqrt{\\frac{\\sum (y_i-\\bar{y})^2}{n-1}*\\frac{\\sum (x_i-\\bar{x})^2}{n-1}}} \\] The resulting Pearson’s r measures the strength of the association between two variables still between [-1, 1]. In 6.1 you can see how it would look like for the ideal cases: on the left r = -1, in the middle r = 0 and on the right r = 1. Figure 6.1: Values for Pearson’s r. A quick example where we calculate Pearson’s correlation coefficient for data from our survey. Figure 6.2: Values for Pearson’s r Using Our Survey. It does make sense that the two study variables are ‘connected.’ But what is the exact mechanism that connects them? Well that depends on the theory why they are connected. The most convincing story to tell is that there is probably a general attitude towards university studies. It affects the effort for your studies and also drives the effort you think you should be putting into your secondary education. Note that the correlation between the two variables does not provide you an answer for the reason why an association between variables exists. It is the task of your theory to tell a convincing story. As such, correlation between variables does not proof of the validity of any causal claim. But it is an observable implication of a theory and therefore a good hint that your reasoning is correct. Figure 6.3: Source: https://xkcd.com/552/ 6.4.2 Measures About the Population Last bit for today. We measured the association between continuous variables and other continuous or categorical variables in the sample. But we still want to make statements about their relationship in the population. Is the evidence that we are observing in our sample strong enough to be able to make a claim about what is going on in the general population? Let us first take a look at a combination of categorical variables and continuous data. We already covered this test two weeks ago when we spoke about t-tests. Let us use it to analyse whether it is likely that there is a difference in the time it takes to commute between men and women—not only between you in this module, but in your cohort overall. This is what the t-test is telling us. ## ## Welch Two Sample t-test ## ## data: dat$commute by dat$gender ## t = -1.838, df = 7.8991, p-value = 0.1038 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -23.138267 2.638267 ## sample estimates: ## mean in group Female mean in group Male ## 8.75 19.00 To interpret the results from this test, I will do a quick video this time. We also studied the relationship between continuous variables. First between the two study variables. In addition, we also took a look at the study variable and the commute variable. This time, the statistical test is the correlation test. Without going into detail, its interpretation is very similar to what we have seen in the t-test. Again, take a quick look at the video to see how to interpret these results. ## ## Pearson&#39;s product-moment correlation ## ## data: dat$studyperweek and dat$studyideal ## t = 4.338, df = 14, p-value = 0.0006815 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4187034 0.9109895 ## sample estimates: ## cor ## 0.7572408 ## ## Pearson&#39;s product-moment correlation ## ## data: dat$studyideal and dat$commute ## t = 1.3255, df = 14, p-value = 0.2062 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.1938747 0.7118046 ## sample estimates: ## cor ## 0.3339222 6.5 Code This week is again very light on code. R is a statistical software, so all the things that you have been learning today are really straightforward. All R code is in the verbatim font and lightly shaded like so: This is R code Combination of Variables Describing Association in Sample Testing for Association in Population At least 1 nominal variable Cramér’s V \\(\\chi^2\\)- test CramerV(var1, var2) chisq.test(var1, var2) 2 ordinal variables Goodman Kruskal’s Gamma \\(\\chi^2\\)-test GoodmanKruskalGamma(var1, var2) chisq.test(var1, var2) 1 categorical and 1 continuous variable Difference in Means t-Test: mean(group1) - mean(group2) t.test(continuousvar ~ categoricalvar) 2 continuous variables Pearson’s r Correlation Test cor(var1, var2) cor.test(var1, var2) 6.6 Reading for This Week The reading for this week is chapter 10 in Fogarty (2019). "],["comparing-apples-and-oranges.html", "Chapter 7 Comparing Apples and Oranges 7.1 Bivariate Regression 7.2 Beyond Bivariate Regression 7.3 The Relevance of Statistical Control 7.4 R Code 7.5 Reading for This Week", " Chapter 7 Comparing Apples and Oranges All the tools that we have so far are nice and good. But there are more things that we would like to do with data. So far, we learned how to make statements about bivariate relationships. But very often, we want to go beyond this. In our data, we might be interested to know the relationship between how many hours you study and how often you go out per week. But it might be, that there are also other factors that drive how many hours you actually study. We could be interested to know whether drinking coffee makes any difference for example—and that irrespective of how often you go out per week. After a late night out, you might be more inclined to have coffee in the morning, so just measuring the coffee is not enough. We will have to control for your partyness. So we want to have a framework that allows us to do the following. Measure the strength of association between more than 3 variables in the sample, controlling for all factors. Measure the strength of association between more than 3 variables in the population, controlling for all factors. Measure the uncertainty we have about these association in the population. 7.1 Bivariate Regression The framework for all that is called regression analysis. Let us build it up one step at a time. What is Regression Analysis? In the literature you can find many descriptions. “Regression analysis involves making quantitative estimates of theoretical relationships between variables.” “A technique which involves fitting a line to a scatter of points.” “Regression represents a most typical association between two variables and can be considered analogous to the mean in univariate descriptive statistics.” “Regression tries to explain changes in one variable (dependent) as a function of changes in other variables (independent).” “Causality is implied.” At the end of the day, what regression does is to fit a line that is closest to all observations. Which one of the three would you pick? We want to chose the one that fits the data best so that we can express the relationship between the variables with a simple formula. \\[ Y = \\alpha + \\beta X \\] with Y the dependent variable X the explanatory variable \\(\\alpha\\) the intercept \\(\\beta\\) the slope Fitting the line is easy with well behaved data. These are three different intercepts \\(\\alpha\\) and we fit them to their respective data. And here is how we could also fit different slopes \\(\\beta\\) through the data. 7.1.1 How Does It Work? The big question of course is, how does this fitting work? The first thing we have to understand is that there will be an error to our estimation. If an association between two variables were perfect, we would speak of a deterministic relation. Take for example the conversion of temperature. When we want to calculate the temperature in Fahrenheit from Celsius, we can follow the easy formula \\[ \\text{Fahrenheit} = 32 + (9/5 ∗ \\text{Celsius})\\]. Here, \\(Y\\) can be perfectly predicted from \\(X\\) using an \\(\\alpha = 32\\) and a \\(\\beta = 9/5\\). In practice things often follow probabilistic relations which are inexact, so we need to account for this mistake with an error and add \\(e\\) to the formula. \\[ Y_i = a+ b X_i + e \\] If we fit such a line, we can calculate residuals. They are the difference between the actual value of \\(y_i\\) and the predicted value \\(\\hat y_i\\) from the regression model. In the figure, you can see the residuals as red lines. \\[y_i − \\hat y = e_i\\] 7.1.1.1 Fitting the Line We can find the perfect line by fitting the line with a technique called Ordinary Least Squares (OLS). We choose the line that minimises the sum of the squares of the distances between each observation and the regression line. \\[ \\sum (y_i-\\hat{y_i})^2 = \\sum {e_i}^2 \\] \\(\\sum {e_i}^2\\) is also called the Residual Sum of Squares (RSS). To fit the regression line, we minimise the RSS. This is it. Implementing this routine means we minimise the distance between our observation and the predictions on the line. In the figure above, we would minimise the (squared) length of the red lines connecting the predictions in yellow and the real data in blue. Using this OLS estimator, we can calculate the slope \\(b\\). \\[ b = \\frac{\\text{covariation}}{\\text{variance}} = \\frac{\\sum (x_i-\\bar{x})\\sum (y_i-\\bar{y})}{\\sum (x_i-\\bar{x})^2} \\] Give we now know the slope \\(b\\) the intercept \\(a\\) is then really straightforward and \\[ a = \\bar{y} - b\\bar{x} \\]. Let us calculate the parameters for a small data set with a regression by hand. 7.1.1.2 Inference We fitted a line through our sample. We now know how to guess the best fitting line through our data. What can we say about the population that is behind our sample? We will proceed in a similar way as we did when we were estimating the mean for a population last week. Of course, we do not know the population parameter for the intercept \\(\\alpha\\) and the slope \\(\\beta\\) and we actually will never be able to know it unless we were to ask every single individual in the population. But given that we have a sample, we already have a good guess. The most likely values \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) are the ones that we can calculate using our sample, so \\[ \\hat \\alpha = a \\] and \\[ \\hat \\beta = b \\]. 7.1.1.3 Uncertainty We also want to be realistic about the uncertainty of our estimate. Remember that we were calculating a standard error for a sample mean? And that we used it to then calculate the confidence interval around our sample mean? Back then, we used the standard deviation from our sample as the best guess. We will do the same for the two parameters \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) here. Let us begin with the standard error for our estimated slope \\(\\beta\\). \\[ se(\\beta_1) = \\frac{\\sigma_\\epsilon}{\\sqrt{\\sum{(x_i-\\bar x)}^2}} \\] This is how to calculate the uncertainty for the intercept \\(\\alpha\\). \\[ se(\\beta_0) = \\sqrt{\\frac{\\sum{x_i^2}}{n \\sum{(x_i-\\bar x)^2}}} \\sigma_\\epsilon \\] Of course, we do not know \\(\\sigma^2_\\epsilon\\). But we will use the same trick like before and take the variance of the regression residuals. \\[ \\hat\\sigma_\\epsilon = \\sqrt{\\frac{\\sum \\epsilon_i^2}{n-p}} \\] This yields the estimated standard error of the estimate—here \\(\\hat\\beta_1\\). \\[ \\hat{se}(\\hat\\beta_1) = \\frac{\\hat\\sigma_\\epsilon}{\\sqrt{\\sum{(x_i-\\bar x)}^2}} \\] With this estimated standard error, we can now calculate a t-score. \\[ t = \\frac{\\hat \\beta}{se(\\hat \\beta)} \\] This t-score is conceptually quite similar to the z-scores we already know. It expresses a parameter that is standardised using its standard error. The beauty is that we know from statistical theory how it is distributed: It follows a t-distribution. This distribution is quite similar to the normal distribution, it is also bell shaped and symmetric. Given that we know its distribution, we can again calculate p-values. We interpret them in just the same way as we were interpreting them for the means in a sample—this time however with regards to the regression parameters. Assuming that the true, unknown population parameter \\(\\beta\\) is actually 0, how likely is it that we observe the estimated parameter \\(\\hat \\beta\\) that we do? 7.1.1.4 Assumptions Just to mention, since we will not go into details here: We can unrol all this framework because we are making a number of assumptions about the data. The relationship between the dependent variable and the explanatory variables needs to be linear. The variance of the residual error must be constant The residual error must be normally distributed with a mean of 0. The expected value of the residual error must not depend on the data. 7.1.2 How Do We Interpret Results? That was quite a bit. Let us take a look at some data to learn how to interpret results. ## ## Call: ## lm(formula = dat$studyperweek ~ dat$studyideal) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.842 -2.950 1.101 2.910 4.884 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.5548 3.0245 0.183 0.857082 ## dat$studyideal 0.6115 0.1410 4.338 0.000682 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.999 on 14 degrees of freedom ## Multiple R-squared: 0.5734, Adjusted R-squared: 0.5429 ## F-statistic: 18.82 on 1 and 14 DF, p-value: 0.0006815 The core idea of interpreting these results is that we can take a look at two things. What is the substantive effect? Remember that we have parameter estimates for our equation \\(Y = \\alpha + \\beta X\\). In addition, there is also the uncertainty around the parameter estimates that we will have to account for. I am walking you through the interpretation via a video here. 7.2 Beyond Bivariate Regression But the real reason we are here is to go beyond bivariate regressions. 7.2.1 Bivariate Regression Plus a Dummy What is a dummy variable? Qualitative information often come in the form of binary information (zero-one variable). We call these kind of variables a dummy variable. The benefit of capturing qualitative information using zero-one variables is that it leads to regression models with easy interpretations. Good coding practice: name your variable after the ‘1’ category; e.g. Female if the variable is coded 1 for female and 0 for male. Gender would be a confusing variable name. 7.2.1.1 How Does It Work? Let us assume we examine the relationship between education and income among women and men and collect the following data. Figure 7.1: Data for Income, Education and Gender. Note: Artifical Data. We want to examine the relationship between education and income among women and men. Our model will be as follows \\[Income = \\beta_{0}+\\beta_{1}*Education+\\beta_{2}*Female \\] 7.2.1.2 How Do We Interpret Results? We can interpret \\(beta_0\\) and \\(beta_1\\) as before. But now there is also \\(beta_2\\). Its interpretation is fairly straightforward: it is nothing different than an intercept shift. Suppose we fit the following regression model to our data: \\[ Income = 25934 + 894*Education - 3876*Female \\] We can see two regression lines, one for males and one for females. For the women we calculate \\[ Income = 25934 + 894*Education - 3876*1= 22058+894*Education \\] And for the men the equation looks like follows. \\[ Income = 25934 + 894*Education - 3876*0 = 25934 +894*Education \\] Figure 7.2: Regression through the data for Income, Education and Gender. Note: Artifical Data. For example, we expect that a woman with 15 years of education earns earns on average. \\[ Income = 25934 + 894*15 - 3876*1 = 35468 \\] 7.2.2 Multivariate Regression We can of course go beyond just adding one dummy variable and one continuous variable to the regression equation—in fact you can add more than one from any variable to the regression equation. In the following example we will take a look at how that could look like. 7.2.2.1 How Does It Work? Let us examine the relationship between economic growth prior to an election, the approval for the president and the vote share received by the incumbent presidential party in the US. This is how the data looks like. Figure 7.3: Data on Presidential Elections in the US. The vote share is on the vertical axis. We also have data for approval rating and for economic growth on the other two axes. Our model then becomes: \\[ \\text{vote} = \\beta_{0} + \\beta_{1} \\text{growth} + \\beta_{2} \\text{approval} + e \\] When we estimate regression model with two explanatory variables, we are effectively estimating the best fitting plane through the three dimensional data cloud. This works just as before: we will use the OLS estimator. 7.2.2.2 How Do We Interpret Results? These are the results of our regression equation. ## ## Call: ## lm(formula = us$vote ~ us$growth + us$approval) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4076 -1.1555 -0.8039 2.1181 4.2504 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.82929 2.77295 12.560 2.90e-08 *** ## us$growth 0.81459 0.27126 3.003 0.011 * ## us$approval 0.31950 0.05603 5.702 9.88e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.644 on 12 degrees of freedom ## Multiple R-squared: 0.808, Adjusted R-squared: 0.776 ## F-statistic: 25.25 on 2 and 12 DF, p-value: 5.01e-05 For the interpretation, let us visit a quick video again. 7.3 The Relevance of Statistical Control When we plot data, we usually do this in two dimensions. Any relationship between those two variables is then easy to observe. However, if more dimensions belong to the data set, things might be a bit more tricky. When we analyse data, each variable that we have in our spreadsheet adds another dimension to the data overall. We already saw how this can look like in the example with the presidential elections. For the data analyst, this means that the true relationships might hide themselves in the multiple dimensions. To illustrate that, here is some artificial data that I made up for you. You can see relationship between education in years and income per year. The relationship seems to be fairly clear: there is a negative relationship in the data. The more you are educated, the less you earn. Substantively, this looks a bit odd, right? But if you were a naive data analyst, you might still take the data for granted. You could even run a regression on that and get a clear result. ## ## Call: ## lm(formula = income ~ education) ## ## Residuals: ## Min 1Q Median 3Q Max ## -33304 -7259 465 7609 26309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 54534.1 2947.3 18.503 &lt; 2e-16 *** ## education -910.5 275.5 -3.304 0.00113 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11350 on 196 degrees of freedom ## Multiple R-squared: 0.05276, Adjusted R-squared: 0.04793 ## F-statistic: 10.92 on 1 and 196 DF, p-value: 0.001133 Data cannot lie, right? So this the higher the education, the less you earn. This is what you would believe? Let us dig a bit deeper. In our data, we also have a variable that stores the information about the gender of the individuals. Let us add this information with some colour to our data. Now, the picture looks quite different. There is a clear pattern going on. First, men seem to be earning much more. In addition, there is also something standing out clearly in the data: men have less education. The multivariate regression tells us the same story. ## ## Call: ## lm(formula = income ~ education + male) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22687.6 -5649.2 499.5 6086.9 19731.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17211.1 3478.0 4.949 1.61e-06 *** ## education 1576.6 270.3 5.833 2.24e-08 *** ## male 21517.8 1589.0 13.541 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8169 on 195 degrees of freedom ## Multiple R-squared: 0.5118, Adjusted R-squared: 0.5068 ## F-statistic: 102.2 on 2 and 195 DF, p-value: &lt; 2.2e-16 Men earn on average 21517.79 more than women—controlling for any influence from education. And for each year in education, you earn 1576.56 more—and here we take the gender of the individual into consideration. Statistical control allows us to make these statements about the relationships of variables. But we not only restrict ourselves to two variables, we can account for the influence of every other variable that we have in the equation, too. 7.4 R Code The R code we are learning is really simple this week. Remember, R is built for statistics, so estimating a regression is as simple as follows. # Load the data load(&#39;preparing_data/data/uspresnew.Rdata&#39;) # estimate a model model1 &lt;- lm(us$vote ~ us$growth + us$approval) # Get the resuilts from the regression summary(model1) ## ## Call: ## lm(formula = us$vote ~ us$growth + us$approval) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4076 -1.1555 -0.8039 2.1181 4.2504 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.82929 2.77295 12.560 2.90e-08 *** ## us$growth 0.81459 0.27126 3.003 0.011 * ## us$approval 0.31950 0.05603 5.702 9.88e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.644 on 12 degrees of freedom ## Multiple R-squared: 0.808, Adjusted R-squared: 0.776 ## F-statistic: 25.25 on 2 and 12 DF, p-value: 5.01e-05 If you save the model summary, you can easily extract some results for later use. For example, you could use the parameters to plot a regression line through your data—here for the bivariate case. model2 &lt;- lm(us$vote ~ us$approval) summary.model2 &lt;- summary(model2) a &lt;- summary.model2$coefficients[1] b &lt;- summary.model2$coefficients[2] plot(us$approval, us$vote, pch = 16, xlab = &#39;Approval&#39;, ylab = &#39;Vote Share&#39;, las = 1) abline(a=a, b=b, col = cardiffred) 7.5 Reading for This Week This week, please take a look at chapter 11 in Fogarty (2019). "],["the-fundamental-problem-of-causal-inference.html", "Chapter 8 The Fundamental Problem of Causal Inference 8.1 Studying Causal Effects 8.2 Key Concepts 8.3 The Fundamental Problem of Causal Inference 8.4 Randomized Control Trials 8.5 Estimating a First Causal Effect 8.6 Plotting Data on Maps 8.7 Reading for This Week", " Chapter 8 The Fundamental Problem of Causal Inference 8.1 Studying Causal Effects In the last section of this module, we will take a closer look at policy evaluations. We want to see whether a policy actually did have any effect. Was the outcome what policy makers intended? Or was it different? These questions will concern any society, since studying the effect of policies is the foundation for any good governance. But evaluating policies comes with an important challenge. It is only possible to observe the world in two stages. Either the policy was enacted and we now see how the world developed with it. Or the policy was not enacted and we observe the world without it. What was the impact of the policy? The answer is the difference between these two worlds—the world with and the world without the policy. For example, imagine a policy introduced mandatory environmental protection for farm land. A year later, biologists measure that harmful substances in ground water are down by 20%. Was this the effect of the policy? Or was it maybe just a particularly rainy year that washed all pollutants away? We could only tell if we observed the world in both stages: farming with the new environmental protection and farming without it. In this last section we will learn how to think systematically about this problem. We will also take a look at some easy solutions to this problem. To further introduce this new section, I was fortunate to interview Claire Rowlands. She is the Strategy and Policy Director for the COVID Vaccination Programme in the Welsh Government. Of course, policy evaluation plays a crucial role in fighting the pandemic: measures are updated regularly and have to be adjusted regarding their impact. A rigorous and analytical evaluation of these policies and their impact is therefore key. 8.2 Key Concepts We have already seen that there are a couple of new concepts that we should define. Let us introduce some terms. Treatment is the actual intervention. The causal inference framework has its roots in experimentation and in particular the life sciences, which is why we speak of the treatment here. The Counterfactual We can observe the world as it is. We can only hypothesise about a different counterfactual world where a key condition is different. Potential Outcomes is the outcome of the world in the presence or the absence of a treatment. Causal Effect In this framework, we care for causality from a particular view. The causal effect is the difference between the two potential outcomes of the world. 8.3 The Fundamental Problem of Causal Inference Let us think a bit more rigorously about the potential outcomes framework. We first need a treatment \\(T\\). In the light of the treatment there are two possible outcomes for our dependent variable \\(Y\\). In theory, we would love to observe both of them: the outcome under the treatment \\(Y(1)\\) and also the outcome of the same unit we are studying without the treatment \\(Y(0)\\). We will often collect data about more than one observation, so we index everything with \\(i\\) resulting in the treatment \\(T_i\\) and the two potential outcomes will be \\(Y_i(1)\\) and \\(Y_i(0)\\). To calculate the causal effect of the treatment \\(T_i\\), we subtract the two potential outcomes from one another: \\(Y_i(1) - Y_i(0)\\). Done. The problem is, we can never observe the same unit \\(i\\) in both conditions. As illustrated in the small table, we can only observe a unit either under the treatment condition \\(T_i=1\\) or the control condition \\(T_i=0\\), but never under both at the same time. ID Outcome with Treatment Outcome without Treatment Treatment Variable 1 Y(1) ? T=1 2 Y(1) ? T=1 3 ? Y(0) T=0 … … … … n ? Y(0) T=0 This framework tells us already a lot about the potential research that we can implement with it: it is necessary to be able to manipulate the treatment variable \\(T\\). Think back at the data we collected about you. From the point of this framework, could we study the causal effect of gender on for example much you spend on average when you go out? No, we could not because it is impossible to change gender in an experimental setting. Does that mean that this framework is unable to study any gender related questions? This is not the case: we need to think carefully about what it is that we are actually interested in—and then manipulate this condition. For example, if we are studying how the perception of gender might affect certain outcomes, we could use a clever research design that helps manipulate this gender perception. 8.4 Randomized Control Trials So what is our way out? How can we estimate causal effects? A very straightforward way to calculate the average treatment effect in a sample is to randomise the treatment. These randomised experiments are called Randomised Control Trials (RCT) and they are the working horse in the Life Sciences and Psychology. They have recently also found increasing attention in those disciplines who study societies at large like Economics, Sociology or Political Science. RCTs are often considered as the gold standard, because they have a lot of internal validity: If randomization is done correctly, they reliably produce an average treatment effect. The big question is whether this validity within the study can be extrapolated to the society at large. For that, researchers need to show that a study also has external validity: the sample that is being studied actually needs to represent the population at large. Ideally, we want to calculate the Sample Average Treatment Effect (SATE) as follows. \\[ \\text{SATE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i(1)-Y_i(0)\\right) \\] However, as mentioned before, we cannot observe the individuals \\(i\\) under both conditions. But what we can do in an RCT is to randomise our treatment across individuals. To do so, we chose those who receive the treatment and those who are in the control group by coincidence. If this condition is satisfied, we can actually expect that the inviduals in both groups are comparable on average. Randomisation makes sure that participants in the treatment group and the control group share on average the same traits. In short, if we randomise the treatment, we can compare the results of both groups with one another. The causal effect from the treatment then simply boils down to the difference in means. We also alredy know from previous weeks how to do inference and express our uncertainty for estimates by simplyt using t-tests. One final note here. We are observing the average treatment effect. Does that mean that the treatment will have the same effect for all individuals? No, it will not. Using randomised control trials in this way we can only estimate the average effect across all participants in a study. 8.5 Estimating a First Causal Effect I sent out an email in Week 8 asking you for more data on the effort you are putting into this module and the effort you think you should be investing into the module. In these emails, I hid a quick experiment: I wanted to investigate whether priming you for how difficult this class is makes any difference regarding the effort you are reporting or the effort you think you should be investing. This was the email to the control group: Hi everybody, To fine tune the material for PL 9239, I would like to collect some more data on your study effort. Could you please fill out the two short questions at the following link until tomorrow Thursday night? Thanks a lot! Best, Christian And here is the email to the treatment group with the extra prompt in bold so that you can easily see it here. Hi everybody, To fine tune the material for PL 9239, I would like to collect some more data on your study effort. I am aware that the module does indeed require quite a bit of work to keep up with the comprehensive material. Could you please fill out the two short questions at the following link until tomorrow Thursday night? Thanks a lot! Best, Christian Let us go and analyse whether this prompt actually made any difference. Note that I excluded one outlier in the control group who is putting in 50h per week and thinks that s/he should be spending 56h. (Whoever it is, feel free to get in touch if this is PL9239 causing all the work! I would love to know more.) To analyse the data, let us first plot the data. On the left, you can see the effort that you are actually putting into your studies in both groups. In the middle, you can see what you think that you should be studying. On the right we are plotting the difference between what you think that you should be studying and what you are actually studying. The differences are quite striking in our little experiment! Let us begin with the actual hours that you are studying. The treatment group studies on average 1.67h more. For the number of hours that you think you should be studying, the difference is also positive: Those in the treatment group think they should study on average 2.67h more. Finally, we can also calculate the difference between the hours that you think you should be studying and the actual hours that you are studying. Think of it as an expression about how ‘bad’ your conscious is. Do we find an effect here, too? We do. The difference between the treatment group and the control group is 1h—telling you that the module requires a lot of work indeed causes you a bad consciousness. Now, what about statistical significance? Could we trust that we would find this in the larger population if we ran the experiment there? All t-test report a unanimous result: the result is not statistically significant. The reason is quite simple: your sample is way to small. Ideally we have at least 100 participants in the controlgroup and another 100 in the treatment group to get nice and small 95% confidence intervals. ## ## Welch Two Sample t-test ## ## data: dat.control$studyperweek and dat.treatment$studyperweek ## t = -0.42273, df = 4.1011, p-value = 0.6937 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.507561 9.174228 ## sample estimates: ## mean of x mean of y ## 13.33333 15.00000 ## ## Welch Two Sample t-test ## ## data: dat.control$studyideal and dat.treatment$studyideal ## t = -0.44582, df = 5.2124, p-value = 0.6736 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -17.85591 12.52257 ## sample estimates: ## mean of x mean of y ## 22.83333 25.50000 ## ## Welch Two Sample t-test ## ## data: dat.control$studydifference and dat.treatment$studydifference ## t = -0.33457, df = 7.5413, p-value = 0.7471 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.966007 5.966007 ## sample estimates: ## mean of x mean of y ## 9.5 10.5 8.6 Plotting Data on Maps There is nothing really new this week regarding code, so let us refine your plotting skills. Today we will take a start at plotting maps. I used the stop and search data that is published online by the police. For this introduction, I am using the data from December 2018 before the pandemic which you can download here if you want to play around with it yourself. The basic idea is pretty simple: We have to load a map of Wales from a package. Then we can plot our stop and search incidents on that map using the longitude and latitude data. # Stop and Search Data load(&quot;stop_and_search_18_12.RData&quot;) # First, we want to have a map library(rworldmap) library(rworldxtra) # we don&#39;t want to take a look at the whole world - # just the bits on the map that have our data points # so we take the min and max min.long &lt;- min(dat.sas$Longitude, na.rm = TRUE) max.long &lt;- max(dat.sas$Longitude, na.rm = TRUE) min.lat &lt;- min(dat.sas$Latitude, na.rm = TRUE) max.lat &lt;- max(dat.sas$Latitude, na.rm = TRUE) # Get a map newmap &lt;- getMap(resolution = &quot;high&quot;) plot(newmap, xlim = c(min.long, max.long), ylim = c(min.lat, max.lat)) # Add the incidents points(dat.sas$Longitude, dat.sas$Latitude, col = cardiffred) This is it—as simple as that. Go wild and explore the data that is available on the police’s website! 8.7 Reading for This Week This week, we will get to know a new book. Please read p.32–53 in Imai (2018). "],["causal-statements-from-observational-data.html", "Chapter 9 Causal Statements from Observational Data 9.1 This Week’s Data: The Effect of the Welsh Circuit Break 9.2 Compare Treated from One Group to the Non-Treated of Another Group 9.3 Compare the Treated Before and After their Treatment 9.4 Best of Both Worlds: The Difference-in-Difference Estimator 9.5 Reading for This Week", " Chapter 9 Causal Statements from Observational Data Last week, we were taking a look at randomised control trials. While they are of course great in generating internal validity, they are not always easy to implement and we may need alternatives for practical reasons. Let me help frame this week’s discussion in this video. 9.1 This Week’s Data: The Effect of the Welsh Circuit Break Let me introduce the data that we will be using for this week. I downloaded data on manual ventilation beds due to COVID-19 in autumn 2020. The data from Wales is publicly available here and the data for England can be downloaded here. I then used the overall population of Wales and England to calculate how many MV beds are being used per 1,000,000 inhabitants. The Figure is plotting the resulting daily time-series from September 1st 2020 until the end of 2020. What you can see is that the numbers were increasing from September onwards. In October, the Welsh government announced the Circuit Break that would start on October 23rd and everybody had to stay at home. The way Corona works, it is reasonable to assume a four weeks lag before the policy is actually becoming effective. Apparently, between being infected and being able to transmit the virus further on, there is normally a one to two weeks lag. And then you can add another two weeks until the critical patients become so severely ill that the patient would have to use an MV bed. So the date that we should pay attention to when we want to understand whether the policy has become effective is around November the 20th. England introduced a stay home order as well, but this only came effective on November the 5th, so almost two weeks after. It is reasonable to assume that this policy will take effect only in the first week of December. We will use this example to study the causal effect of the Welsh circuit break. How did the policy affect the pandemic situation measured by the use of envy beds per 1,000,000 population? Remember that our goal is to calculate the treatment effect from a policy. What difference does the policy make? The definition of the causal effect means that we are interested in the difference between the world as it is with the new policy and the world without that policy. Obviously, both states cannot be observed at the same time. 9.2 Compare Treated from One Group to the Non-Treated of Another Group One solution to this problem would be to compare units that have been treated to units that have not been treated. In a policy context, we could for example compare the country where a policy is enacted with another country that does not have this policy. What would that mean for the Welsh circuit breaker? We could compare the policy outcome in Wales to what is going on in England. The calculation is actually pretty straightforward: we take the seven day average of Wales during the week of November 20th (17.36) and compare it to the same time in England (22.35). The difference between the two would be our estimate for the causal effect: -4.99 MV beds per 1 Million inhabitants. All this rests on a fairly strong assumption: we are actually comparing Wales to England. Now, is that a fair comparison to make? You might rightfully say that this might not be a good idea, simply because Wales and England are different on very many levels. From a more systematic point of view, we might fall prey to an important bias that can influence your results: the confounding bias. Confounding bias happens when a pretreatment variable is related to the treatment and the outcome at the same time. In our case, this could be for example the quality of the NHS. A better performing Welsh NHS would inform Welsh politicians in a better way and therefore affects the likelyhood that a new policy is enacted. At the same time, a better NHS of course can care for its patients in better way, which is why it is also likely to affect the outcome. There are ways to control for this influence and we will take a closer look at them next week. But for now suffice it to say that it is not a good idea to “just” compare two different countries, one with the treatment and one without. 9.3 Compare the Treated Before and After their Treatment So comparing different countries does not make a lot of sense. But what about comparing a unit with a treatment to its state just before the treatment? Or, in the context of our policy evaluation, why not comparing the country with a policy enacted with the same country, but before the policy actually came into force? Indeed, comparing the same units before and after policy intervention will make sure that the units are the same. But, are they really the same? Note that meanwhile some time has gone past. This means that a lot of things apart from our new policy might have changed and the environment for the policy might be a different one. Let us take a look at the Welsh MV beds again, in particular around the 20th of November at the finely dotted line. Compare the figures to what happened on October 23rd when the Wales circuit break actually came into force. During these two points in time, the number of empty beds actually was rising and did not remain flat or was even going down. This is simply due to the fact that the whole environment in Wales changed and the Corona situation was completely different then. To estimate the causal effect in this way, we would take the seven day average of Wales during the week of November 20th (17.36) and compare it to the week of October the 23rd for example (10.98). The difference now would be 6.38 MV beds per 1 Million inhabitants. Any intuition with the numbers would tell us that this cannot be correct. What happens is that we are again observing a confounding factor that biases the estimation of the causal effect. This time it is a time-varying confounder. In short, we need something that can clearly combine the best of both worlds: making sure that we neither bias our results due to pre-treatment counfounders nor time varying confounders. 9.4 Best of Both Worlds: The Difference-in-Difference Estimator Can you have it both ways? Indeed, you can. The estimator that will solve a lot of our problems is called the difference-in-difference estimator. What it does is it calculates the difference between the treated group before and after the treatment and subtracts from it the difference in the control group before and after the treatment. This is the formula for the difference-in-difference estimator. You can use it to calculate the sample average treatment effect for the treated (SATT). \\[ \\text{DiD estimate} = \\left(\\overline{Y}_{\\text{treated}}^{\\text{after}} -\\overline{Y}_{\\text{treated}}^{\\text{before}} \\right) - \\left(\\overline{Y}_{\\text{control}}^{\\text{after}} -\\overline{Y}_{\\text{control}}^{\\text{before}} \\right)\\] Here is a quick video where I explained the core intuition and show how to calculate everything on the basis of the figure below. How do we get around the two assumptions? As explained in the video: We account for the time varying confounders by comparing the difference between two times two points in time. We account for confounding pretreatment variables by comparing the difference between two times two points in time. Remember that we buy this leverage by making a third assumption: the outcome variables are following a parallel trend without the treatment. How can you check this assumption? Simply look at the data. Given what we see in our figures, this assumption is met fairly well. Even better: for the MV beds in our sample it is not only a similar trend—which means that both lines should have the same slope—both curves actually overlap pretty much. This parallel trend assumption is therefore really well fulfilled for our example. Let us calculate the SATT. We take the seven day average of Wales during the week of November 20th (17.36) and subtract the seven day average of Wales during the Week of October 23rd (10.98), which gives us a difference of 6.38. Then we do the same for our control group England. We calculate the seven day average of England during the week of November 20th (22.35) and subtract the seven day average of England during the Week of October 23rd (10.91), resulting in a difference of 11.44. Last step: To calculate the sample average treatment effect of the treated (SATT), we subtract the two differences from one another once more 6.38 - 11.44 = -5.06 MV beds per 1 Million inhabitants. Note that this result is remarkably close with our very first result when we simply compared Wales to England. Why is this the case? It is, because Wales and England not only share the same trend before the treatment, but their values are actually really close to one another. 9.5 Reading for This Week To learn more about all this and going through everything with yet another great example, please read p.54–63 in Imai (2018). "],["multivariate-regression-and-heterogenous-treatment-effects.html", "Chapter 10 Multivariate Regression and Heterogenous Treatment Effects 10.1 Analysing Experimental Data Revisited 10.2 Controling for Confounders 10.3 Treatment Heterogeneity 10.4 Outro 10.5 Reading for This Week", " Chapter 10 Multivariate Regression and Heterogenous Treatment Effects 10.1 Analysing Experimental Data Revisited To analyse experimental data, we so far did an easy calculation. We subtracted the average of the control group from the average of the treatment group. It turns out, that you can analyse experimental data with the regression framework from our second section in this class, too. Let us take a look at this comparison with an example. We want to understand the causal effect of unemployment training. For this purpose, I generated some artificial data for illustration—so no real life data today. In our scenario, people have been unemployed for three months. After three months, we randomly assign a treatment. Half of the unemployed will get a four weeks class that improve on their IT skills. The other half continue in the unemployment programme as is. The outcome we are interested in is the monthly income six months into the new job. This is the result from the (hypothetical) experiment. As you can see, there is indeed evidence that the training did have an effect. How again did we get to the effect that is caused by the training for the unemployed? We simply calculated the mean of both groups and subtracted them from one another. So we take the average monthly income after 6 months in the new job of those who participated in the training 2850.1 GBP and subtract from it the average monthly income after 6 months in the new job of those who did not participate in the programme 2271.81 GBP. The difference between the two is our causal effect of the programme: mean(reg.dat$wage[reg.dat$treat == 1]) - mean(reg.dat$wage[reg.dat$treat == 0]) ## [1] 578.2929 But we can get to this result also with a regression analysis: regression.result &lt;- lm(reg.dat$wage ~ reg.dat$treat) summary(regression.result) ## ## Call: ## lm(formula = reg.dat$wage ~ reg.dat$treat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1701.90 -494.83 22.38 471.43 1960.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2271.81 29.28 77.58 &lt;2e-16 *** ## reg.dat$treat 578.29 41.41 13.96 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 654.8 on 998 degrees of freedom ## Multiple R-squared: 0.1634, Adjusted R-squared: 0.1626 ## F-statistic: 195 on 1 and 998 DF, p-value: &lt; 2.2e-16 Why do we get to the same results with both ways of calculating it? Remember that for the regression we are calculating the following regression model \\[ Y(X) = \\alpha + \\beta X + \\epsilon. \\] We interpreted the estimate of the parameter \\(\\hat\\beta\\) as follows. It is the change of the result in \\(Y\\) if we change the value of \\(X\\) by one unit. In our context this means the change from the control to the treatment group. Let us consider the effects for the two groups, the estimated outcomes in the treatment group \\(\\widehat{Y(X=1)}\\) and the estimated outcomes in the control group \\(\\widehat{Y(X=0)}\\): \\[ \\widehat{Y(X = 1)} - \\widehat{Y(X=0)} = (\\hat\\alpha - \\hat\\beta) - \\hat\\alpha = \\hat\\beta. \\] So, indeed, the estimated difference between the two outcomes boils down to the estimated parameter \\(\\hat\\beta\\). 10.2 Controling for Confounders OK, so the difference between the control group and the treatment group can also be calculated with a regression. Fancy enough. But why going through all this trouble if we can simply subtract the means of two groups? Isn’t that much more easy? Knowing how to calculate causal effects with the regression framework can be useful in different ways. Let us begin with the problem of confounders. Remember that for our estimator to work, we had to make an important assumption: the treatment assignment really has to be random in both groups. If the assignment is not fully random and the outcome depends on another variable, then we run into problems. In the context of our running example, we might have collected information on other variables. For example, we recorded participants’ gender and also the number of years they spent in education, which in effect measures how long they spent in school or university. summary(lm(reg.dat$wage ~ reg.dat$treat + reg.dat$female)) ## ## Call: ## lm(formula = reg.dat$wage ~ reg.dat$treat + reg.dat$female) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1757.34 -496.63 23.27 471.01 1906.07 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2326.16 35.86 64.867 &lt; 2e-16 *** ## reg.dat$treat 579.37 41.30 14.030 &lt; 2e-16 *** ## reg.dat$female -107.84 41.30 -2.611 0.00916 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 652.9 on 997 degrees of freedom ## Multiple R-squared: 0.1691, Adjusted R-squared: 0.1675 ## F-statistic: 101.5 on 2 and 997 DF, p-value: &lt; 2.2e-16 summary(lm(reg.dat$wage ~ reg.dat$treat + reg.dat$female + reg.dat$education)) ## ## Call: ## lm(formula = reg.dat$wage ~ reg.dat$treat + reg.dat$female + ## reg.dat$education) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1405.03 -267.08 -0.94 259.26 1150.44 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 64.284 57.014 1.128 0.260 ## reg.dat$treat 539.729 24.549 21.985 &lt;2e-16 *** ## reg.dat$female -38.913 24.589 -1.583 0.114 ## reg.dat$education 154.577 3.614 42.770 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 387.9 on 996 degrees of freedom ## Multiple R-squared: 0.7071, Adjusted R-squared: 0.7062 ## F-statistic: 801.4 on 3 and 996 DF, p-value: &lt; 2.2e-16 We observe the effects from this omission in the two regressions that we can run. First, we are only adding the variable for gender. The variable is not significant, so there is no evidence that we have a systematic effect. The randomization of the treatment worked well. Things are different when we also add the variable on education. Indeed, the level of education does have an effect on the outcome. For some reason, when designing the experiment there was a mistake in the randomisation of our treatment and in result we do have a bias. If we only use the difference in means estimator, we would have missed this bias. Now that we include education in our regression analysis, we can control for it. In result, the effect from our treatment diminishes is quite strongly. 10.3 Treatment Heterogeneity The other area where the regression framework is really useful is in the context of treatment heterogeneity. Treatment heterogeneity describes the situation where the treatment you are assigning might have different effects on different groups. For example, in our experiment we could believe that retraining has different effects on different age cohorts. For anybody below 50, IT skills are quite normal and employers would expect that they can all handle a computer well. But what about those who did not grow up with a computer and are 50 years or older? For them, IT skills are much more rare and having this extra training under the belt might give you a real advantage. How can we express such a relationship with a regression equation? We not only include the terms for the treatment and also the age, but we also multiply the two variables and a parameter we estimate along. For our example, the regression equation would look like the following. \\[ \\text{Income} = \\beta_0 + \\beta_1 \\text{Training} + \\beta_2 \\text{Over 50} + \\beta_3 \\text{Training * Over 50} + \\beta_4 \\text{Female} + \\beta_5 \\text{Education} \\] How do we interpret this result for the resulting regression equation? The coefficient \\(\\beta_3\\) is activated if both of the data equal \\(1\\). So only if we observe somebody over \\(50\\) who receives the treatment will we actually notice the impact of the estimated parameter \\(\\beta_3\\) on our outcome. If either of the two variables is \\(0\\), there will be no effect from \\(\\beta_3\\) on the income. If the age is below \\(50\\) and this variable equals \\(0\\), the whole product term equals 0. Likewise, if there is no treatment, the whole product term is \\(0\\) as well. Overall, we will be able to make the following statement. In addition to the normal treatment effect that we already observe from \\(\\beta_2\\), is there also an additional difference in the treatment for those who are older than 50 years? # 3 Treatment Heterogeneity regression.interaction.result &lt;- lm(reg.dat$wage ~ reg.dat$treat*reg.dat$age.50plus + reg.dat$female + reg.dat$education) summary(regression.interaction.result) ## ## Call: ## lm(formula = reg.dat$wage ~ reg.dat$treat * reg.dat$age.50plus + ## reg.dat$female + reg.dat$education) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1401.82 -257.81 1.11 263.65 1080.25 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.588 57.942 0.959 0.338 ## reg.dat$treat 418.075 34.812 12.009 &lt; 2e-16 *** ## reg.dat$age.50plus -22.687 34.185 -0.664 0.507 ## reg.dat$female -36.905 24.119 -1.530 0.126 ## reg.dat$education 155.954 3.557 43.841 &lt; 2e-16 *** ## reg.dat$treat:reg.dat$age.50plus 240.940 48.353 4.983 7.38e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 380.4 on 994 degrees of freedom ## Multiple R-squared: 0.7188, Adjusted R-squared: 0.7174 ## F-statistic: 508.2 on 5 and 994 DF, p-value: &lt; 2.2e-16 Taking a look at the regression results, what can we learn? First of all, we see that apparently controlling for this interaction term was really necessary ended brought our overall treatment effect baseline treatment effect highly up. It is at 418 GBP and we are quite certain that it is not just random coincidence. Next, just the age does not make any big difference on the income in our example. Neither does actually the difference between men and women. However, education matters—for every extra year that you spent in education you will earn 156 GBP per month more. Finally, let us also take a look at the interaction turn. We learned that for everyone who is over 50, going through the training gives them an additional advantage of 241 per month. This effect is significant as well, so that we can be quite certain that in the larger population there is an effect that is different from zero. In sum, those who are below \\(50\\) and go through the IT training can expect an income increase of 418 GBP per month. In addition, those who are older than \\(50\\) can add an extra 241 GBP to this baseline treatment effect summing to overall 659. Let me explain you this interaction effect in a quick video again. By the way, here is the ‘real’ data generating process that I defined to get to our data. You can clearly see that all our estimates of the treatment effect are off if the model is not appropriate—be it with a simple calculation of the difference in means or ‘wrongly’ specified regression models. We only get regression results close to the ‘real’ coefficients in the data generating process if we correctly specify the regression equation. # Explanatory Variables education &lt;- sample(seq(9, 20), 1000, replace = TRUE) treat &lt;- c(rep(1, 500), rep(0, 500)) age.50plus &lt;- sample(c(0, 1), replace = TRUE, 1000) female &lt;- sample(c(0,1), 1000, replace = TRUE) # Outcome wage &lt;- education * 160 + treat * 400 + age.50plus*treat*250 + rnorm(1000, 0, 400) 10.4 Outro This Is It! What an amazing journey we had! Please allow me a couple of final words. 10.5 Reading for This Week For this last week, please read p.161–181 in Imai (2018). "],["useful-internet-resources.html", "Chapter 11 Useful Internet Resources", " Chapter 11 Useful Internet Resources Quick-R has been a go-to reference when you look up simple examples for quite some time. It is a really helpful source. If you want to dig a little deeper on the things we are doing, you can check out this beautiful online book titled Causal Inference: The Mix Tape. Also available as a print version of course… The project Seeing Theory is a visually really beautiful and interactive take on what we are doing in this class. Note: The authors are undergrads from Brown University. Interested in Data Visualization? This is a really great book on the topic from Jonathan Schwabish. If you are interested in critically reflecting on what you are doing when you are analysing data, go and read Critical Thinking from Tom Chatfield. He also has a great video where he explains the core ideas. "],["bibliography.html", "Chapter 12 Bibliography", " Chapter 12 Bibliography Agresti, Alan. 2018. Statistical Methods for the Social Sciences. 5th ed. Pearson. Fogarty, Brian. 2019. Quantitative Social Science Data With R. London: Sage. Imai, Kosuke. 2018. Quantitative Social Science–an Introduction. Princeton University Press. "]]
