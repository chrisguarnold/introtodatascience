[["index.html", "PL 9248 Data Science for Politics and IR Welcome! Data to Answer Your Questions Your Weekly Data Workout The Module Webpage Getting in Touch", " PL 9248 Data Science for Politics and IR Christian Arnold, Cardiff University 2023-02-03 Welcome! Welcome to this module. In PL 9248 you will learn how to make the most of data. How can you handle and analyse data efficiently? How to communicate and visualise results? While everything we do here has of course a wider appeal, we will focus in particular on our area: How can we use data to study politics and international relations? I am really looking forward to teaching the module again in Spring of 2023. Data to Answer Your Questions Today, data are everywhere. Data are not only collected for scientific studies. Governments, companies and non-profit organisations are amassing and increasingly sharing data at an unprecedented level. Data on their own can actually be a bit boring. Few people might enjoy to just browse numbers. Everything changes, however, if we can use data to answer a question we are interested in. This is why we we will build the module around three scenarios. Scenario 1: The Data Report Whatever you actually want to do with data, the first step is always getting a good oversight over the data itself. You will have to open the data and describe its most characteristic features. Often, you want to communicate these results to other colleagues in a data report. And all of your work with the data is ideally documented in a transparent way so that others—or even your future self—can easily understand what you were doing. This first scenario might actually be quite typical for your first years at work. Scenario 2: Knowing It All By Knowing A Few For the second scenario, imagine you are working for an NGO that helps implement voting observation missions in fragile democracies. Even though you are partnering up with local civil society organisations, you cannot send election observers too all polling stations during a mission. Luckily, this is not strictly necessary: As long as you properly randomise the polling stations you want to observe, you can infer the real, countrywide numbers on the basis of just a few hundred few hundred observations. In this class, we will learn why this is working and how you can do inference yourself. Scenario 3: Did the New Policy Have an Effect? Finally, take another example: A new law introduced mandatory environmental protection for farm land. A year later, biologists measure that harmful substances in ground water are down by 20%. Was this the effect of the policy? Or was it maybe just a particularly rainy year that washed all pollutants away? Together we will learn how we can find answers by asking our data the right kind of questions. The module is particularly relevant if you are planning to undertake a dissertation, since it will help you find the right data and how to work with it. Data literacy is not only a really useful skill to have as a modern citizen, it is also really valuable when you are looking for a job. Your Weekly Data Workout In this module, we will expose you to core concepts prior to the actual class. This frees up large chunks of time during the class that we can spend on activities where you typically need the most help, such as application of basic material and engaging in deeper discussions and creative work with it. Two years ago, all this would have been quite revolutionary. But now, right after the pandemic, all of this feels actually quite intuitive. Let me introduce all of our activities one by one. 1 Work Through This Homepage Your first port of call in every week will be the respective chapter in this module homepage. This site is a mixture between a website and a teaching book. It will walk you through the content of each week and you can find the most important content explained here. The website allows you to take your own pace and revisit the parts that you find most challenging as many times as you want and at the pace that you feel comfortable with. The respective content of the week will be uploaded on Monday morning, latest. I expect that you work through that home page on your own over the course of the week. In case you have questions, please reach out to me via email or book time in the office hour. We will also use the module cafe every second week for answering any of your questions. 2 Apply Your Knowledge in Our Workshops While the homepage is focusing on knowledge acquisition, our workshops on Tuesdays between 3.10pm-5pm will focus on skills development. Learning how to do data analysis requires practice. And practice means failing a lot and learning from the own mistakes. The workshops on Tuesdays are meant to serve as a place where we can jointly explore how to properly manage, analyse and visualise data. The workshops will always cover the content of the past week, e.g. in the workshop in week 2 we will be covering the content of the homepage from week 1. Please make sure that you come prepared. Workshops will run from week 1 until week 11. Over the course of these two workshop hours, you will be working online in small groups. For each week, I will prepare lab sheets that you will jointly solve. Do not worry in case you struggle: I will be always around to help. Without participating in these workshops, the assignments will be really challenging. I strongly recommend you do not miss a single session. 3 Bring Your Questions to the Seminars We will have bi-weekly module cafes where you can discuss any additional questions that you might have. In addition, these module cafes will also serve to answer any question regarding the assignments and also feed back core mistakes from the labsheets. 4 Read Guideance on Planning and Structuring To guide you through all the hoops, I will also stay in touch with you with am email every Friday to make sure that you are on board. It is vital that you check your university email regularly, since we will only communicate with you through this address. The Module Webpage Let us take a closer look at how this website is working. This space is going to be the main learning tool for this term’s Data Science for Politics and IR. Obviously, I will guide you through the material on this web-page via text. Think of it as a replacement for the lecture slides that you might find in other contexts: A place to find the core concepts and central ideas of this class. But not all is text. Sometimes, nothing beats someone actually explaining you core ideas. For that purpose, I will record small videos and embedd them here. The best thing about videos: You can watch them as many times as you want… Finally, this format is also really handy, because we will be able to take a look at R code. R is a computer language invented for data analysis, and you will learn how to use it in PL 9248. This is how it looks like when we will be covering R code. # All text in a line after the hashtag is a comment. # Here we assign some variables a &lt;- 1 b &lt;- 2 # This is how to add them and assign the result to another variable c &lt;- a+b # And finally we print the result cat(c) ## 3 Last but not least, each week will have some readings that will reflect the core content of the respective week from yet another perspective. I will add pointers to the readings whenever appropriate. You can find all information about the readings in the [bibliography]. Getting in Touch If you have questions or need any kind of help, feel free to get in touch. The whole purpose of our weekly workshops and our bi-weekly module cafe is to provide you with ample space for your questions. Make use of this! You can always drop me an email. I will respond at the end of the working day. You can also book time in my office hours Wednesday 3.15-4.15pm; Friday 5.15-6.15pm. "],["tools-for-working-with-data.html", "Chapter 1 Tools for Working with Data 1.1 What Is In a Data Spreadsheet? 1.2 Key Concepts 1.3 Meet: R 1.4 Readings for This Week", " Chapter 1 Tools for Working with Data This week we will get to know the basic tools we need. 1.1 What Is In a Data Spreadsheet? When we work with data, the first step is always to get an overview of the data—be it in academia, for a company or in government. The whole first part of this module is dedicated to teaching you how to understand your data in a quick and efficient way. We will learn a couple of important things. How to access data. How to get a quick overview over the data. How to describe data and extract their core characteristics. How to communicate and visualise results. How to manage everything efficiently. To give you an idea how and why even these basic skills matter in real life, I talked to Dr Sebastian Sternberg. He has a degree in politics and works as a data scientist with KPMG at the moment. In the interview, he was so kind to offer us insights into how he is working with data and how relevant data reports are in his daily work. 1.2 Key Concepts Let as begin with some key definitions. 1.2.1 What is a Statistic? Statistics summarise large amounts of numerical data. Statistics are really useful if one wants to get a good overview over data. A statistic is a characteristic of a sample. Imagine we make a number of observations and put in numbers what we see. When we calculate a statistic on this data, the statistic is able to describe the sample we collected. If we collect a different sample, the statistic is very likely to have slightly different values. The goal of statistical methods is to make inference about a population based on information from a sample of that population. Often, we might be interested in more than just the data at hand. By drawing a sample, we hope to generalise beyond our sample and learn something about the the overall population. For example, by asking a few hundred Welsh voters about their voting intentions we aim at saying something about the voting intention of all voters in Wales. To estimate parameters, we use statistical methods. Estimating a parameter means that you need to be able to say something about a certain parameter on the basis of a couple of data points that you collected. We will be learning how to use the correct statistics to infer what we believe is the most likely value for a certain parameter. In the voting example, we might be interested to understand what share of voters who would cast their vote for Plaid Cymru for example. Statistics can separate the probable from the possible. When we collect data, the data can have a range of different values. Of course, not all possible data points are equally likely. The beauty of statistics is that they can be used to tell us which values we can expect to see more often and which ones least often. For example, voters might be able to cast their vote for a whole range of parties—e.g. Labour, Conservatives, Plaid Cymru or the Welsh Nation Party. However, voting for these parties is not necessarily equally likely. It is reasonable to expect that, say, the Conservatives will receive more votes than the Welsh Nation Party. 1.2.2 Some More Definitions Some more definitions that a really useful for our module. It would not hurt if you could learn them by heart. Population: The full set of cases about which we want to generalise. Sample: A subset of the population. Variable: Logical set of attributes (characteristics) of an object (person, thing, etc.) that can vary across a range. Parameter: A characteristic of a population; usually unknown. Descriptive Statistics: Statistics that summarise the distribution of the values of variables in a sample. Inferential Statistics: The use of statistics to make inferences about a larger population based on data collected from a sample. 1.2.3 Types of Data Levels Example from our Survey Real World Example Measurement Possible Operations Nominal Tea or coffee? Gender, religion Categories Frequencies Ordinal Do you like summer? Social class, attitudes Categories and ranking Frequencies and ranking Interval How many hours do you study? Age, income All above and distance All above and addition and subtraction 1.2.4 Data in Action Remember that you were filling out the survey just before term? Overall, 17 of you responded (thanks!). Let us go and take a look at some examples of your data to better understand the different types of data that are out there. Nominal Data What do you prefer to drink in the morning? The variable encodes three different answer categories. These categories cannot be ranked, obviously. These are your responses: Ordinal Data How much do you like summer? You were given a scale that ranged from 1 to 5. The answers to this question can be ranked. However, it is not necessarily true that the distances between the answer categories is always the same. Now look at your responses: Here comes the sun! Interval Data Finally, let us consider some interval data. How many hours do you study per week? This data provides full hours. The data is discrete, its values can be ranked, and the distance between any two neighbouring categories is always the same. 1.3 Meet: R Now that we have a bit of an overview how data can look like, let us take a look at the main tool that we will use to manage, analyse and visualise data. We will use a programming language for this module that is called R. It is free and open source, so you can install it easily on your computers. It is also very powerful which means that all the effort that you are investing in these 10 weeks of the term to learn it will heavily pay-off when you are analysing data in the future—be it for research or in any other professional context. And ‘paying-off’ is meant quite literally here. R is a really valuable skill set to have on your CV and is certain to boost your employability quite a bit. If you want to know more about R, where its coming from and how it all developed, check the Wikipedia Page as a start. Also, feel free to go wild on the homepage of the R Project itself or in any other of your favourite corners of the internet. Beware: The rabbit hole is quite deep. #nerdalert 1.3.1 Install R Installing R is pretty straightforward: Go and visit the homepage of the The Comprehensive R Archive Network where you can find the latest version of R. At the top of the page, you can chose between your operating system: 1.3.2 Install R Studio Now that you have installed the programming language, let’s go and get a nice interface that actually helps us get our work down. R-Studio is a programme that makes it much more easy to write and execute R code. Go and get the free R-Studio desktop version. Install the version that suits your operating system. Nota bene: Make sure you first install R and then R-Studio. 1.3.3 How to Work with R Here is a quick video in which I show you around. You can also take a look at Chapter 2 in Fogarty (2019) for a lot of helpful details. 1.3.4 First Steps in R Now that you know how R-Studio looks like and how to use it, let’s go and try it out. As you saw, in the console tab you can run commands directly. But it is better practice to type them in an R script and send them. Every line of the text editor can be sent using Str + Enter in Windows and Cmd + Enter in MacOS. Objects R can keep several objects in memory at the same time. To distinguish them, object have names. Objects are assigned with an arrow like this: &lt;- Let us assign some values to objects. a &lt;- 5 b &lt;- 6 c &lt;- a * b Objects can be called using their name. Here on this homepage, you will see a second grey block that will give you the output. If you type all up in R Studio, you will find all R related output in the console. a ## [1] 5 b ## [1] 6 c ## [1] 30 We can combine several values with the function c(). Functions are really useful. There are basically three elements to a function. * First of call, they have a name—here in this case it is the letter c which stands for concatenate. * Then, functions always need one more more inputs. A function receives its inputs in the brackets (). * If you call a function and provide it with its correct inputs, it will do its thing and return the output that you asked for. a &lt;- c(1,2,3) a The object a is not a scalar any more. It is now a vector that has three elements to it, the numbers 1, 2 and 3. You can of course also assign more than just numbers. We can assign strings—here a couple of three digit country codes. cntry &lt;- c(&quot;BRA&quot;, &quot;GER&quot;, &quot;FRA&quot;, &quot;NLD&quot;) cntry ## [1] &quot;BRA&quot; &quot;GER&quot; &quot;FRA&quot; &quot;NLD&quot; If you want to see all the objects that we have in the workspace so far, try the function ls(). ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;cntry&quot; &quot;dat21&quot; &quot;dat22&quot; You want to remove an object? Then use the function rm() like so. rm(cntry) See, the object cntry is not in the workspace any more. ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;dat21&quot; &quot;dat22&quot; R Data Types In R, there can be different types of objects. Some can only take specific types of data. Scalar: numbers, characters, logical values Vector: sets of scalars Matrix: two-dimensional set of scalars of same type Data frame: Collections of vectors of (possibly) different types but with same length Let us begin from scratch with am empty workspace. To delete everything, we will nest two of the functions above. rm(list=ls()) First we assign some scalars. # scalar a &lt;- 43 b &lt;- a + 7 a ## [1] 43 b ## [1] 50 And here we go with some vectors. x &lt;- c(a,b,a,b) y &lt;- x + 10 cntry &lt;- c(&#39;Brasil&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Singapore&#39;) x ## [1] 43 50 43 50 y ## [1] 53 60 53 60 cntry ## [1] &quot;Brasil&quot; &quot;Canada&quot; &quot;China&quot; &quot;Singapore&quot; Something interesting just happened here: You see the object x which is a vector? R added 10 to each of the scalars in x when calculating y. In programming you call this broadcasting. Pretty nifty! Next, we build a matrix, for example by binding two columns with the function cbind() or two rows with the function rbind(). z &lt;-cbind(x,y) z2 &lt;-rbind(x,y) z ## x y ## [1,] 43 53 ## [2,] 50 60 ## [3,] 43 53 ## [4,] 50 60 z2 ## [,1] [,2] [,3] [,4] ## x 43 50 43 50 ## y 53 60 53 60 This is how a data frame looks like. It accepts vectors with any value. Data frames are quite similar to spreadsheets, for example in Excel or LibreOffice. dat1 &lt;- data.frame(cntry, z) dat1 ## cntry x y ## 1 Brasil 43 53 ## 2 Canada 50 60 ## 3 China 43 53 ## 4 Singapore 50 60 1.3.4.1 Selecting Elements Finally, we will learn how to select elements from objects. We will begin with vectors. a &lt;- c(1,2,3,4,5) b &lt;- a + 10 We can select elements in R with [ ]. a ## [1] 1 2 3 4 5 a[2] ## [1] 2 a[2:4] ## [1] 2 3 4 Like in many other programming languages, the colon : expresses a range. Here we select all values in the range from the second to the fourth entry. We can also select elements from a matrix. The , helps to distinguish between the two dimensions. The selection m[1,1] will return the first element of the first column. In a similar vein, the selection m[3,2] will return the third element of the second column. m &lt;- cbind(a,b) m[1,1] ## a ## 1 m[3,2] ## b ## 13 Finally, we select elements from a data frame. We create a variable ‘name’ with the names of Mark, Luise and Peter. The variable ‘bike’ contains their bikes: Mountainbike, Single Speed and Racing Bike. We capture the hours per week they ride on it: 4, 7, 8 and finally we bring all variables together in a common data frame dat2. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat2 &lt;- data.frame(name, bike, hours) In data frames, you can select elements with the operator $ and the name of the variable dat2$bike ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; You can of course still use the positions, too! dat2[,2] # gives you all entries for the second column ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; dat2[1,2] # gives you the first entry for the second column ## [1] &quot;Mountainbike&quot; Since just selecting might not always be enough, R allows us to select elements based on conditions. R comes with everything formal logic requires. is equal == is not != smaller &lt; larger &gt; smaller equal &lt;= larger equal =&gt; You can build more complex queries via AND &amp; OR | Let’s go: x &lt;- c(1,56,23,89,-3,5) y &lt;- c(24,78,32,27,8,1) x[x &gt;20] # greater as 20 ## [1] 56 23 89 x[x &gt;20 &amp; x !=89] # greater as 20 and unequal 89 ## [1] 56 23 x[x&gt;0 | x==-3] # x where x greater 0 oder x=-3 ## [1] 1 56 23 89 -3 5 y[x==1] # y where x=1 ## [1] 24 1.4 Readings for This Week Please read chapter 2 and chapter 3 in (Fogarty 2019). This is where we end for this week. I am looking forward to meeting you in the workshops on Tuesday 3.10pm-5pm in Tower/0.03! References "],["describing-data.html", "Chapter 2 Describing Data 2.1 Statistics to Summarise Data 2.2 Data Management with R 2.3 Describing Data Using R 2.4 Readings for This Week", " Chapter 2 Describing Data 2.1 Statistics to Summarise Data When we describe data, we typically have three questions: What do typical values look like? How clustered or dispersed are these values? In short: How are the values of a variable in a sample distributed? 2.1.1 Tables Data that comes in categories—be they nominal or ordinal—can be easily summarised. Simply list possible values for a variable, together with the number of observations for each value. Can be used to summarise distributions of one nominal, ordinal or interval variable. Intervals must be constructed for interval-level variables (e.g. age, income). Absolute frequencies record the actual number of observations for each value. Relative frequencies record the proportional distribution of observations. This is an example from our data: Frequency Percent Coffee 10 0.59 Other 2 0.12 Tea 5 0.29 This is how the data looks like for the question on how much you like summer. Frequency Percent Highest 5 0.29 4 1 0.06 3 1 0.06 2.1.2 Central Tendencies One of the first questions that comes to our minds when we consider a variable is: What is a typical value for it? Intuitively it makes sense to chose a value that shows the central tendency of a variable. There are three different statistics that can help summarise the distribution of scores by reporting the most typical or representative value of it. Mode The mode is the value that occurs most frequently. You can use it for all kind of categorical data. There can be more than one mode and under this circumstance you would speak of multimodal data. In case the answer categories can be ranked, the mode does not need to be near the centre of the distribution. Finally, the mode is resistant to outliers. For political scientists, the mode is a really important measure: We use it whenever we want to determine the winner of an election. When I asked you for your vote (if you had had the choice), this is how you responded. Frequency Percent 1 0.06 Biden 14 0.82 Trump 2 0.12 The modal candidate is clearly Biden for your class. Two students would have voted for Trump. Median The median is the value that falls in the middle of an ordered sample. Obviously, the measure cannot be used for nominal variables—they can not be ranked. The median is the 50th percentile point. This means that when you count all cases, half of the sample will be smaller than the median and the other half is larger than the median. When the sample size is even, the median is the midpoint between the two measurements in the centre. By definition, the median is resistant to outliers: Irrespective of how small the smallest value or how large the largest one, the one value that splits the sample in half will remain always the same. These are your responses to the question how many hours you think you should study. I chose the dark blue line to indicate the median value of the data. Overall, we have 17 responses in the data. If the number of respondents is odd, the median is the average between two central responses that come into questions. Mean The mean is the sum of the observations divided by the number of observations \\[\\begin{equation*} \\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_N}{N} = \\frac{\\sum_{i=1}^{N}x_i}{N} \\end{equation*}\\] with \\(\\bar{x}\\) being the mean of variable \\(x\\); \\(\\sum\\) being the sum; \\(i\\) the individual cases (of x); \\(N\\) the number of cases. \\end{itemize} This all looks a bit fancy, but it is actually just a matter of understanding the notation. Conceptually, the mean is really straightforward—it is nothing different than the good old average. The mean has a couple of interesting characteristics. It is only applicable to interval variables. The mean is a good measure of central tendency for roughly symmetric distributions, but it can be misleading in skewed distributions. Most importantly, the mean is really susceptible to outliers. There is a nice physical interpretation of the mean: it is the centre of gravity of the observations. Take a look at your data for how many hours you actually study per week. Your mean is slightly higher than your median. Why is that so? Basically, those who study more than 20h per week have a considerable influence on the mean and push the mean to the right of the median. Compare your data with the data from the cohort in 2021: Here the median and the mean are closer together. Why is this so? 2.1.3 Spread You now know how to chose a typical value that summarises your data. Next on the list is to characterise their spread. Are all values really close to one another? Are they far apart? Do many of them hang out on one side of the distribution, and are they far apart on the other side, i.e. is their distribution skewed? To measure all this, we will now take a look at different measures of spread. In essence, they are statistics that summarise the variation around our average value. We will consider four different measures that all build on each other. Range: Difference between two values, typically the minimum and the maximum. Deviation: Difference of a value from the mean. Variance: Squared difference of a value from the mean. Standard Deviation: Square root of the squared difference of a value from the mean. Range The range is the the distance between the largest and the smallest values, i.e. maximum–minimum. It will be distorted by extreme values. The interquartile range is another really important range. It covers the middle 50% of observations, so the range from the 25th percentile to the 75th percentile (lower quartile–upper quartile). Here you can a number of concepts that we covered working together to describe the data: The 25% quantile, the 50% quantile, the 75% quantile, the median, the mean and the interquartile range. Let us describe some of your data, here how old you are. How large would be the interquartile range? Hint: the value is 2. Deviation The deviation of any observation is its difference from the mean. \\[\\begin{equation*} (x-\\bar{x}) \\end{equation*}\\] What is the sum of deviations? Do the maths with a couple of numbers on a piece of paper. \\[\\begin{equation*} \\sum(x-\\bar{x}) = ? \\end{equation*}\\] Just in case some of you might still wonder about the \\(\\sum\\) at this point: I put together a small video as a refresher. And it also shows the answer to the issue with deviations. Apparently, the deviations always sum up to 0. The values keep canceling each other out. So what can we do? One solution would be to calculate the Mean Absolute Deviation \\(\\text{MAD}\\). \\[\\begin{equation*} \\text{MAD}=\\frac{\\sum(|x_i-\\bar{x}|)}{n} \\end{equation*}\\] In case you do not know the sign “\\(|\\)”: anything that is in between two “\\(|\\)” will always return its positive value. So \\(|5| = 5\\) and also \\(|-5| = 5\\). Let us take a look at your data, here how much you actually study and how much you think you should study. I am including again the interquartile range in red (which goes from where to where again?), and the mean in gold. When you do the maths you will find out that the \\(\\text{MAD}_{\\text{actual}} = 5\\) and \\(\\text{MAD}_{\\text{ideal}} = 6.44\\). Variance In practice, however, you will find that the Mean Absolute Deviation, is rarely used. Instead, you can often find the variance. It is basically the same as the \\(\\text{MAD}\\), but different. To avoid the canceling out, we will square the distance of each value to the mean. And for arcane statistical reasons that are irrelevant for this class, we now subtract 1 from the overall cases \\(n\\) in the denominator. \\[\\begin{equation*} s^2=\\frac{\\sum(x_i-\\bar{x})^2}{n-1} \\end{equation*}\\] The variance of the actual number of hours you study is \\(s^2_{\\text{actual}} = 64.67\\) and the variance for the number of hours you consider ideal is \\(s^2_{\\text{ideal}} = 93.93\\). Contrast the difference between the two variances to the difference in the two \\(\\text{MAD}\\)—it is much larger! The reason is simply that we are now taking the sum of the square of the distances and not just the absolute distances, which of course weighs much more for larger numbers. With the \\(\\text{MAD}\\) each data point contributes an equal share to the overall measure of spread. For the variance, this is no longer true. Those data that are further apart from the mean will drive the variance to a much larger degree than those data that are close to the mean. Standard Deviation While the variance is already a big step forward in measuring spread, it has one important drawback: It is quite abstract and really hard to interpret. Ideally, we would want to understand the measure for spread on the same metric as the data themselves. Doing so is straightforward. We simply take the square root of the variance—and the resulting standard deviation is in the metric of our data \\(x\\). \\[\\begin{equation*} s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}} \\end{equation*}\\] In our running example—the number of h you study and the number of h you think you should study—this is what we get. The standard deviation for the former is \\(s_{\\text{actual}} = 8.04\\) and the standard deviation for the latter is \\(s_{\\text{ideal}} = 9.69\\). Compare these values to the Mean Absolute Distances: \\(\\text{MAD}_{\\text{actual}} = 5\\) and \\(\\text{MAD}_{\\text{ideal}} = 6.44\\). They, too, are on the original scale. The standard deviation, however, is much more sensitive to outliers, which is a really desirable characteristic. 2.1.4 Ratios and Rates Finally, ratios and rates. Proportions We start with something really simple, the proportion. It is calculated as \\[\\begin{equation*} p = \\frac{f}{N} \\end{equation*}\\] with \\(p\\) being the proportion; \\(f\\) being the number of cases (frequency) in one category; \\(N\\) being the number of cases in all categories of the variable. The proportion is useful if we want to answer a question like: What is the proportion of students having tea for breakfast? \\[\\begin{equation*} p = \\frac{5}{17} = 0.29 \\end{equation*}\\] Percentages You like percentages better? Simply multiply your proportions with \\(100\\). \\[\\begin{equation*} P = \\left( \\frac{f}{N}\\right) 100 = \\left( \\frac{5}{17} \\right) 100 = 29.41\\% \\end{equation*}\\] Rates A rate is really useful if you want to express for example how often a proportion occurs in a given amount of time. We can calculate the rate as \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} \\end{equation*}\\] with \\(r\\) being the rate: the frequency per time in a certain set \\(f\\) being the number of cases (frequency) in one category \\(t\\) being the time under consideration \\(N\\) being the number of cases in all categories of the variable \\(u\\) being the unit under consideration Let us advance step by step and wrap our head around this with the help of an example. We want to understand how many of you actually bought a computer during 2020. This simplifies our formula a bit. \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\text{all students}} = \\frac{\\frac{5}{1}}{\\frac{17}{1}} = 0.29 \\end{equation*}\\] We can take this a little further and ask How many computers did 10 student buy in 2021?. To answer this question, we simply adapt the number of people in the unit and set \\(u=10\\). \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\frac{\\text{all students}}{\\text{unit of 10 students}}} = \\frac{\\frac{5}{1}}{\\frac{17}{10}} = 2.94 \\end{equation*}\\] So 10 students bought 2.94 computers in 2021. Growth The last thing we want to look at is growth. In particular in economics, growth has a really prominent role, and a lot of theory is built around all kinds of growth related to different cash flow: GDP, GDP per capita, return on investments to just name a few. But of course, growth can also happen in other areas like literacy rates in low-income countries, unemployment or votes. Growth can be expressed as a percentage change. \\[\\begin{equation*} G = \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\end{equation*}\\] with \\(G\\) being the growth rate in a variable from time 1 to time 2 \\(f_1\\) being the number of cases (frequency) at time \\(t_1\\) \\(f_2\\) being the number of cases (frequency) at time \\(t_2\\) Again, let us take a look at our own data to understand what is going on here. \\[\\begin{align*} G &amp;= \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\\\ &amp;= \\left(\\frac{\\text{Purchases in 2022} - \\text{Purchases in 2021}}{\\text{Purchases in 2021}}\\right) 100 \\\\ &amp;= \\left(\\frac{-2}{5}\\right) 100 = -40\\% \\end{align*}\\] Apparently, you were purchasing less computers during 2022, more specifically, the sales had a negative growth of \\(G = -40\\%\\). It could be that a number of you bought a new computer when you started your university career back in 2021. 2.2 Data Management with R To calculate all above, we first need to take a closer look at some data management this week. 2.2.1 Libraries Libraries are functions that do not ship on board your original R programme. Instead, you have to get them from them internet. Think of it like wanting to read a book. You first have to get it from a shop and bring it home, where you will add it to your book shelf in your own, personal library at home. In R, you can use the command install.packages() to download a package to your computer. If you execute the command, R might prompt you for a location—simply pick one that is close to you. Obviously, \"name_of_library\" is a placeholder here, so don’t try this at home with that particular code snippet, but replace it with the package you actually need. install.packages(&quot;name_of_library&quot;) You now have downloaded the programme to your computer. Or, in other words, you have added the book into your bookshelf. However, you are not sitting in your lounge chair with the book in your hand, yet. For that, you would still have to go to the library in your house and get the book. This is exactly what we will be doing now with the R package. We will collect the package from our library and load it into the active work space. library(&quot;name_of_library&quot;) 2.2.2 Setting the Working Directory R is quite stupid. It does not know where to look for files—be they R code or any other data—unless you really tell it where to look for a file. Typically, we will instruct R to make its home in the exact place where you save your main R script with a function called setwd(). As its argument, you provide the path you are working in. For me on my office machine, this is how it looks like. setwd(&#39;/Users/foo/PL9239 Intro_data_science/intro_data_science_homepage&#39;) Now, R will start looking for everything starting in that particular working directory. To see which working directory you are in, you can type getwd() This step might seem a bit minor and technical, but it is the nr.1 rookie mistake to forget setting your working directory properly. In case the idea of folders, files and working directories are still a bit confusing, here goes another video. 2.2.3 Reading Data Working directories are particularly relevant if you want to read in data sets. Data mostly comes in two formats: comma separated values, or short .csv, and as a Microsoft Excel spreadsheet .xls. Most open data formats can be read in with a function that begins with read.foo. Of course, just reading it is not enough—you have to assign it to an object if you want to work with it, so we type for example: csvdata &lt;- read.csv(&quot;dataset1.csv&quot;) If we want to read in .xls data, we have to load a library that can help us with that. We will go with the readxl package. Again, we are assigning the data to an object so that we can call it later. library(readxl) xlsdata &lt;- read_excel(&quot;dataset1.xls&quot;, sheet=&#39;sheet_foo&#39;) 2.2.4 Saving Data You can also save data. Let us create a toy data set again. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat &lt;- data.frame(name, bike, hours) Now save it. save(dat, file = &quot;toydata.RData&quot;) To check the magic of this we remove the data set and then try to call the object. rm(dat) dat ## Error in eval(expr, envir, enclos): object &#39;dat&#39; not found Nothing there. load(&quot;toydata.RData&quot;) dat ## name bike hours ## 1 Mark Mountainbike 4 ## 2 Luise Single_Speed 7 ## 3 Peter Racing_Bike 8 Tada! Worth noting at this stage, that when you use the native R way of saving data, R saves your actual object, here the object dat. You can of course also save data as an Excel spreadsheet. library(writexl) write_xlsx(dat, &quot;toydata.xlsx&quot;) 2.3 Describing Data Using R Now, with a bit more of a background, we can calculate all of this week’s statistics. In R, this is really straightforward. I am showing you how I did that using your data. Of course, you cannot run the script yourselves, since I will not share the data. 2.3.1 Working with Data Frames First, I load the data. Then I check which type it is: Apparently a data frame. dat &lt;- read.csv(&quot;../other_files/preparing_data/data/class_survey/lecture_survey_23.csv&quot;) class(dat) ## [1] &quot;data.frame&quot; Data frames are particularly useful, because we can call the individual variables simply by adding the $ symbol and then calling the name of the variable we are interested in. For example, if we want calculate how much you spend per week partying, we could simply multiply how much you spend on one night out and multiply it with how often you go out per week. dat$spend * dat$partydays ## [1] 80 60 20 24 120 30 50 10 15 90 24 30 20 40 40 30 45 2.3.2 Central Tendencies You can take a look at the frequency of categorical data with the function table(). This is your data on how many days you are partying per week. table(dat$partydays) ## ## 1 2 3 ## 8 5 4 Indeed some party animals here. Now, what is the mode? We have to call a package for that function. library(DescTools) Mode(dat$partydays) ## [1] 1 ## attr(,&quot;freq&quot;) ## [1] 8 The output is a bit cryptic at first, but it tells us that the value 8 is the most frequent one. For the median and the mean, let us take a look at a continuous variable. For example, how much money you spend when you go out. You call the median with median() and the mean with mean(). median(dat$spend) ## [1] 20 mean(dat$spend) ## [1] 25.29412 Stop for a second and think about the results: What does the relationship between the median and the mean tell us here? Is the distribution really symmetric? Is the distribution maybe skewed? If so, how? 2.3.3 Spread The rest is a piece of cake just the like. You can calculate the variance using var() and the standard deviation with sd(). var(dat$spend) ## [1] 167.5956 sd(dat$spend) ## [1] 12.94587 2.4 Readings for This Week Please read chapter 7 in Fogarty (2019). Chapters 4 and 5 are a good idea, but you do not necessarily have to. References "],["useful-internet-resources.html", "Chapter 3 Useful Internet Resources 3.1 Homepages 3.2 Online Books", " Chapter 3 Useful Internet Resources 3.1 Homepages Quick-R has been a go-to reference when you look up simple examples for quite some time. It is a really helpful source. This is a nice overview over all kinds of PolSci and IR data sets that are out there. Data Science is fun! Why not starting your own project on a topic that catches your interest? You could share your insights as a homepage. Lillian Petersen could be a great inspiration. The project Seeing Theory is a visually really beautiful and interactive take on what we are doing in this class. Note: The authors are undergrads from Brown University. 3.2 Online Books Here is another good book that covers all we do in depth: Andrew Gelman, Jennifer Hill and Aki Vehtari (2020): “Regression and Other Stories”. You can download it here. (all legal…) The LOST homepage is a bit of a repository for all kinds of models. It not only has code in R, but also a couple of other relevant statistical softwares, such as Julia or Python. If you want to dig a little deeper on the causal aspects, you can check out this beautiful online book titled Causal Inference: The Mix Tape. Also available as a print version of course… If you want to go even further down the causal path, this is another module at Mannheim University that also comes with a teaching homepage. Interested in Data Visualization? This is a really great book on the topic from Jonathan Schwabish. If you are keen to critically reflecting on what you are doing when you are analysing data, go and read Critical Thinking from Tom Chatfield. He also has a great video where he explains the core ideas. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
