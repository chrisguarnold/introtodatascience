[["index.html", "PL 9239 Introduction to Data Science for Politics and IR Welcome! Data to Answer Your Questions Your Weekly Data Workout The Module Webpage Getting in Touch", " PL 9239 Introduction to Data Science for Politics and IR Christian Arnold, Cardiff University 2021-03-01 Welcome! Welcome to this module. In PL 9239 you will learn how to make the most of data. How can you handle and analyse data efficiently? How to communicate and visualise results? While everything we do here has of course a wider appeal, we will focus in particular on our area: How can we use data to study politics and international relations? Data to Answer Your Questions Today, data are everywhere. Data are not only collected for scientific studies. Governments, companies and non-profit organisations are amassing and increasingly sharing data at an unprecedented level. Data on their own can actually be a bit boring. Few people might enjoy to just browse numbers. Everything changes, however, if we can use data to answer a question we are interested in. This is why we we will build the module around three scenarios. Scenario 1: The Data Report Whatever you actually want to do with data, the first step is always getting a good oversight over the data itself. You will have to open the data and describe its most characteristic features. Often, you want to communicate these results to other colleagues in a data report. And all of your work with the data is ideally documented in a transparent way so that others—or even your future self—can easily understand what you were doing. This first scenario might actually be quite typical for your first years at work. Scenario 2: Knowing It All By Knowing A Few For the second scenario, imagine you are working for an NGO that helps implement voting observation missions in fragile democracies. Even though you are partnering up with local civil society organisations, you cannot send election observers too all polling stations during a mission. Luckily, this is not strictly necessary: As long as you properly randomise the polling stations you want to observe, you can infer the real, countrywide numbers on the basis of just a few hundred few hundred observations. In this class, we will learn why this is working and how you can do inference yourself. Scenario 3: Did the New Policy Have an Effect? Finally, take another example: A new law introduced mandatory environmental protection for farm land. A year later, biologists measure that harmful substances in ground water are down by 20%. Was this the effect of the policy? Or was it maybe just a particularly rainy year that washed all pollutants away? Together we will learn how we can find answers by asking our data the right kind of questions. The module is particularly relevant if you are planning to undertake a dissertation, since it will help you find the right data and how to work with it. Data literacy is not only a really useful skill to have as a modern citizen, it is also really valuable when you are looking for a job. Your Weekly Data Workout In this module, we will expose you to core concepts prior to the actual class. This frees up large chunks of time during the class that we can spend on activities where you typically need the most help, such as application of basic material and engaging in deeper discussions and creative work with it. A year ago, all this would have been quite revolutionary. But now, 12 months into the pandemic, all of this feels actually quite intuitive. Let me introduce all of our activities one by one. 1 Work Through This Homepage Your first port of call in every week will be the respective chapter in this module homepage. This site is a mixture between a website and a teaching book. It will walk you through the content of each week and you can find the most important content explained here. The website allows you to take your own pace and revisit the parts that you find most challenging as many times as you want and at the pace that you feel comfortable with. The respective content of the week will be uploaded on Monday morning, latest. I expect that you work through that home page on your own over the course of the week. In case you have questions, please reach out to me via email or book time in the office hour. We will also use the module cafe every second week for answering any of your questions. 2 Apply Your Knowledge in Our Workshops While the homepage is focusing on knowledge acquisition, our workshops on Tuesdays between 12:00 and 2:00 o’clock will focus on skills development. Learning how to do data analysis requires practice. And practice means failing a lot and learning from the own mistakes. The workshops on Tuesdays are meant to serve as a place where we can jointly explore how to properly manage, analyse and visualise data. The workshops will always cover the content of the past week, e.g. in the workshop in week 2 we will be covering the content of the homepage from week 1. Please make sure that you come prepared. Workshops will run from week 1 until week 11. Over the course of these two hours, you will be working online in small groups. For each week, I will prepare a lab sheet that you will jointly solve. Do not worry in case you struggle: I will be always around to help. Without participating in these workshops, the assignments will be really challenging. I strongly recommend you do not miss a single session. 3 Bring Your Questions to the Module Cafés We will have bi-weekly module cafes where you can discuss any additional questions that you might have. In addition, these module cafes will also serve to answer any question regarding the assignments. 4 Read Guideance on Planning and Structuring To guide you through all the hoops, I will also stay in touch with you with am email every Friday to make sure that you are on board. It is vital that you check your university email regularly, since we will only communicate with you through this address. The Module Webpage Let us take a closer look at how this website is working. This space is going to be the main learning tool for this term’s Introduction to Data Science for Politics and IR. Obviously, I will guide you through the material on this web-page via text. Think of it as a replacement for the lecture slides that you might find in other contexts: A place to find the core concepts and central ideas of this class. But not all is text. Sometimes, nothing beats someone actually explaining you core ideas. For that purpose, I will record small videos and embedd them here. The best thing about videos: You can watch them as many times as you want… Finally, this format is also really handy, because we will be able to take a look at R code. R is a computer language invented for data analysis, and you will learn how to use it in PL 9239. This is how it looks like when we will be covering R code. # All text in a line after the hashtag is a comment. # Here we assign some variables a &lt;- 1 b &lt;- 2 # This is how to add them and assign the result to another variable c &lt;- a+b # And finally we print the result cat(c) ## 3 Last but not least, each week will have some readings that will reflect the core content of the respective week from yet another perspective. I will add pointers to the readings whenever appropriate. You can find all information about the readings in the bibliography. Getting in Touch If you have questions or need any kind of help, feel free to get in touch. The whole purpose of our weekly workshops and our bi-weekly module cafe is to provide you with ample space for your questions. Make use of this! You can always drop me an email. I will respond at the end of the working day. You can also book time in my office hours Tuesday 3-4pm; Wednesday 3.15-4.15pm; Friday 5.15-6.15pm. "],["tools-for-working-with-data.html", "Chapter 1 Tools for Working with Data 1.1 What Is In a Data Spreadsheet? 1.2 Key Concepts 1.3 Meet: R 1.4 Readings for This Week", " Chapter 1 Tools for Working with Data This week we will get to know the basic tools we need. 1.1 What Is In a Data Spreadsheet? When we work with data, the first step is always to get an overview of the data—be it in academia, for a company or in government. The whole first part of this module is dedicated to teaching you how to understand your data in a quick and efficient way. We will learn a couple of important things. How to access data. How to get a quick overview over the data. How to describe data and extract their core characteristics. How to communicate and visualise results. How to manage everything efficiently. To give you an idea how and why even these basic skills matter in real life, I talked to Dr Sebastian Sternberg. He has a degree in politics and works as a data scientist with KPMG at the moment. In the interview, he was so kind to offer us insights into how he is working with data and how relevant data reports are in his daily work. 1.2 Key Concepts Let as begin with some key definitions. 1.2.1 What is a Statistic? Statistics summarise large amounts of numerical data. Statistics are really useful if one wants to get a good overview over data. A statistic is a characteristic of a sample. Imagine we make a number of observations and put in numbers what we see. When we calculate a statistic on this data, the statistic is able to describe the sample we collected. If we collect a different sample, the statistic is very likely to have slightly different values. The goal of statistical methods is to make inference about a population based on information from a sample of that population. Often, we might be interested in more than just the data at hand. By drawing a sample, we hope to generalise beyond our sample and learn something about the the overall population. For example, by asking a few hundred Welsh voters about their voting intentions we aim at saying something about the voting intention of all voters in Wales. To estimate parameters, we use statistical methods. Estimating a parameter means that you need to be able to say something about a certain parameter on the basis of a couple of data points that you collected. We will be learning how to use the correct statistics to infer what we believe is the most likely value for a certain parameter. In the voting example, we might be interested to understand what share of voters who would cast their vote for Plaid Cymru for example. Statistics can separate the probable from the possible. When we collect data, the data can have a range of different values. Of course, not all possible data points are equally likely. The beauty of statistics is that they can be used to tell us which values we can expect to see more often and which ones least often. For example, voters might be able to cast their vote for a whole range of parties—e.g. Labour, Conservatives, Plaid Cymru or the Welsh Nation Party. However, voting for these parties is not necessarily equally likely. It is reasonable to expect that, say, the Conservatives will receive more votes than the Welsh Nation Party. 1.2.2 Some More Definitions Some more definitions that a really useful for our module. It would not hurt if you could learn them by heart. Population: The full set of cases about which we want to generalise. Sample: A subset of the population. Variable: Logical set of attributes (characteristics) of an object (person, thing, etc.) that can vary across a range. Parameter: A characteristic of a population; usually unknown. Descriptive Statistics: Statistics that summarise the distribution of the values of variables in a sample. Inferential Statistics: The use of statistics to make inferences about a larger population based on data collected from a sample. 1.2.3 Types of Data Levels Example from our Survey Real World Example Measurement Possible Operations Nominal Tea or coffee? Gender, religion Categories Frequencies Ordinal Do you like summer? Social class, attitudes Categories and ranking Frequencies and ranking Interval How many hours do you study? Age, income All above and distance All above and addition and subtraction Remember that you were filling out the survey just before term? Let us go and take a look at some examples of your data to better understand the different types of data that are out there. Nominal Data What do you prefer to drink in the morning? The variable encodes three different answer categories. These categories cannot be ranked, obviously. These are your responses: Ordinal Data How much do you like summer? You were given a scale that ranged from 1 to 5. The answers to this question can be ranked. However, it is not necessarily true that the distances between the answer categories is always the same. Now look at your responses: Here comes the sun! Interval Data Finally, let us consider some interval data. How many hours do you study per week? This data provides full hours. The data is discrete, its values can be ranked, and the distance between any two neighbouring categories is always the same. 1.3 Meet: R Now that we have a bit of an overview how data can look like, let us take a look at the main tool that we will use to manage, analyse and visualise data. We will use a programming language for this module that is called R. It is free and open source, so you can install it easily on your computers. It is also very powerful which means that all the effort that you are investing in these 10 weeks of the term to learn it will heavily pay-off when you are analysing data in the future—be it for research or in any other professional context. And ‘paying-off’ is meant quite literally here. R is a really valuable skill set to have on your CV and is certain to boost your employability quite a bit. If you want to know more about R, where its coming from and how it all developed, check the Wikipedia Page as a start. Also, feel free to go wild on the homepage of the R Project itself or in any other of your favourite corners of the internet. Beware: The rabbit hole is quite deep. #nerdalert 1.3.1 Install R Installing R is pretty straightforward: Go and visit the homepage of the The Comprehensive R Archive Network where you can find the latest version of R. At the top of the page, you can chose between your operating system: 1.3.2 Install R Studio Now that you have installed the programming language, let’s go and get a nice interface that actually helps us get our work down. R-Studio is a programme that makes it much more easy to write and execute R code. Go and get the free R-Studio desktop version. Install the version that suits your operating system. Nota bene: Make sure you first install R and then R-Studio. 1.3.3 How to Work with R Here is a quick video in which I show you around. You can also take a look at Chapter 2 in Fogarty (2019) for a lot of helpful details. 1.3.4 First Steps in R Now that you know how R-Studio looks like and how to use it, let’s go and try it out. As you saw, in the console tab you can run commands directly. But it is better practice to type them in an R script and send them. Every line of the text editor can be sent using Str + Enter in Windows and Cmd + Enter in MacOS. Objects R can keep several objects in memory at the same time. To distinguish them, object have names. Objects are assigned with an arrow like this: &lt;- Let us assign some values to objects. a &lt;- 5 b &lt;- 6 c &lt;- a * b Objects can be called using their name. Here on this homepage, you will see a second grey block that will give you the output. If you type all up in R Studio, you will find all R related output in the console. a ## [1] 5 b ## [1] 6 c ## [1] 30 We can combine several values with the function c(). Functions are really useful. There are basically three elements to a function. * First of call, they have a name—here in this case it is the letter c which stands for concatenate. * Then, functions always need one more more inputs. A function receives its inputs in the brackets (). * If you call a function and provide it with its correct inputs, it will do its thing and return the output that you asked for. a &lt;- c(1,2,3) a The object a is not a scalar any more. It is now a vector that has three elements to it, the numbers 1, 2 and 3. You can of course also assign more than just numbers. We can assign strings—here a couple of three digit country codes. cntry &lt;- c(&quot;BRA&quot;, &quot;GER&quot;, &quot;FRA&quot;, &quot;NLD&quot;) cntry ## [1] &quot;BRA&quot; &quot;GER&quot; &quot;FRA&quot; &quot;NLD&quot; If you want to see all the objects that we have in the workspace so far, try the function ls(). ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;cntry&quot; You want to remove an object? Then use the function rm() like so. rm(cntry) See, the object cntry is not in the workspace any more. ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; R Data Types In R, there can be different types of objects. Some can only take specific types of data. Scalar: numbers, characters, logical values Vector: sets of scalars Matrix: two-dimensional set of scalars of same type Data frame: Collections of vectors of (possibly) different types but with same length Let us begin from scratch with am empty workspace. To delete everything, we will nest two of the functions above. rm(list=ls()) First we assign some scalars. # scalar a &lt;- 43 b &lt;- a + 7 a ## [1] 43 b ## [1] 50 And here we go with some vectors. x &lt;- c(a,b,a,b) y &lt;- x + 10 cntry &lt;- c(&#39;Brasil&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Singapore&#39;) x ## [1] 43 50 43 50 y ## [1] 53 60 53 60 cntry ## [1] &quot;Brasil&quot; &quot;Canada&quot; &quot;China&quot; &quot;Singapore&quot; Something interesting just happened here: You see the object x which is a vector? R added 10 to each of the scalars in x when calculating y. In programming you call this broadcasting. Pretty nifty! Next, we build a matrix, for example by binding two columns with the function cbind() or two rows with the function rbind(). z &lt;-cbind(x,y) z2 &lt;-rbind(x,y) z ## x y ## [1,] 43 53 ## [2,] 50 60 ## [3,] 43 53 ## [4,] 50 60 z2 ## [,1] [,2] [,3] [,4] ## x 43 50 43 50 ## y 53 60 53 60 This is how a data frame looks like. It accepts vectors with any value. Data frames are quite similar to spreadsheets, for example in Excel or LibreOffice. dat1 &lt;- data.frame(cntry, z) dat1 ## cntry x y ## 1 Brasil 43 53 ## 2 Canada 50 60 ## 3 China 43 53 ## 4 Singapore 50 60 1.3.4.1 Selecting Elements Finally, we will learn how to select elements from objects. We will begin with vectors. a &lt;- c(1,2,3,4,5) b &lt;- a + 10 We can select elements in R with [ ]. a ## [1] 1 2 3 4 5 a[2] ## [1] 2 a[2:4] ## [1] 2 3 4 Like in many other programming languages, the colon : expresses a range. Here we select all values in the range from the second to the fourth entry. We can also select elements from a matrix. The , helps to distinguish between the two dimensions. The selection m[1,1] will return the first element of the first column. In a similar vein, the selection m[3,2] will return the third element of the second column. m &lt;- cbind(a,b) m[1,1] ## a ## 1 m[3,2] ## b ## 13 Finally, we select elements from a data frame. We create a variable ‘name’ with the names of Mark, Luise and Peter. The variable ‘bike’ contains their bikes: Mountainbike, Single Speed and Racing Bike. We capture the hours per week they ride on it: 4, 7, 8 and finally we bring all variables together in a common data frame dat2. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat2 &lt;- data.frame(name, bike, hours) In data frames, you can select elements with the operator $ and the name of the variable dat2$bike ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; You can of course still use the positions, too! dat2[,2] # gives you all entries for the second column ## [1] &quot;Mountainbike&quot; &quot;Single_Speed&quot; &quot;Racing_Bike&quot; dat2[1,2] # gives you the first entry for the second column ## [1] &quot;Mountainbike&quot; Since just selecting might not always be enough, R allows us to select elements based on conditions. R comes with everything formal logic requires. is equal == is not != smaller &lt; larger &gt; smaller equal &lt;= larger equal =&gt; You can build more complex queries via AND &amp; OR | Let’s go: x &lt;- c(1,56,23,89,-3,5) y &lt;- c(24,78,32,27,8,1) x[x &gt;20] # greater as 20 ## [1] 56 23 89 x[x &gt;20 &amp; x !=89] # greater as 20 and unequal 89 ## [1] 56 23 x[x&gt;0 | x==-3] # x where x greater 0 oder x=-3 ## [1] 1 56 23 89 -3 5 y[x==1] # y where x=1 ## [1] 24 1.4 Readings for This Week Please read chapter 2 and chapter 3 in (Fogarty 2019). This is where we end for this week. I am looking forward to meeting you in the workshop on Tuesday 12pm–2pm! "],["describing-data.html", "Chapter 2 Describing Data 2.1 Statistics to Summarise Data 2.2 Data Management with R 2.3 Describing Data Using R", " Chapter 2 Describing Data 2.1 Statistics to Summarise Data When we describe data, we typically have three questions: How are the values of a variable in a sample distributed? What do typical values look like? How clustered or dispersed are these values? 2.1.1 Tables Data that comes in categories—be they nominal or ordinal—can be easily summarised. Simply list possible values for a variable, together with the number of observations for each value. Can be used to summarise distributions of one nominal, ordinal or interval variable. Intervals must be constructed for interval-level variables (e.g. age, income). Absolute frequencies record the actual number of observations for each value. Relative frequencies record the proportional distribution of observations. This is an example from our data: Frequency Percent Coffee 7 0.44 Other 4 0.25 Tea 5 0.31 This is how the data looks like for the question on how much you like summer. Frequency Percent Highest 14 0.88 4 2 0.12 2.1.2 Central Tendencies One of the first questions that comes to our minds when we consider a variable is: What is a typical value for it? Intuitively it makes sense to chose a value that shows the central tendency of a variable. There are three different statistics that can help summarise the distribution of scores by reporting the most typical or representative value of it. Mode The mode is the value that occurs most frequently. You can use it for all kind of categorical data. There can be more than one mode and under this circumstance you would speak of multimodal data. In case the answer categories can be ranked, the mode does not need to be near the centre of the distribution. Finally, the mode is resistant to outliers. For political scientists, the mode is a really important measure: We use it whenever we want to determine the winner of an election. When I asked you for your vote (if you had had the choice), this is how you responded. Votes Biden 15 NA 1 The modal candidate is clearly Biden for your class. Nobody would have voted for Trump. One response is missing. Median The median is the value that falls in the middle of an ordered sample. Obviously, the measure cannot be used for nominal variables—they can not be ranked. The median is the 50th percentile point. This means that when you count all cases, half of the sample will be smaller than the median and the other half is larger than the median. When the sample size is even, the median is the midpoint between the two measurements in the centre. By definition, the median is resistant to outliers: Irrespective of how small the smallest value or how large the largest one, the one value that splits the sample in half will remain always the same. These are your responses to the question how many hours you think you should study. I chose the dark blue line to indicate the median value of the data. Overall, we have 16 responses in the data. Since the number of respondents is odd, the median is the average between the 8th and the 9th response. Mean The mean is the sum of the observations divided by the number of observations \\[\\begin{equation*} \\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_N}{N} = \\frac{\\sum_{i=1}^{N}x_i}{N} \\end{equation*}\\] with \\(\\bar{x}\\) being the mean of variable \\(x\\); \\(\\sum\\) being the sum; \\(i\\) the individual cases (of x); \\(N\\) the number of cases. \\end{itemize} This all looks a bit fancy, but it is actually just a matter of understanding the notation. Conceptually, the mean is really straightforward—it is nothing different than the good old average. The mean has a couple of interesting characteristics. It is only applicable to interval variables. The mean is a good measure of central tendency for roughly symmetric distributions, but it can be misleading in skewed distributions. Most importantly, the mean is really susceptible to outliers. There is a nice physical interpretation of the mean: it is the centre of gravity of the observations. Take a look at your data. Your mean is slightly lower than your median. Why is that so? Basically, those who study less than 10h per week have a considerable influence on the mean and push the mean to the left of the median. 2.1.3 Spread You now know how to chose a typical value that summarises your data. Next on the list is to characterise their spread. Are all values really close to one another? Are they far apart? Do many of them hang out on one side of the distribution, and are they far apart on the other side, i.e. is their distribution skewed? To measure all this, we will now take a look at different measures of spread. In essence, they are statistics that summarise the variation around our average value. We will consider four different measures that all build on each other. Range: Difference between two values, typically the minimum and the maximum. Deviation: Difference of a value from the mean. Variance: Squared difference of a value from the mean. Standard Deviation: Square root of the squared difference of a value from the mean. Range The range is the the distance between the largest and the smallest values, i.e. maximum–minimum. It will be distorted by extreme values. The interquartile range is another really important range. It covers the middle 50% of observations, so the range from the 25th percentile to the 75th percentile (lower quartile–upper quartile). Here you can a number of concepts that we covered working together to describe the data: The 25% quantile, the 50% quantile, the 75% quantile, the median, the mean and the interquartile range. Let us describe some of your data, here how old you are. How large would be the interquartile range? Hint: the value is 1. Deviation The deviation of any observation is its difference from the mean. \\[\\begin{equation*} (x-\\bar{x}) \\end{equation*}\\] What is the sum of deviations? Do the maths with a couple of numbers on a piece of paper. \\[\\begin{equation*} \\sum(x-\\bar{x}) = ? \\end{equation*}\\] You will find that it is always 0—simply because the values keep cancelling each other out. So what can we do? One solution would be to calculate the Mean Absolute Deviation \\(\\text{MAD}\\). \\[\\begin{equation*} \\text{MAD}=\\frac{\\sum(|x_i-\\bar{x}|)}{n} \\end{equation*}\\] In case you do not know the sign “\\(|\\)”: anything that is in between two “\\(|\\)” will always return its positive value. So \\(|5| = 5\\) and also \\(|-5| = 5\\). Let us take a look at your data, here how much you actually study and how much you think you should study. I am including again the interquartile range in red (which goes from where to where again?), and the mean in gold. When you do the maths you will find out that the \\(\\text{MAD}_{\\text{actual}} = 5.19\\) and \\(\\text{MAD}_{\\text{ideal}} = 6.12\\) Variance In practice, however, you will find that the Mean Absolute Deviation, is rarely used. Instead, you can often find the variance. It is basically the same as the \\(\\text{MAD}\\), but different. To avoid the canceling out, we will square the distance of each value to the mean. And for arcane statistical reasons that irrelevant for this class, we now subtract 1 from the overall cases \\(n\\) in the denominator. \\[\\begin{equation*} s^2=\\frac{\\sum(x_i-\\bar{x})^2}{n-1} \\end{equation*}\\] With the study data, the variance of the actual number of hours you study is \\(s^2_{\\text{actual}} = 35\\) and the variance for the number of hours you consider ideal is \\(s^2_{\\text{ideal}} = 53.67\\). Contrast the difference between the two variances to the difference in the two \\(\\text{MAD}\\)—it is much larger! The reason is simply that we are now taking the sum of the square of the distances and not just that absolute distances, which of course weighs much more for larger numbers. With the \\(\\text{MAD}\\) each data point contributes an equal share to the overall measure of spread. For the variance, this is no longer true. Those data that are further apart from the mean will drive the variance to a much larger degree than those data that are close to the mean. Standard Deviation While the variance is already a big step forward in measuring spread, it has one important drawback: It is quite abstract and really hard to interpret. Ideally, we would want to understand the measure for spread on the same metric as the data themselves. Doing so is straightforward. We simply take the square root of the variance—and the resulting standard deviation is in the metric of our data \\(x\\). \\[\\begin{equation*} s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}} \\end{equation*}\\] In our running example—the number of h you study and the number of h you think you should study—this is what we get. The standard deviation for the former is \\(s_{\\text{actual}} = 5.92\\) and the standard deviation for the latter is \\(s_{\\text{ideal}} = 7.33\\). Compare these values to the Mean Absolute Distances: \\(\\text{MAD}_{\\text{actual}} = 5.19\\) and \\(\\text{MAD}_{\\text{ideal}} = 6.12\\). They, too, are on the original scale. The standard deviation, however, is much more sensitive to outliers, which is a really desirable characteristic. 2.1.4 Ratios and Rates Finally, ratios and rates. Proportions We start with something really simply, the proportion. It is calculated as \\[\\begin{equation*} p = \\frac{f}{N} \\end{equation*}\\] With * \\(p\\) being the proportion; * \\(f\\) being the number of cases (frequency) in one category; * \\(N\\) being the number of cases in all categories of the variable. The proportion is useful if we want to answer a question like: What is the proportion of students having tea for breakfast? \\[\\begin{equation*} p = \\frac{5}{16} = 0.31 \\end{equation*}\\] Percentages You like percentages better? Simply multiply your proportions with \\(100\\). \\[\\begin{equation*} P = \\left( \\frac{f}{N}\\right) 100 = \\left( \\frac{5}{16} \\right) 100 = 31.25\\% \\end{equation*}\\] Rates A rate is really useful if you want to express for example how often a proportion occurs in a given amount of time. We can calculate the rate as \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} \\end{equation*}\\] with * \\(r\\) being the rate: the frequency per time in a certain set * \\(f\\) being the number of cases (frequency) in one category * \\(t\\) being the time under consideration * \\(N\\) being the number of cases in all categories of the variable * \\(u\\) being the unit under consideration Let us advance step by step and wrap our head around this with the help of an example. We want to understand how many of you actually bought a computer during 2019. This simplifies our formula a bit. \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\text{all students}} = \\frac{\\frac{6}{1}}{\\frac{16}{1}} = 0.375 \\end{equation*}\\] We can take this a little further and ask How many computers did 10 student buy in 2019?. To answer this question, we simply adapt the number of people in the unit and set \\(u=10\\). \\[\\begin{equation*} r = \\frac{\\frac{f}{t}}{\\frac{N}{u}} = \\frac{\\frac{\\text{computer purchases}}{\\text{one year}}}{\\frac{\\text{all students}}{\\text{unit of 10 students}}} = \\frac{\\frac{6}{1}}{\\frac{16}{10}} = 3.75 \\end{equation*}\\] So 10 students bought 3.75 computers in 2019. Growth The last thing we want to look at is how much growth. In particular in economics, growth has a really prominent role, and a lot of theory is built around all kinds of growth related to different cash flow: GDP, GDP per capita, return on investments to just name a few. But of course, growth can also happen in other areas like literacy rates in development countries, unemployment or votes. Growth can be expressed as a percentage change. \\[\\begin{equation*} G = \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\end{equation*}\\] with * \\(G\\) being the growth rate in a variable from time 1 to time 2 * \\(f_1\\) being the number of cases (frequency) at time \\(t_1\\) * \\(f_2\\) being the number of cases (frequency) at time \\(t_2\\) Again, let us take a look at our own data to understand what is going on here. \\[\\begin{align*} G &amp;= \\left(\\frac{f_2 - f_1}{f_1}\\right) 100 \\\\ &amp;= \\left(\\frac{\\text{Purchases in 2020} - \\text{Purchases in 2019}}{\\text{Purchases in 2019}}\\right) 100 \\\\ &amp;= \\left(\\frac{-2}{6}\\right) 100 = -33.33\\% \\end{align*}\\] Apparently, you were purchasing less computers during the pandemic year, more specifically, the sales had a negative growth of \\(G = -33.33\\%\\). This could be related to the fact that a number of you bought a new computer when you started your university career back in 2019. 2.2 Data Management with R To calculate all above, we first need to take a closer look at some data management this week. 2.2.1 Libraries Libraries are functions that do not ship on board your original R programme. Instead, you have to get them from them internet. Think of it like wanting to read a book. You first have to get it from a shop and bring it home, where you will add it to your book shelf. In R, you can use the command install.packages() to download a package to your computer. If you execute the command, R might prompt you for a location—simply pick one that is close to you. Obviously, \"name_of_library\" is a placeholder here, so don’t try this at home with that particular code snippet and replace it with the package you actually need. install.packages(&quot;name_of_library&quot;) Now you have downloaded the programme to your computer. Or, in other words, you have added the book into your bookshelf. However, you are not sitting in your lounge chair with the book in your hand, yet. For that, you would still have to go to the library in your house and get the book. This is exactly what we will be doing now with the R package. We will collect the package from our library and load it into the active workingspace. library(&quot;name_of_library&quot;) 2.2.2 Setting the Working Directory R is quite stupid. It does not know where to look for files—be they R code or any other data—unless you really tell it where to look for a file. Typically, we will instruct R to make its home in the exact place where you save your main R script with a function called setwd(). As its argument, you provide the path you are working in. For me on my office machine, this is how it looks like. setwd(&#39;/Users/foo/PL9239 Intro_data_science/intro_data_science_homepage&#39;) Now, R will start looking for everything starting in that particular working directory. To see which working directory you are in, you can type getwd() 2.2.3 Reading Data Working directories are particularly relevant if you want to read in data sets. Data mostly comes in two formats: comma separated values, or short .csv, and as a Microsoft Excel spreadsheet .xls. Most open data formats can be read in with a function that begins with read.foo. Of course, just reading it is not enough—you have to assign it to an object if you want to work with it. csvdata &lt;- read.csv(&quot;dataset1.csv&quot;) If we want to read in .xls data, we have to load a library that can help us with that. We will go with the readxl package. Again, we are assigning the data to an object so that we can call it later. library(readxl) xlsdata &lt;- read_excel(&quot;dataset1.xls&quot;, sheet=&#39;sheet_foo&#39;) 2.2.4 Saving Data You can also save data. Let us create a toy data set again. name &lt;- c(&#39;Mark&#39;, &#39;Luise&#39;, &#39;Peter&#39;) bike &lt;- c(&#39;Mountainbike&#39;, &#39;Single_Speed&#39;, &#39;Racing_Bike&#39;) hours &lt;- c(4,7,8) dat &lt;- data.frame(name, bike, hours) Now save it. save(dat, file = &quot;toydata.RData&quot;) To check the magic of this we remove the data set and then try to call the object. rm(dat) dat ## Error in eval(expr, envir, enclos): object &#39;dat&#39; not found Nothing there. load(&quot;toydata.RData&quot;) dat ## name bike hours ## 1 Mark Mountainbike 4 ## 2 Luise Single_Speed 7 ## 3 Peter Racing_Bike 8 Tada! Worth noting at this stage, that when you use the native R way of saving data, R saves your actual object, here the object dat. 2.3 Describing Data Using R Now, with a bit more of a background, we can calculate all of this week’s statistics. In R, this is really straightforward. Let us cover the commands for central tendencies and spread for now, since they are built into R. 2.3.1 Central Tendencies You can take a look at the frequency of categorical data with the function table(). This is your data on how many data you are partying per week—of course, out of pandemic times… table(partydays) ## partydays ## 1 2 3 4 5 6 ## 2 2 5 2 3 2 Quite a number of party animals here. Now, what is the mode? We have to call a package for that function. library(DescTools) Mode(partydays) ## [1] 3 ## attr(,&quot;freq&quot;) ## [1] 5 The output is a bit cryptic at first, but it tells us that the value \\(3\\) is the most frequent one and that \\(5\\) individuals chose it. For the median and the mean, let us take a look at a continuous variable. For example, how much money you spend when you go out. You call the median with median() and the mean with mean(). median(spend) ## [1] 20 mean(spend) ## [1] 20.78125 Stop for a second and think about the results: * What does the relationship between the median and the mean tell us here? * Is the distribution really symmetric? * Is the distribution maybe skewed? If so, how? 2.3.2 Spread The rest is a piece of cake just the like. You can calculate the variance using var() and the standard deviation with sd(). var(spend) ## [1] 154.7656 sd(spend) ## [1] 12.44048 Please read chapter 7 in (Fogarty 2019). Chapters 4 and 5 are a good idea, but you do not necessarily have to. "],["visualising-data.html", "Chapter 3 Visualising Data 3.1 Communicating with Data Effectively 3.2 Plotting Data with R 3.3 Readings for This Week", " Chapter 3 Visualising Data 3.1 Communicating with Data Effectively Good data visualisations tell stories that do not need much explanation. Those who view the data can understand what insights you want to communicate from the data without necessarily using a lot of text (like this one) as their explanation. Viewing data visualisations is a lot of fun and a seemingly effortless way to explore and process the information in the data. The Internet is full with great examples at different levels of complexity. It is easy to get lost, take a look for example here, here or here. I am sure you can find many more interesting examples on the web. From a creator point of view, they allow to communicate a lot of information in a direct way. But it goes without saying that a well done data visualisation requires a bit of thinking. 3.1.1 Thinking About your Data Visualisation At its core, data visualisation is actually pretty simple. How can we possibly visualise the data in a way that it a) communicates the key information in our data well while b) transporting it so that it is easy to understand for the viewer? Whenever you want to visualise data you should probably first sit down and think about what it is that you want to tell the end user of your visualisation? What is your key message? The visualisation ideally expresses more content with less ink. Or in more mathy terms: \\[\\begin{equation} \\text{maximize} \\left( \\frac{\\text{content}}{\\text{ink}} \\right) \\end{equation}\\] This optimization problem comes with an important sub-constraint: The skill level of your potential audience. It will greatly determine the level of complexity that you can pitch. Who is your audience? Are they a lay audience without a lot of statistical training? So they would probably require a visualisation that allows for a more intuitive understanding. If the audience is more technically versed, your approach to data visualisation can be more sophisticated. Let us talk about colour. Think about the medium on which you want to use your figure. Is it made for publication in any printed format such as a report or a book? If so, be aware that not everybody is printing in colour, but usually in black and white. This means that you have to consider closely how light or how dark your colours are. For example, it might not be a good idea to use green and blue at the same time, simply because they are likely to have similar shadings. A colour that is too light might not be seen well on a white paper when printed. Of course, you are much more free in the choice of your colours when you your figures are for display on screens—like this homepage. Also think about the colourblind. Can they view your figures, too? Be careful when you use the spectrum from red to green. Ultimately, the choice of colours is a matter of preference and expression. If you feel arty, go wild! At the same time, picking the right colours is also a deep and awesome rabbit whole. Check this talk by the guys who invented the recent viridis colourscheme in case you want to get a taste. 3.1.2 Visualising ‘The Data’ When we are visualising data, we probably want to get an overview over it ourselves, so the first move is to take a look at ‘the data’ itself. How does it look like? How is it distributed? We learned that we have different data types, i.e. nominal, ordinal, interval and continuous data. 3.1.2.1 Nominal Data The best way to visualise nominal data is the bar chart. It gives you a nice and succinct overview over the data without distorting your perception. You can plot data vertically as we have done here in blue or you can plot them horizontally—here in red. Be aware of using of not using pie charts. While they are indeed quite popular, they are wrongly so. It seems that Microsoft made us believe that this is a good way to represent data when they introduced visualisations in Excel. You can plot them with R, but it will remind you that there is research showing that it distorts your perception of the data. You will systematically overestimate the relevance of small proportions of the data and think that these proportions are much larger than they really are. 3.1.2.2 Ordinal Data Ordinal data are nominal data that can be put into a logic order. Of course you can again use the barcharts like before. But you can also make use of the fact that now you actually have ranked data. Why not plotting them as a stacked barchart? 3.1.2.3 Interval or Continuous Data Interval data converges to continuous data in the extreme, so let us treat them at the same time here. The left figure charts data simply on the basis of their value in red. We run into a pretty straight forward problem: We cannot see how often we are actually seeing each observation. The solution is to stack values in case they occur more often, like in the purple figure to the right. Here we have a really nice overview of how often we observe each case. Plotting each individual data point is realistic for our case where we have just a few observations. However, this can become way too complex to process in case you have a larger number of values. In these circumstances you can visualise the distribution of the data with bins in which you collect your data. On the left in the blue histogram you can see how our data would look like if we split it up into eight groups—it results in a really nice pattern! On the right you can see how the data looks like if we just used four bins for it. 3.1.3 Visualising Typical Values and Spreads Now that we have a first overview over how the data usually looks like, we want to become a bit more systematic in our approach to getting to know the data. Remember that we talked about typical values and spread when we wanted to describe data? This is what we want to visualise now. When you have interval or continuous data, there are a number of typical questions that you might want to know from the data. What is a typical value for the data? How are the values of a variable in a sample distributed? How clustered or dispersed are these values? There is one visualisation that summarised this all: boxplots. They easily maximise the content-per-ink criterion. We can identify a number of things from this plot. The median of the distribution is a vertical bar and somewhere in the the colourful box in the middle. This box itself also expresses information: it covers the interquartile range, so the central 50% of the distribution are within the range of this box. If the distribution is symmetric, you can see that the median is in the centre of this box. In case the distribution is skewed, the median will push to one side or the other. The whiskers that you see give an impression of the magnitude of the overall spread of the data. Typically they are 1.5 times the interquartile range starting from the box. Data that goes beyond this range will be plotted with a circle. Let us take a look at how useful this really is in practice. In this first figure we are plotting the hours that you are actually studying in red and the hours that you think you should be studying in yellow. Let us begin with the median values. The median for the hours actually studied is 12, the median for the hours ideally studied is 21. You also get an impression of the magnitude of the interquartile range: it ranges from 8.75 to 18.5 for the top figure in red. For the lower figure, it begins at 14.75 and goes to a value to 25. Both distributions are a little skewed. On the top, the median is a bit to the left of the box. This means that a quarter of you study between the lower end of the box and the median value so somewhere between 8.75 and 12. The same interquartile range between the lower 25% quantile and the median is much larger for the hours that you think you should ideally be studying. The whiskers offer an impression of the data beyond the interquartile range. The smallest value for the hours that you actually study is 2 in the top figure. The maximum hours that any of you studies is 20. The whiskers are a little different for the hours that you think you should ideally be studying. On the left, they range from 8 hours and they grow all the way up to 30 hours. In the next figure, let us take a look at your age: I plotted your age between 18 and all the way up to 30 years and you can see it is not necessary to cover anymore space. Large parts of you—at least 75%—are either 19 or 20 years. You can also see an outlier for one person who is 25 years old and is outside of the whiskers. He or she can be seen as clearly distinct circle. Lastly, you also offered information about a variable that I think is particularly interesting. How much money do you spend when you go out? Here we can see the information nicely summarised in a box plot. The median person spends 20 GBP when he or she goes out at night. A quarter of you spent between 10 GBP and 20 GBP. Another 25% spent between 20 GBP and 26 GBP when you go out. The whiskers give an indication of the whole spread of the distribution. There are people who typically spend 0 GBP when they go out. On the other side of the spectrum somebody seems to have quite deep pockets and is able of spending up to 50 GBP per night out. 3.2 Plotting Data with R Now it is your turn. When it comes to plotting, there is a package that has been put out a couple of years ago which is called ggplot2. On the web, you can find a lot of R visualisations that make use of it. For this class, however, we will stick to the basics—simply because I think it is good to first learn how to really control R from scratch. You can then always switch to higher level printing packages such as ggplot. Think of it as learning how to drive: In this module we will shift gears manually and you will find that switching to a car with automatic gears will not be any problem later. If you want to know more about ggplot2, this week’s reading of Chapter 8 in Fogarty (2019) is your friend. In all we do here, I am showing you some basic options. Look for the name of the function in the help tab to find out more details about the functions. 3.2.1 Basics in Plotting First, let us whip up some data. # This creates a vector with a sequence of integers from -10 to 10 with a distance of 1 x &lt;- seq(-10,10,1) x ## [1] -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 ## [20] 9 10 We calculate a second variable. a &lt;- 5 b &lt;- 3 c &lt;- .5 y &lt;- a + b*x + c*x^2 y ## [1] 25.0 18.5 13.0 8.5 5.0 2.5 1.0 0.5 1.0 2.5 5.0 8.5 13.0 18.5 25.0 ## [16] 32.5 41.0 50.5 61.0 72.5 85.0 And then we use the function plot() to chart the data. plot(x,y) Simple, right? R takes over a number of jobs for you, from selecting the right data range in both axes, to chosing the shape and colour of representations for the data etc.. To give you a first idea, this is what happens when you tweak a number of options. plot(x,y, cex = 3, # size of the data points pch = 16, # point character col = &#39;aquamarine3&#39;, # chose the colour. R defaults with inverted commas. las = 1, # rotates labels in y axis by 90 degrees, xlim = c(-15, 15), # This defines the limit of the x-axis you want to plot in main = &#39;My First Figure&#39; # Title ) You select the options by adding them with a comma. Check the help file in R-Studio for what is available. We will have a proper session on plotting bivariate data in a couple of weeks for all this, so do not worry about the details at this stage. The important message that needs to come across is that R will handle a lot for you—but the possibilities to adapt everything that you were seeing are literally endless. 3.2.2 Colours R is great for using all different kinds of colours. You can check the already pre-defined colours by calling the function colors(). Here, let us call the first 15 ones. colors()[1:15] ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; ## [5] &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; ## [9] &quot;aquamarine1&quot; &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; ## [13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; And you can even define your own colours using the rgb function. Cardiff University’s corporate identity colours for example can be generated like so. # Predefined Cardiff colours # primary cardiffred &lt;- rgb(211,55,74, maxColorValue = 255) cardiffblack &lt;- rgb(35,31,32, maxColorValue = 255) cardiffgrey &lt;- rgb(47,68,78, maxColorValue = 255) cardiffgold &lt;- rgb(189,158,94, maxColorValue = 255) # secondary cardiffblue &lt;- rgb(21,44,81, maxColorValue = 255) cardiffpurple1 &lt;- rgb(29,15,51, maxColorValue = 255) cardiffpurple2 &lt;- rgb(60,44,89, maxColorValue = 255) And then you can use them for plotting. a &lt;- seq(1,10,1) b &lt;- rep(1,10) # repeats the value &#39;1&#39; 10 times plot(a, b, col = c(cardiffred, cardiffgrey, cardiffgold), cex = 2, las = 1, pch = 15 ) The number of data points is larger than the number of colours. R will therefore cycle through the colours. 3.2.3 Visualising Nominal and Ordinal Data To visualise data in a way that we can chart them as a barplot, we first have to count them. How often do we observe each value? There is a really useful function for that in R, it is called table(). Just calling it without plotting is quite revealing. table.breakfast &lt;- table(breakfast) table.breakfast ## breakfast ## Coffee Other Tea ## 7 4 5 Just a quick fyi: You can use the function prop.table() to get percentages. Then round() will round the resulting values. prop.table.breakfast &lt;- prop.table(table.breakfast) prop.table.breakfast.rounded &lt;- round(prop.table.breakfast, digits = 2) prop.table.breakfast.rounded ## breakfast ## Coffee Other Tea ## 0.44 0.25 0.31 But we are here for visualising data. We need the object table.breakfast for our barplot. barplot(table.breakfast) Let us make this a bit more pretty. barplot(table.breakfast, ylab = &quot;Frequency&quot;, # Adds label to the y-axis col = cardiffred, # You know this already las = 1, # and this border = NA # Removes the small black border around the bars ) barplot(table.breakfast, xlab = &quot;Frequency&quot;, las = 1, col = cardiffgold, horiz = TRUE # plots bars horizontally instead of vertically ) 3.2.4 Visualising Continuous Data We also wanted to take a look at continuous data. Let us see how far you are living from the university for that matter. # R uses an algorithm to determine the number of bins hist(commute, col = cardiffgold, xlab = &quot;Minutes&quot;, ) # Here we control a bit more by ourselves hist(commute, breaks = 3, # This is a very rough break into three bins col = cardiffpurple1, xlab = &quot;Minutes&quot;, main =&#39;&#39;, # suppress the default main title border = &quot;white&quot; # border around the histograms--here set to white ) 3.2.5 Boxplots Finally, let us take a look at boxplots. The default function is called boxplot(). Calling it with a continuous variable such as spend as its argument will already generate a plot that has all the important information. boxplot(spend) But admittedly it is far from pleasent to look at the figure. Since we want that others pay attention to our message, let us make our visual communication more appealing. # here, we create a data object that is a list. Think of it as a super flexible data object. list.study &lt;- list(studyideal, studyperweek) boxplot(list.study, horizontal = TRUE, col = c(cardiffred, cardiffpurple1), # different colours for the plots axes = FALSE, # we are suppressing all axes boxwex = .6, # this is about the width of the interquartile range box ylim = c(0,40), # defines the limit of the x-axis (!). # It would have been the y-axis, but we rotated the figure. # This is a superconfusing command of the boxplot function # We just have to live with it... xlab = &#39;Hours&#39; # Label for the x-axis ) # Here, we are adding axes manually to have more control over them. axis(2, col = &quot;white&quot;, las = 1, at = c(1,2), # where to put the tickmarks lab =c(&quot;Ideally \\n Studied&quot;, &quot;Ideally \\n Studied&quot;) # Labels. &#39;\\n&#39; is a new line ) axis(1) This is it for this week. 3.3 Readings for This Week Don’t forget to check into the readings of this week which is chapter 8 in Fogarty (2019). As mentioned it is all about ggplot2—take it as a pointer to what is possible. We will not cover ggplot2 in this class, but you are of course free to go wild and become a proper ggplotter. "],["testing-a-hypothesis.html", "Chapter 4 Testing a Hypothesis 4.1 Generating Good Hypotheses 4.2 Studying Our Sample 4.3 Is What We See Just Coincidence? 4.4 Coding 4.5 Mathcamp: Summing With Sigmas 4.6 Reading for This Week", " Chapter 4 Testing a Hypothesis Welcome to the second part of the module PL9239. In our second scenario we turn to one of the core tasks of any data analysis: inference. The basic idea is that you want to be able to make a statement—e.g. calculate a statistic—about the world out there. But since the world out there is so large it is impossible to collect data about everything, we will have to chose a different approach. Instead, we are going to learn how to collect data about a sample and then say something with a little uncertainty about the world at large. To show you how useful all this actually is, I interviewed Michelle Brown who is working as a Senior Advisor on Elections for the National Democratic Institute. This NGO cooperates with local civil society actors in fragile democracies and trains them to do voting observation missions. These election observers tabulate the ballots in parallel to the official representatives and can thus get an idea about the amount of voting fraud that might be occurring. Obviously, it is prohibitive to observe every single polling station. In some countries there might be tens of thousands of polling stations and an NGO never has the resources to cover all that. Instead, the NDI relies on sampling. Election observers only monitor randomly selected polling stations, and the NDI then uses statistics to infer what is really going on. Together with colleagues, we are doing research with the National Democratic Institute that helps them improve their technology stack even further. In case you are interested, feel free to take a look at a presentation I gave at last year’s CogX—it might give you a good overview about what we are doing. Of course, feel free to reach out in case you want to know more. 4.1 Generating Good Hypotheses All research requires hypotheses. What is a good hypothesis? Good Hypotheses have a number of characteristics. A hypothesis is falsifiable. A hypothesis relates two observations. A hypothesis qualifies this relation, i.e. makes a statement whether the relation is the same, larger or smaller. Let me walk you through good hypothesis in a short video using our data as an example. 4.2 Studying Our Sample 4.2.1 Investigating the Hypothesis We investigate our data and take a look at what the data is telling us. First we compare the hours that you are studying with the hours that you think you should be studying. Given that we assume that you have a bit of a bad conscious regarding your efforts, there should be a clear difference. Hypothesis 1: On average, you think that you should be studying more than you actually are. Let us investigate the question. We begin with Hypothesis 1. It seems, there is a lot to our hypothesis. Given the two distributions, there seems to be a clear difference. Next we will compare you to the cohort here in Cardiff two years ago. Actually, I asked them the same questions that I asked you. Would we expect any difference between you and the previous cohorts? There is no real reason to believe that the cohorts are different, so: Hypothesis 2a: On average, all three cohorts are putting in the same effort. Hypothesis 2b: On average, you and your previous cohorts have the same expectations regarding the efforts that you think you should be making for your university degree. Let us take a look at what the data is telling us. It seems as if on average you are investing less than the cohorts in 2019 and 2018. It is also interesting that the variation between you is much lower than the variation in the previous cohorts. Now when you compare your cohort to the cohorts with regards to how much you think you should be studying, your ideal number of hours is on average not so different and you seem to be a bit more in line with the other students. 4.3 Is What We See Just Coincidence? We have seen that indeed there are slight differences between your module and the students in the module before you. Now, can we say something about your cohort—all Politics and IR students in your year—more in general? It could be, for example, that out of pure coincidence you are a particularly efficient crowd of students who do not need as much time to study as the cohorts before you. So given the differences in the observations from your class and the classes in previous years, can we say something about whether we would expect differences in your cohorts in general? 4.3.1 Are the Two Cohorts the Same? To be able to answer this question, we first need to understand the idea of sampling. Sampling means to draw a number of observations randomly from a larger population. Intuitively, our sample should be able to provide us with a good account of what is going on in the population. Of course, with bad luck we can always be wrong and sample a result that is quite unusual given what is going on in the population. It is also intuitive that larger samples should be less prone to coincidences. If we only ask a few number of individuals, there is fair likelihood that these few people do not represent the overall population well. However, if we ask a larger amount of people, we could be fairly certain that this sample gives us a good insight about the overall population. How does it help us understand whether your cohort is different from the cohort before you? Let us assume that on average the effort that you put into your studies is the same, for you and for the cohorts before you. How likely is it that we would observe these two samples assuming that the cohorts invest on average the same amount of time? What are the odds to observe these differences? We have a clear expectation. If the average amount of hours in our data sample is largely different, then this is already a good indication that it should be quite unlikely to observe so different samples given that they all come from the same population. If it is unlikely that we observe such a group of students on the basis of pure coincidence, we should maybe revisit our initial assumption. Maybe, it was wrong to assume that you and the other cohorts all share the same efficiency in studying. We would then have to refute this initial assumption and rather conclude that, well maybe your group indeed puts in a different effort into your studies. 4.3.2 How Can We Tell the Difference? In the light of the two samples that we observe, we want to be able to determine how likely it is that they are from the same population? So we need a measure. It is called the p-value and in our example it will be able to tell us how likely is it to observe the mean of the two samples given that we assume they come from the same population. This is a very useful measure, because now we can say something about the likelihood that these samples really are from cohorts with the same amount of effort in both groups. If the samples look quite similar, then we can be certain that they are from same population. However, if they are really distinct, the probability that the cohorts have similar efforts is quite small. So what should we conclude? We might want to reconsider our initial assumption: Maybe the two cohorts are not one large population. Instead, maybe the two cohorts are different after all. 4.3.3 Hypothesis Testing What we are doing here is called hypothesis testing. More formally, we distinguish between two hypothesis. The first one is called the null-hypothesis and it assumes that there is no difference between the two groups. We can specify it explicitly as follows. Null hypothesis (\\(H_0\\)): The samples come from populations with the same mean. As a default, we will always at first believe in the fact that there is no difference between the cohorts. We change our minds only if we see that it is quite unlikely that we would observe our two samples. In that case we will then rather prefer a different hypothesis, the alternative hypothesis. Alternative hypothesis (\\(H_A\\)): The samples come from populations with different means. If we believe in the alternative hypothesis, then we say that the two populations are different. How unlikely is ‘unlikely enough’ to change our minds? By convention, in the Social Sciences we define a threshold of 5% for that: We will believe in the null-hypothesis until the two samples are so different that we think it would be less than 5% probable to observe two samples that are so different by chance. More formally, the threshold \\(\\alpha\\) for the acceptable p-value is set to \\(p \\leq 0.05\\). 4.3.4 Test! Test! Test! Now, let us go and test the hypotheses in our data. Regarding the actual effort you put into studying, is your cohort different from the cohort in 2019? With R, we can do a so called t-test. We will cover it more formally next week, so do not worry at this stage how it really works. What matters for now is that it will return us the p-value we are interested in: the probability that the mean in the cohorts is the same given the two samples we observe. We first compare your cohort to the cohort in 2018. ## ## Welch Two Sample t-test ## ## data: dat18$studyperweek and studyperweek ## t = 1.3862, df = 26.205, p-value = 0.1774 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.902392 9.792098 ## sample estimates: ## mean of x mean of y ## 16.88235 12.93750 R reports a number of statistics and for now we only care about some. First, for the mean of both of the samples at the very bottom of the table with 12.94 for your cohort and 16.88 for the cohort back in 2018. The important bit for us is the p-value. Here it is reported as 0.18. How do we interpret this result? Well it tells us that assuming that both of the samples come from the same population, there is a 17.74% probability that we observe the result if both are from populations with the same mean. In the light of this evidence, we stick with the null hypothesis and believe that apparently the amount of average effort is the same in both cohorts. Let us now compare your cohort to the one in 2019. Here are the results from the t-test. ## ## Welch Two Sample t-test ## ## data: dat19$studyperweek and studyperweek ## t = 2.161, df = 29.225, p-value = 0.03903 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4413764 15.9444932 ## sample estimates: ## mean of x mean of y ## 21.13043 12.93750 The means are even further apart. The cohort in 2019 was putting in on average 21.13 hours, while you on average study 12.94 hours. Can we still assume that the mean in both cohorts is the same in the light of the data we collected from the students in the two methods classes? The p-value is reported as 0.04. If both of the samples were drawn from cohorts with the same mean, we would have a 3.9% probability that we observe the data in our two samples. This is quite unlikely and below the general accepted the threshold of \\(\\alpha\\) = 0.05. So what do we do now? Well, we refute the null hypothesis \\(H_0\\). Instead, we will trust in the alternative hypothesis \\(H_A\\). Your cohort is on average studying less than the cohort in 2019. We can do the same exercise for the hours that you think you should be studying ideally. Let me show you how this works in a quick video. 4.3.5 Type 1 and Type 2 Errors When we test a hypothesis based on samples, we can of course be wrong. Remember that everything do involves probabilistic statements. An \\(\\alpha\\) level of 0.05 means that in 19 out of 20 cases we should be correct. However, in one out of 20 cases we might wrongly refute a hypothesis. We could observe two samples that seem to be different even though in reality the null hypothesis is actually true. This kind of error is called a type-1 error or also a false positive. Then again, the two samples may come from populations with distinct means. But out of bad luck we get samples that actually look quite similar. In that case, we speak of a false negative, or a type-2 error. Figure 4.1 offers you a nice summary. Figure 4.1: Type 1 and Type 2 Errors A good way of remembering this difference is with the help of the following story that you certainly know from your childhood. The first time the young shepherd is calling for help, the farmers believe that the wolf is there even though in reality it is not—clearly a type 1 error or a false positive. In the second instance, the farmers think there is no wolf, even though in reality there is actually one. We speak of a false negative, or a type 2 error. 4.4 Coding This week was quite heavy on conceptual things, so we will be much lighter with the code. 4.4.1 Data Management We learned that we can select from objects using their position and these brackets [ ]. Since data frames are R’s spreadsheets, you can select from them also by the name of their column. # Create data people.living.in.household &lt;- c(3,2,5) education.in.years &lt;- c(10, 13, 15) favourite.colour &lt;- c(&#39;forestgreen&#39;, &#39;darkred&#39;, &#39;lemongreen&#39;) dat &lt;- data.frame(people.living.in.household, education.in.years, favourite.colour) Let us select the first column. dat[,1] ## [1] 3 2 5 dat$people.living.in.household ## [1] 3 2 5 Or what about the third element of second column? dat[3,2] ## [1] 15 dat$education.in.years[3] ## [1] 15 Sometimes you want to recode a variable. The oldschool way using the car package goes as follows. library(car) dat$favourite.colour &lt;- recode(dat$favourite.colour, &quot; c(&#39;forestgreen&#39;, &#39;lemongreen&#39;) = &#39;green&#39;; &#39;darkred&#39; = &#39;red&#39;&quot;) dat$favourite.colour ## [1] &quot;green&quot; &quot;red&quot; &quot;green&quot; Finally let us meet a problem that will keep haunting you until the end of your professional life: missing data. Sometimes, data has empty entries. By convention R uses NA for it. Please not that even if it looks like a string variable, it actually is not treated as such by R. ## [1] 5e+04 1e+05 NA Things begin to be complicated as soon as you want to do calculations with the variable. The reason is that R protects you from making stupid mistakes. mean(dat$income) ## [1] NA I know this is a pain at the moment, but believe me, once you write more complex programmes for analysing data, it will become really useful. You can set options in the functions to override the default behaviour. mean(dat$income, na.rm = TRUE) ## [1] 75000 Another more general way of handling the issue is to filter your data set for missing values with na.omit(). dat.no.na &lt;- na.omit(dat) dim(dat) ## [1] 3 4 dim(dat.no.na) ## [1] 2 4 As you can see, R kicked out a whole row from our observations. This certainly means that quite a bit of information just go lost here. Deleting missing data is a quick and convenient fix, but it comes at a high price. There is a whole subfield that is concerned with imputing values that are reasonable for the missing observation. The intuition is that you would use information from the fully observed variables to get a good guess for the observation that is missing. For example, in our case the years in education might give us a good proxy for the income that someone receives and we can get a good estimate for it. If we then want to analyse the relationship between—say—preferred colours and income we would then have a better data basis for that. 4.4.2 Hypothesis Testing R is made for statistical analysis. No surprise that doing a t-test is really easy: With the function t.test(). ttest.perweek.21.19 &lt;- t.test(dat19$studyperweek, studyperweek) 4.5 Mathcamp: Summing With Sigmas You were asking for a refresher on how to read the symbol \\(\\sum\\), so I put together a small video. 4.6 Reading for This Week The readings for this week are chapter 6 in Fogarty (2019) which tells you all about generating hypotheses. Chapter 9 is all about how to test them. "],["why-does-hypothesis-testing-work-a-primer-on-probability.html", "Chapter 5 Why Does Hypothesis Testing Work? A Primer on Probability 5.1 Distributions 5.2 Inference 5.3 Revisiting the Original Questions 5.4 A Simple Figure With Two Variables 5.5 Reading for This Week", " Chapter 5 Why Does Hypothesis Testing Work? A Primer on Probability Why do we care so much about inference? We can calculate central tendencies and spreads. Why learn more? This week we are revisiting the concepts from last week and will take a closer look at them. To goal is that you gain a deeper understanding about what it is that we are actually doing here. What does inference mean? Why does all this inference actually work? And why is it so important to properly randomise the samples that we draw? How can we be 95% confident about the test of a hypothesis? 5.1 Distributions What does inference actually mean? To properly understand the idea behind it, let us begin with distributions and a number of related ideas that will all be useful. 5.1.1 Probability We start with with probability and take a look at a definition. Let us define probability from a frequentist perspective. The probability of an outcome is the frequency of that outcome if the process were repeated a large number of times. This says that we can learn the probability of an event by trying it out many times—whatever it means. Here are some examples that you know from everyday life. Toss a coin: The probability to get a ‘head’ is Pr(head) = 1/2 — if the coin is fair of course. Toss a dice: The probability to throw a 1 is Pr(1) = 1/6 The probability to throw everything else than a 1 is Pr(not 1) = 5/6 While you cannot observe these probabilities from one throw alone, try it out a couple of times, say, 100? Or even better 1000? You should get really close to the true underlying value of the probability. 5.1.2 Distribution The next useful concept is the distribution. You can get to the distribution of an event in a pretty straightforward way. Imagine we were playing Boule in our local club ‘Allez les Bleus.’ On our Thursday evening training sessions we are trying to hit a line that is exactly 5m away. As avid data aficionados we are measuring each try and record the data in a spread sheet. At home you sit down and chart of histogram of all the tries that we have been throwing. HAUI Figure 5.1: A Histogram Becoming a Distribution. On the horizontal axis you are observing the possible outcomes. In statistical theory, this area is also called the sampling space. In the example with the coin, the sampling space \\(S\\) of one fair flip one coin is \\(S = {H, T}\\) where \\(H = \\text{heads}\\) and \\(T = \\text{tails}\\) are the outcomes. Here it is how far we can throw. On the vertical axis you are at first simply observing the frequencies. Increasing the number of bins of the histogram leads to increasingly realistic representations of the data. With infinite number of bins, you end up with a continuous representation of the data. Once the data is continuous, we do not speak of frequencies any longer—simply because each point on the x-axis is not infinitely small. We now call the vertical axis density. Given our definition of probability above, if we were to repeatedly throw the boule infinitely many times, the resulting data would be the distribution of the probability to actually hit the 5m line. There is an important property of distributions: Everything under the curve adds up to 1. This means that probability distributions have a straightforward geometric interpretation: we can simply ask for value ranges that we are interested in and see how often we would observe it. 5.1.3 The Normal Distribution But if we want to know more, we have to introduce a more rigorous concept of what a probability distribution actually is. Let us get to know the Normal Distribution, a very special probability distribution. This is how it looks like. Figure 5.2: A Histogram Becoming a Distribution. The Normal Distribution has a number of important properties. It is always a bell-shaped curve. It is always symmetrical. The mean = median = mode. The tails are asymptotic, which means the values get closer to the x-axis the further you go into infinity, but never intercept it. Two parameters drive the distribution: The mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). The Normal Distribution has a proper formula that allows us to calculate its density. Remember, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) determine how the function looks like. Then, given any value \\(x\\) we can calculate the resulting density. The formula for the probability density function was discovered by Carl Friedrich Gauss. \\[f(x; \\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} exp \\left[ - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} \\right] \\] The parameter \\(\\pi\\) is the famous mathematical constant. The expression \\(exp(\\cdot)\\) is short for \\(e^{(\\cdot)}\\), where \\(e\\) is yet another important mathematical constant, the Euler number. You do not have to understand all details here, the main point is that you can see how \\(\\mu\\) and \\(\\sigma\\) drive the shape of the whole thing. The resulting shape of the function has yet another couple of great characteristics. The area covered within the bounds of the standard deviation always remains the same: The area ± 1 SD from the mean always covers 68.3% The area ± 2 SD from the mean always covers 95.4% The area ± 3 SD from the mean always covers 99.7% etc. Take a look at the figure below. We have two different Normal distributions. On the left, \\(\\mu\\) = 5 and \\(\\sigma\\) = 1 and on the right \\(\\mu\\) = 5 and \\(\\sigma\\) = 2. In both cases, the area that covers one standard deviation to the left and to the right of the mean covers 68.3%. This means that if we have a process that we can describe with this function, then we know that there is a probability of 68.3% that we end up with a value between 4 and 6 on the left figure. Likewise, with the same probability of 68.3% we will end up between 3 and 7 on the right figure. The same holds for all other values of the standard deviation (2 SD, 3 SD, …) Fascinating!. Figure 5.3: Two Different Normal Distributions and The Probability Mass Covered By Their Standard Deviations. This is really useful, because independent of the exact shape of the Normal Distribution—that is indenendent of the values of \\(\\mu\\) and \\(\\sigma\\)—we can now draw all kinds of probabilities using simple geometry. Figure 5.4: A Histogram Becoming a Distribution. Remember what we know: one standard deviation to both sides covers 68.3%, two standard deviations to both sides covers 95.4%. The probability to observe a value between 0 and 1: 34.15%. We know that 1 standard deviation to the left and right adds to 68.3%, so 68.3%/2 = 34.15%. The probability to observe a value &gt; 1: 15.85%. Simply take the left 50% and also add the one standard deviation to the right of the mean 68.3%/2 = 34.15%. So 50% + 34.15% = 84.15%. Now given that all under the curve adds up to 100% we simply subtract the 84.15%—and get 15.85%. 5.1.4 Your Standard Measure z-scores What are z-scores and why do we need them? We can use them if we want to make comparisons across samples. For example, we might want to compare the effort from two students Alexandra and Bastian who are in different cohorts. In comparison to their peers, is Alexandra more diligent than Peter? Alexandra studies 40h per week, Bastian 42h. So prima facie, Bastian is working more. But: Is he working more than Alexandra in the light of the different study regimes of their cohorts? To determine who studies harder we can standardize the effort of each of the students: How different are Alexandra and Bastian from the typical student in their class? The idea is straightforward: We calculate the difference of each student from the average per spread of their respective cohorts. To compare variables from different distributions, we can standardize them by building so called ‘z-scores.’ \\[ z_{i} = \\frac{x_{i}-\\bar x}{\\sigma} \\] A so standardized variable will have mean zero and a standard deviation of one. Let us do the math. We learn that Alexandra’s class has a mean of 30h with a standard deviation of 5. Bastian’s class has mean of 40h with standard deviation 6. We begin with the z-score from Alexandra. It is \\(z_A =(40−30)/5=2\\) Bastian’s z-score is \\(z_B =(42−40)/6=0.33\\) What do we conclude? Well, Alexandra is on average much more ‘off’ in comparison to her peers than Bastian. She is much more of an outlier and he actually is much more of an average guy when it comes to studying. One more thing. The z-scores have a link to the Normal distribution. We can actually use them to calculate the probability of an event. The only thing you have to do is convert your data into z-score. And then you can look up its probability in the probability tables of a normal distribution. Figure 5.5: A Histogram Becoming a Distribution. Here is a quick video where I show you how to use the table. 5.2 Inference Let us continue with a proper look at inference. We will revisit some topics we talked about last week—but this time in a much more rigorous way. We might be interested in all sorts of research questions. Which party would citizens vote for? How many hours do school-children study at home? How many children do parents wish to have? The problem is pretty straightforward: We cannot ask everybody or count everything. What do we do? Well, we take a look at a sample. With the sample, we will try to answer to questions. Given what we observe in the sample: What is the most probable value in the population out there? How certain are we about our results? 5.2.1 Some Key Concepts Before we learn how to answer these questions, we will take a look at some key concepts. Population The full set of cases about which we want to generalise. Sample A subset of the population. Variable Logical set of attributes (characteristics) of an object (person, thing, etc.) that can vary across a range. Parameter A characteristic of a population; usually unknown. Descriptive Statistics Statistics that summarise the distribution of the values of variables in a sample. Inferential Statistics The use of statistics to make inferences about a larger population based on data collected from a sample. 5.2.2 Population and Samples We said already that we can never know the whole population, simply because it is impossible to ask every single individual. Imagine we are interested in learning how the distribution of a particular variable looks like in the population—e.g. income, voting intention etc.. This means that we are particularly keen learn two key variables. The population mean \\(\\mu\\) The population standard deviation \\(\\sigma\\) How could learn about the population? We can draw one sample and measure the sample mean \\(\\bar m_i\\); and the sample standard deviation \\(s_i\\). Note that the notation for population parameters is in Greek letters, while the sample parameters will always be Roman letters. This will give us a first impression about how the distribution looks like. Can we take any sample? Think of the following situation. Imagine you wanted to predict the upcoming elections in Wales. You are asking 1000 people about their voting intention. To do so, you walk in the rainy streets of Cardiff until you collected 1000 responses—and 36% intend to vote for the green party. Can you trust in this result? Well, let us see: You have an idea about how 1000 people who had a reason to be on the streets of Cardiff are intending to vote. But it could be that those people might have a particular political preference. For example, they were out and about despite the weather conditions. It would be quite reasonable to assume that they are more environmentalists than those who stayed at home—or even those who live outside Cardiff. If you want to know about the voting intention in all Wales, you probably need a good representation of what is going on in all Wales. Ideally, you would be able to have a completely random draw from all Welsh voters—and not only those who were on the streets of Cardiff on a particularly rainy day. Luckily, when we generate the sample with our computer, we can simulate a truly random sample How could we improve our knowledge about the population? We could draw many more samples, calculate the mean and standard deviation from them and learn how much all these means and standard deviations vary. This would refine our idea about the average typical value in the population and the average spread of the data in the population. Figure 5.6: Drawing Many Samples from the Population. Doing so, we are retrieving another distribution: The distribution over the averages of our samples. This distribution is also called sampling distribution. As with any distribution, we are interested in two key values. First the mean of sampling distribution of means \\(\\bar m_i\\). Second the standard deviation of sampling distribution of means \\(s_n\\) Let us take a look at a real example in Figure 5.7. On the left, there is the unknown distribution in the population. We then randomly draw one sample from this distribution. You can see that for a random reason, it is a bit off. If this were in the real world, we of course would not know that we are off—we only have this one sample. Since we are simulating everything on the computer we take many more samples. You can see on the right that we are retrieving many samples with their bell-shaped distributions. For each of these individual samples we can calculate an average. And you can see all averages accumulate at the bottom of the right hand figure as red points: if you closely inspect these means, they are clustering in ‘the middle.’ This center actually is the true underlying population parameter. Figure 5.7: Drawing Many Samples from the Population. If we were able to repeat the sampling a great number of times, there actually is a way in which we can really measure the unkown population parameter \\(\\mu\\), the average of the population. We can estimate the population mean \\(\\mu\\) as the average over all means from all samples. The means of all samples also vary of course, which means we can calculate a standard deviation for them. This standard deviation of the means of the samples—so the standard deviation of the sampling distribution—is called the standard error. 5.2.3 The Central Limit Theorem If you closely look at how the means of the samples are actually distributed, do you notice something? Well, it looks a bit as if they were distributed in a bell shaped way—just like the Normal distribution we heard of before. So here is another amazing thing about ‘the Normal’. The whole world would probably not really work if it did not exist. The reason is: all statistical processes boil down to being normal eventually—as proven by the mathematician Pierre-Simon Laplace some 200 years ago. Informally, this is what the Central Limit Theorem is saying. Imagine we have a population distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and we are interested in its mean. Repeatedly taking samples from that distribution, yields the sampling distribution of the mean which approaches a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as the observation in each sample \\(n\\) increases. This holds regardless of the shape of the original distribution. The Central Limit Theorem is the basis for application of statistics to many natural phenomena (which are the sum of many unobserved random events). How? Take a sample, calculate its mean. Do the same thing again and again. The distribution of sample means will be normal. This is a fun application of it: the Galton Board. Balls are entered at the top and have to take a series of left-right decisions. The result of it in the end is the normal distribution. Remember: independent of the original stochastic process, if all tries are truly random, the resulting distribution ends up being normal. By the way, here in Cardiff’s Techniquest there is a massive version of the Galton Board where you can try this yourself. In the video, this works well for the large and the medium balls. But it does not really work for the small ones. Why not? David Bulger offers a great intuition in the comments: Watching the small balls fall, you can see that they tend to gather momentum and run in diagonal paths to the left or right—that is, they don’t change direction much. This illustrates the importance of the independence assumption in the central limit theorem: if the individual random variables are not independent, then their sum may not tend to a normal distribution. So you see how key it is that the sampling happens truly at random. Everything breaks down if we cannot guarantee the randomness of the sampling process. The Central Limit Theorem is the foundation for any inference that we want to do. And it will be really helpful for answering a question you might have on your mind already. With the computer you can draw many samples and calculate their averages to get to the population parameter. But in practice this is not possible: We typically have money to field one survey—and not many different ones. How do all these insights about the sampling distribution help us in practice? Enter the Central Limit Theorem (CLT). We know that if sampling occurs randomly the whole sampling distribution results in a normal distribution. It is OK therefore to be bold enough and assume that the sample we draw is normally distributed. We can therefore calculate an estimate for the key parameters we are interested in from our sample. Given our sample, what would be our best guess for the unknown population parameter \\(\\mu\\)? Well, the average of our sample, right? Our estimate for \\(\\mu\\) is therefore \\[ \\hat{\\mu} = \\bar m_i \\] Note that the hat indicates an estimate. What can we say about our uncertainty of our mean of the sample, the standard error? We know it will follow a normal distribution (CLT!). If we knew the true population standard deviation \\(\\sigma\\), we could easily calculate it from one single sample. \\[ s_n = \\frac{\\sigma}{\\sqrt{n}} \\] Of course we do not know the true population parameter. So what do we do? We again use the value from the sample and assume that we have the right value. The estimate for the standard error then is as follows. \\[ \\hat{s}_n = \\frac{s_i}{\\sqrt{n}} \\] Let us take a look at an example to make this more easy to understand. We assume your sample is a random sample from your cohort. How many hours does your cohort sleep per night? And what is our uncertainty around that? \\[\\hat \\mu = \\bar m_i = 7.69 \\] \\[ \\hat \\sigma_i = \\frac{\\hat \\sigma}{\\sqrt{n}} = \\frac{1.08}{\\sqrt{16}} = 0.27 \\] 5.2.4 Summing Up: What is Inference? Let us take stock and understand the core idea of inference. * We can never know the true population mean \\(\\mu\\). * We will take the next best guesses: Measures on the basis of our sample. * We will make use of the mean, the standard deviation and the size of the sample. * We rely on the fact that random samples would typically give us a good idea about the population (Central Limit Theorem). * And then we assume that the sample mean is the population mean. * We then use the standard deviation from our sample to estimate the standard error of the sampling distribution. 5.2.5 Confidence Intervals: Being 95% Certain Last thing to wrap your heads around for today: We have the standard error. How certain can we be that our sampled mean is close to the population mean? The thing is, in real life we can of course never know the true population mean \\(\\hat \\mu\\). But we can use the Central Limit Theorem! We know that our sample—if well implemented—is a result of a random sampling process. And if that is the case, all samples including ours will follow the Central Limit Theorem (CLT). Let us assume we want to catch the population mean with our sample in 95 out of 100 samples. That means from the mean, we have to cover the area 42.5% to the left of the mean and 42.5% to the right of the mean. We call this area the 95% confidence interval. Figure 5.8: Two Different Normal Distributions and The Probability Mass Covered By Their Standard Deviations. Remember that we were talking about the z-scores above? Using z-scores, we now can translate this statement about probabilities into units of standard deviation in a real distribution. If we want to cover the central 95% of a distribution, we have to go 1.96 times the standard deviation to the left of the mean and 1.96 times the standard deviation to the right of the mean. Knowing that we can assume that the sampling distribution is normally distributed (hat tip CLT), we can calculate the probability that we cover the range of values where we will capture the real mean in 95 out of 100 times. It is the range given by \\[ \\hat\\mu ± 1.96 \\hat \\sigma \\] Or in our example How many hours do you sleep at night? \\[ 7.69 ± 1.96 1.08 = [5.5732, 9.8068] \\] This means that if we drew 100 samples, the true population mean of your overall cohort would be in the interval [5.5732, 9.8068] in 95 out of 100 times. 5.2.6 Revisiting t-tests The great thing about confidence intervals is that we can also use them to understand hypothesis testing. Imagine we are testing whether there is actually a difference in the mean of two samples, just like we did last week with the t-tests. Instead of checking the p-value, we can also study the confidence interval. In 95 out of 100 cases, how much of a difference do we expect to be there between the two samples, given what we observe in the data? ## ## Welch Two Sample t-test ## ## data: dat18$studyperweek and studyperweek ## t = 1.3862, df = 26.205, p-value = 0.1774 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.902392 9.792098 ## sample estimates: ## mean of x mean of y ## 16.88235 12.93750 R returns us the confidence interval: Here, it is anywhere between -1.9 and rround(ttest.perweek.21.18$conf.int, 2)[2]`. This means that your confidence interval covers the 0—with a probability of 95% you could not tell whether the true difference in the means is lower 0, equal 0 or higher than 0. Hence, you cannot refute the original hypothesis that the means are the same. Therefore, you have to conclude that the means are probably the same. Remember that the difference between you and the cohort in 2019 was statistically significant? ## ## Welch Two Sample t-test ## ## data: dat19$studyperweek and studyperweek ## t = 2.161, df = 29.225, p-value = 0.03903 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4413764 15.9444932 ## sample estimates: ## mean of x mean of y ## 21.13043 12.93750 Now, the confidence interval it is anywhere between 0.44 and rround(ttest.perweek.21.19$conf.int, 2)[2]`. The confidence interval does not cover the 0. You can conclude that with a probability of 95% the difference between the cohort is larger than 0. Now, you can refute the original hypothesis that the means are the same—at least with a 95% confidence in the results. 5.3 Revisiting the Original Questions Let us revisit the original questions we initially set out above. What does inference mean? Why does all this inference actually work? And why is it so important to properly randomise the samples that we draw? How can we be 95% confident about the test of a hypothesis? If you worked through this class properly and also did the readings, you should be able to answer the questions. If not, let’s be in touch in the weekly workshop on Tuesdays. 5.4 A Simple Figure With Two Variables This week was quite heavy on concepts, hence let us be really quick on the coding. Just a simply plot of two variables. In all we are doing, I am introducing some functions for randomly sampling in different ways. Let us first do a random draw from a normal distribution. We then calculate a second variable from it, adding a bit of normally distributed noise on top. Then we plot it using colour. # Select var1 randomly from a normal distribution var1 &lt;- rnorm(n = 100, mean = 5, sd = 3) # Select some noise again randomly from a normal distribution var2.noise &lt;- rnorm(n=100, mean = 0, sd = 0.5) # create second variable var2 &lt;- var1 + var2.noise # Here I sample two colours randomly allcolours &lt;- colours() two.colours &lt;- sample(allcolours, 2) # Plot two variables plot(var1, var2, pch = 21, # Setting Point Characters, here filled circles col = two.colours[1], # using the first colour for the frame bg = two.colours[2], # using the second colour for the background las = 1, #rotates numbers on the left xlab = &#39;Label on the x-Axis&#39;) # abline is very useful for plotting all sorts of lines # a: intercept, b: slope abline(a= 0, b = 1, col = cardiffred) # horizontal line at 0, lwd: line width abline(h = 0, col = cardiffblue, lwd = 3) # vertical line at 0, lty: line type abline(v = 0, lty = 3) 5.5 Reading for This Week For this week, please read chapter 4 in Agresti (2018). "],["are-different-people-different.html", "Chapter 6 Are Different People Different?", " Chapter 6 Are Different People Different? "],["comparing-apples-and-oranges.html", "Chapter 7 Comparing Apples and Oranges", " Chapter 7 Comparing Apples and Oranges "],["the-fundamental-problem-of-causal-inference.html", "Chapter 8 The Fundamental Problem of Causal Inference", " Chapter 8 The Fundamental Problem of Causal Inference "],["causal-statements-from-observational-data.html", "Chapter 9 Causal Statements from Observational Data", " Chapter 9 Causal Statements from Observational Data "],["multivariate-regression-and-heterogenous-treatment-effects.html", "Chapter 10 Multivariate Regression and Heterogenous Treatment Effects", " Chapter 10 Multivariate Regression and Heterogenous Treatment Effects "],["useful-internet-resources.html", "Chapter 11 Useful Internet Resources", " Chapter 11 Useful Internet Resources Quick-R has been a go-to reference when you look up simple examples for quite some time. It is a really helpful source. If you want to dig a little deeper on the things we are doing, you can check out this beautiful online book titled Causal Inference: The Mix Tape. Also available as a print version of course… "],["bibliography.html", "Chapter 12 Bibliography", " Chapter 12 Bibliography Agresti, Alan. 2018. Statistical Methods for the Social Sciences. 5th ed. Pearson. Fogarty, Brian. 2019. Quantitative Social Science Data With R. London: Sage. "]]
